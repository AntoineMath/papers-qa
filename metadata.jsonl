{"file": "2510.06042v1.pdf", "section": "Introduction", "chunk": "Agent+P: Guiding UI Agents via Symbolic Planning\nShang Ma1, Xusheng Xiao2\u2020, Yanfang Ye1\u2020\n1University of Notre Dame2Arizona State University\n\u2020Corresponding Authors\n{sma5, yye7}@nd.edu,xusheng.xiao@asu.edu\nAbstract\nLarge Language Model (LLM)-based UI agents\nshow great promise for UI automation but of-\nten hallucinate in long-horizon tasks due to\ntheir lack of understanding of the global UI\ntransition structure. To address this, we in-\ntroduce AGENT+P, a novel framework that\nleverages symbolic planning to guide LLM-\nbased UI agents. Specifically, we model an\napp\u2019s UI transition structure as a UI Transi-\ntion Graph (UTG), which allows us to refor-\nmulate the UI automation task as a pathfinding\nproblem on the UTG. This further enables an\noff-the-shelf symbolic planner to generate a\nprovably correct and optimal high-level plan,\npreventing the agent from redundant explo-\nration and guiding the agent to achieve the\nautomation goals. AGENT+P is designed as\na plug-and-play framework to enhance exist-\ning UI agents. Evaluation on the Android-\nWorld benchmark demonstrates that AGENT+P\nimproves the success rates of state-of-the-art\nUI agents by up to 14% and reduces the ac-\ntion steps by 37.7% . Our code is available\nat: https://anonymous.4open.science/r/agentp-\nF7AF.\n1 Introduction\nWith mobile applications (apps) woven into all\nparts of our daily life, it is critically important to\nensure the high quality of apps. User interface (UI)\nautomation, the process of programmatically exe-\ncuting sequences of UI interactions, has become an\nessential method for improving app quality by en-\nabling automated testing for bug and vulnerability\ndetection (Lai and Rubin, 2019; Ma et al., 2025)\nand supporting user task automation (Orr\u00f9 et al.,\n2023; Rawles et al., 2024; Li et al., 2025b; Zhang\net al., 2023).\nWhile recent advances in UI automation, no-\ntably the integration of LLM-based UI agents (Liu\net al., 2025; Zhao et al., 202"}
{"file": "2510.06042v1.pdf", "section": "1 Introduction", "chunk": "2025b; Zhang\net al., 2023).\nWhile recent advances in UI automation, no-\ntably the integration of LLM-based UI agents (Liu\net al., 2025; Zhao et al., 2024; Ran et al., 2024)\nthat explicitly model the available actions in each\nUI AgentPlan\nUI AgentApp\nActionPlanApp\nUI Transition GraphPlanning Domain\nDefinition LanguageUI StateSymbolic PlannerTask Goal: Update the event named  \u201cMeeting\nwith boss\u201d by changing its timezone to EDTUI StateAction\nExisting UI agents Agent+P\nFigure 1: AGENT+P compared with existing UI agents\nin UI automation. AGENT+P improves performance by\nconstructing a UI Transition Graph via program analysis\nand leveraging an external symbolic planner to generate\na high-level, globally aware transition plan, thereby\nguide the agent towards the automation goal.\nUI screen, have demonstrated encouraging results,\nthe ever-growing complexity of modern UIs con-\ntinues to hinder effective and efficient automation.\nIn particular, existing approaches struggle inlong-\nhorizon planningtasks that require navigating via\nmultiple UIs since such multi-step planning often\nleads to increased hallucination rates (Liu et al.,\n2023; Wei et al., 2025; Xie et al., 2025; Wu et al.).\nFor example, the LLM-based agents employed by\nthese approaches typically follow a depth-first strat-\negy to find valid action sequences, making deci-\nsions based on local UI states without understand-\ning the global transition structure, where different\nUIs can lead to distinct subsequent actions. Con-\nsequently, they often fail to derive valid sequences\nthat accomplish task goals and repeatedly waste\neffort on actions that diverge from those goals.\nKey Insights.To address this fundamental limita-\ntion,we introduce an external planner module thatarXiv:2510.06042v1  [cs.MA]  7 Oct 2025\nprovides LLM-based UI agents with global transi-\ntion knowledge extracted through program analysis.\nThis module leverages established planning algo-\nrithms to prevent redundant exploration and guide\nthe agent toward diverse action sequences that are\nmore likely to achieve the automation goals. Specif-\nically, we model an app\u2019s global transition structure\nvia a UI Transition Graph (UTG) (Sun et al., 2025"}
{"file": "2510.06042v1.pdf", "section": "1 Introduction", "chunk": "guide\nthe agent toward diverse action sequences that are\nmore likely to achieve the automation goals. Specif-\nically, we model an app\u2019s global transition structure\nvia a UI Transition Graph (UTG) (Sun et al., 2025;\nWen et al., 2024), with nodes representing UIs and\nedges representing user-triggered UI transitions. A\nUI automation problem, navigating from a start UI\nto a target UI, can thus be formulated as apathfind-\ning problemon the UTG, where the objective is\nto find an optimal path from the start node to the\ntarget node. This formulation enables the use of\noff-the-shelf symbolic planners to derive provably\ncorrect and optimal plans, thereby fundamentally\neliminating high-level planning hallucinations and\nenhancing the reliability of LLM-based UI agents.\nOur Method.Building upon these insights, in this\npaper, we introduce AGENT+P, an agentic frame-\nwork that leverages symbolic planning to guide UI\nagents. Given a natural language UI automation\ngoal and the app for automation, AGENT+P op-\nerates iteratively in four stages, each executed by\nan LLM-based module, until the goal is achieved:\ntheUTG Builderconstructs a static UTG of the\napp and dynamically updates it during automation.\nTheNode Selectormaps the natural language goal\nto a targeted node in the UTG. ThePlan Gen-\neratortranslates the UTG into a problem prob-\nlem using Planning Domain Definition Language\n(PDDL) (Aeronautiques et al., 1998), which is then\nsolved by an external symbolic planner. The result-\ning symbolic plan is subsequently converted into\nnatural language instructions. Finally, theGUI Ex-\nplorerinteracts with the app to execute the trans-\nlated instructions to navigate to the goal.\nAGENT+P is designed as a plug-and-play plan-\nning framework that can be incorporated with\nand enhance existing UI agents. We evaluate\nAGENT+P by integrating it with four state-of-the-\nart agents on the AndroidWorld benchmark. Our\nresults demonstrate that by leveraging symbolic\nplanning on the UTG, AGENT+P increases the suc-\ncess rates of baseline agents by up to 14.0% and\nreduces the action steps by 37.7%.\nOur primary contributions are as follows:\n\u2022We propose AGENT+"}
{"file": "2510.06042v1.pdf", "section": "1 Introduction", "chunk": ", AGENT+P increases the suc-\ncess rates of baseline agents by up to 14.0% and\nreduces the action steps by 37.7%.\nOur primary contributions are as follows:\n\u2022We propose AGENT+P, a novel framework that\nleverages symbolic planning to provide LLM-\nbased UI agents with global transition informa-tion derived from program analysis, mitigating\nthe long-horizon planning failures commonly\nfaced by these agents.\n\u2022We present a novel formulation that maps the\nproblem of UI automation into a pathfinding\nproblem in the UTG, making it solvable with\nprovably correct symbolic planners.\n\u2022We conduct extensive evaluation on the Android-\nWorld benchmark, demonstrating that AGENT+P\nsubstantially improves the success rate and effi-\nciency of three state-of-the-art UI agents.\n2 Motivation\n2.1 UI and UI Transition Graph\nTo motivate our method, we begin by introducing\nthe concepts of widget, UI, and the modeling of UI\ntransitions (i.e., UTG).\nDefinition 1(Widget, Action).Awidget,denoted\nasw, is a basic interactive element on a UI screen.\nAnaction,denoted as a, is a 2-tuple a= (w,e) ,\nwhere wis a widget, eis the user event (e.g., click ,\ninput).\nFollowing existing UI agents that represent a UI\nas a sequence of widgets and supported actions (An-\ndroid World, 2025; Ye et al., 2025; Dai et al., 2025;\nLi et al., 2025b), we define the UI state (shortened\nas UI) as follows:\nDefinition 2(UI).AUI,denoted as u, is an n-\ntuple of all unique actions available on the screen,\nu= (a 1,a2,...,a n), where nis the total number of\navailable actions.\nThis definition is sufficient to represent UI tran-\nsitions while avoiding state explosion (Valmari,\n1996).\nDefinition 3(UI Transition Graph).A UTG for an\napp is a directed graph G= (U,T,\u03b5) that models\nthe transition structure of the app.\n\u2022U is a finite set of nodes, where each node u\u2208U\nrepresents a UIuin the app.\n\u2022T\u2286U\u00d7U is"}
{"file": "2510.06042v1.pdf", "section": "2.1 UI and UI Transition Graph", "chunk": "(U,T,\u03b5) that models\nthe transition structure of the app.\n\u2022U is a finite set of nodes, where each node u\u2208U\nrepresents a UIuin the app.\n\u2022T\u2286U\u00d7U is a set of directed edges. An edge\n(ui,uj)\u2208T represents a transition from UI uito\nUIu j.\n\u2022\u03b5:T\u2192A is an edge-labeling function. It maps\neach transition (ui,uj)to the action a= (w,e)\nthat triggers it, where the widget wis an element\nof the source UIu i.\nFigure 3 shows an example of UTG of an An-\ndroid app named Simple Calendar Pro in Android-\nWorld benchmark.\nTable 1: Average number of UTG nodes and edges for\napps where existing UI agents succeeded vs. failed.\nNodes Edges\nAgent Success Failed Success Failed\nDroidRun 27.0 71.2 62.7 168.0\nLX-GUIAgent 29.3 53.8 65.9 129.2\nAutoGLM 39.4 42.0 90.6 100.3\nFinalrun 23.8 55.0 59.5 125.6\nUI-Venus 18.0 50.7 66.0 108.0\n2.2 Motivational Study\nTo investigate how UI complexity affects UI agent\nperformance, we conduct a motivational study us-\ning the AndroidWorld benchmark (Rawles et al.,\n2024). This benchmark has 116 programmatic\ntasks across 20 Android apps, where an agent is\ngiven a natural language instruction and must nav-\nigate the app to achieve the goal. Performance is\nevaluated using a task-level \u201csuccess rate\u201d. It is\nworth noting that published results typically report\nonly an overall average for all tasks. To obtain\nper-app success rates, we download and analyze\nthe trajectories from five top-performed UI agents.\nSpecifically, we classify an app as \u201csuccessful\u201d for\na given agent if its performance on that app exceeds\nits overall average, and as a \u201cfailed\u201d app otherwise.\nTable 1 compares the UI complexity measured\nby the number of UTG nodes and edges between\nsuccessful and failed apps. Our Wilcoxon Signed-\nRank test (Wilcoxon, 1945) shows"}
{"file": "2510.06042v1.pdf", "section": "2.2 Motivational Study", "chunk": "a \u201cfailed\u201d app otherwise.\nTable 1 compares the UI complexity measured\nby the number of UTG nodes and edges between\nsuccessful and failed apps. Our Wilcoxon Signed-\nRank test (Wilcoxon, 1945) shows that failed apps\nhave significantly more UTG nodes and edges than\nsuccessful ones ( p=0.03 ). This finding indicates\nthat existing UI agents struggle with navigating\napps with complex UIs, which motivates our work\nto leverage an app\u2019s transition structure to enhance\nagent performance.\n3 Problem Formulation\nIn this section, we first define the problem of tar-\ngeted UI automation, and how to convert it into a\nequivalent classical planning problem.\n3.1 Problem Definition\nWith the structure of the app modeled as a UTG, we\ncan now formally define the task of UI automation.\nDefinition 4(UI Automation).UI automation is\nthe process of programmatically executing a se-\nquence of actions \u03c0=\u27e8a 1,a2,...,a N\u27e9, where each\naction a i\u2208A.\nIn this work, we focus on a specific, goal-\noriented variant of this task.Listing 1: Domain PDDL for targeted UI automation.\n1( define ( domain utg - automation )\n2(: requirements : strips : typing )\n3\n4(: types\n5node - object\n6)\n7\n8(: predicates\n9(at ?n - node )\n10( connected ? from - node ?to - node )\n11( visited ?n - node )\n12(goal - node ?n - node )\n13(goal - achieved ?n - node )\n14)\n15\n16(: action navigate\n17: parameters (? from - node ?to - node )\n18: precondition ( and\n19(at ? from )\n20( connected ? from ?to)\n21)\n22: effect ( and\n23( not (at ? from ))\n24(at ?to)\n25( visited ?to)\n26( when (goal - node ?to) (goal - achieved ?to))\n27)\n28)\n29)\nDefinition 5(Targeted UI Automation).Given an\napp with an initial UI uinitand a target UI utarget,\nthe objective is to find a valid sequence of actions\n\u03c0=\u27e8a 1,a2,...,a N\u27e9, where ai= (w i,ei), that navi-\ngates the app from u initto u target."}
{"file": "2510.06042v1.pdf", "section": "3.1 Problem Definition", "chunk": "the objective is to find a valid sequence of actions\n\u03c0=\u27e8a 1,a2,...,a N\u27e9, where ai= (w i,ei), that navi-\ngates the app from u initto u target.\nWe formulate targeted UI automation as a\npathfinding problem on the UTG. Let \u03ba:A\u2192\nR+be a cost function that assigns a positive cost to\neach action, representing computational resources,\nexecution time, or other relevant metrics. The prob-\nlem is then to find a path from uinittoutarget that\nminimizes the total execution cost:\n\u03c0\u2217=argmin\n\u03c0N\n\u2211\ni=1\u03ba(a i).\nTo simplify the formulation, in this work, we adopt\na uniform action cost. This reduces the cost-\nminimization task to the classical shortest path\nproblem, where the objective is to find the path\nfromu inittou target with the fewest actions.\n3.2 Symbolic and Classical Planning\nSymbolic planning is a long-standing area of AI\nconcerned with finding a sequence of actions to\nachieve a predefined goal. The most fundamen-\ntal and widely studied form isclassical plan-\nningwhere a planning problem instance, P, is for-\nmally described as a tuple P=\u27e8D,s init,G\u27e9, where\nD=\u27e8F,A\u27e9 is the planning domain. These com-\nponents are defined as follows:\n\u2022States: Fis a set of fluents or predicates that\ndescribe the properties of the world. A state sis a\nTable 2: Mapping of UI automation notation to classical planning equivalent.\nUI Automation Notation Classical Planning Equivalent\nA UI state (UI)u\u2208UA states\u2208Swhere the predicateat(u)is true\nThe set of all UIsUThe state spaceS\nThe initial UIu init The initial states init, defined byat(u init)\nThe target UIu target The goalG, specified by the conditionat(u target )\nA UI transition via the edge(u i,uj)with labelaA planning action\nPrecondition: the app is on UI u ipre(a):at(u i)\nEffect: the app moves to UI u je f f(a):\u00acat(u i)\u2227at(u j)\nA path fromu inittou target via the sequence of action\u03c0A plan\u03c0\ncomplete assignment of truth values"}
{"file": "2510.06042v1.pdf", "section": "3.2 Symbolic and Classical Planning", "chunk": "(u i)\nEffect: the app moves to UI u je f f(a):\u00acat(u i)\u2227at(u j)\nA path fromu inittou target via the sequence of action\u03c0A plan\u03c0\ncomplete assignment of truth values to all fluents\ninF. The set of all possible states is the state\nspaceS.\n\u2022Initial States: sinit\u2208S is the initial state of the\nworld.\n\u2022Goal :Gis the goal specification, a set of condi-\ntions on states. Any state s\u2208S that satisfies all\nconditions inGis a goal state.\n\u2022Actions: Ais a set of actions. Each action a\u2208\nAis defined by its preconditions, pre(a) , and its\neffects, eff(a) . An action can only be executed\nin a state where its preconditions are met, and its\neffects describe how the state changes after its\nexecution.\nA solution, or plan, \u03c0, is a sequence of actions\n\u27e8a1,a2,...,a N\u27e9that transforms the initial state sinit\ninto a goal state. This is achieved by applying the\nactions sequentially, where each action aiis appli-\ncable in the state resulting from the execution of\nai\u22121, and the final state after executing aNsatisfies\nG.\nThe Planning Domain Definition Language\n(PDDL) (Aeronautiques et al., 1998) is the standard\nlanguage for representing such planning problems,\ntypically using two files: a domain file defining F\nandA, and a problem file definings initandG.\n3.3 UI Automation to Classical Planning\nThe pathfinding problem can be naturally cast into a\nclassical planning problem, allowing us to leverage\nclassical planners to compute the solution, i.e., the\nshortest path. Table 2 illustrates the mapping from\nthe UI automation domain to the classical planning\ndomain, which is elaborated as follows:\n\u2022States: A planning state scorresponds to the app\nbeing at a specific UI u. We can define a pred-\nicate at(u) which is true if the app is currently\non UI u\u2208U . The state space Sis the set of all\npossible UIs,U.\n\u2022Initial State: The initial state sinitis defined bythe predicateat(u init)being true.\n\u2022Goal: The goal Gis specified by the condition"}
{"file": "2510.06042v1.pdf", "section": "3.3 UI Automation to Classical Planning", "chunk": "\ufffdU . The state space Sis the set of all\npossible UIs,U.\n\u2022Initial State: The initial state sinitis defined bythe predicateat(u init)being true.\n\u2022Goal: The goal Gis specified by the condition\nthat the predicateat(u target )must be true.\n\u2022Actions: For each UI transition (ui,uj)in the\nUTG triggered by an action a= (w,e) , we define\na planning operator with precondition at(u i)and\neffects\u00acat(u i)andat(u j).\nFollowing this formulation, we define a general\nPDDL domain file template applicable to any tar-\ngeted UI automation task, as shown in Listing 1.\nAny specific user task, represented by UTG with\na specified target UI, utarget, can be translated into\na corresponding PDDL problem file via this tem-\nplate. For instance, to solve the \u201cChange the time\nzone\u201d task in the Simple Calendar Pro app, the\nUTG from Figure 3 is converted into the problem\nfile presented in Listing 2. This symbolic represen-\ntation allows us to employ a classical planner to\nefficiently compute a valid and optimal sequence\nof actions to navigate the app from the initial UI,\nuinit, to the target,u target.\n4 AGENT+P\nAlgorithm 1 shows the workflow of AGENT+P.\nGiven a natural language goal specified by the user,\nAGENT+P operates through four primary modules\nthat interact in a continuous loop until the goal is\nachieved: theUTG Builder, theNode Selector,\nthePlan Generator, and theUI Explorer. In the\nfollowing subsections, we elaborate on the design\nrationale and functionality of each module.\n4.1 UTG Builder\nExisting methods for constructing a UTG rely on ei-\nther dynamic analysis (Wen et al., 2024; Sun et al.,\n2025), which is accurate but often suffers from\nhigh cost and incomplete coverage, or static analy-\nsis (Azim and Neamtiu, 2013; Yang et al., 2018),\nwhich is more comprehensive but can introduce\ninfeasible transitions (Liu et al., 2022).\nTo overcome these limitations, AGENT+P uti-\nlizes a hybrid approach that synergizes both tech-\nniques to build UTG. AGENT+P begins by perform-\ning"}
{"file": "2510.06042v1.pdf", "section": "4.1 UTG Builder", "chunk": "transitions (Liu et al., 2022).\nTo overcome these limitations, AGENT+P uti-\nlizes a hybrid approach that synergizes both tech-\nniques to build UTG. AGENT+P begins by perform-\ning static analysis to construct an initial UTG (Line\n1 of Algorithm 1), following established practices\nthat track API calls responsible for UI transitions in\nthe app\u2019s source code (Yang et al., 2018; Liu et al.,\n2022).\nThis initial UTG is then dynamically verified\nand refined as the UI Explorer explores the app\n(Line 10 and Line 16). Specifically, let the current\nUTG be G= (U,T,\u03b5) . During UI automation,\nan observed transition (ui,aobs,uj), where ui\u2208U ,\nupdates the graph to G\u2032= (U\u2032,T\u2032,\u03b5\u2032)in one of\nthree ways:\n\u2022Update Edge:An action leads from an existing\nsource node to a target node, but the recorded ac-\ntion does not match the corresponding one in the\nUTG. Formally, if (ui,uj)\u2208T and\u03b5((u i,uj))\u0338=\naobs, the labeling function is updated such that\n\u03b5\u2032((ui,uj)) =a obs, whileU\u2032=U andT\u2032=T .\n\u2022Add Edge:An action connects two existing UI\nnodes, but no corresponding edge exists in the\nUTG. This occurs when uj\u2208U and(ui,uj)/\u2208T .\nA new transition is added by setting T\u2032=T\u222a\n{(u i,uj)}and extending \u03b5\u2032with \u03b5\u2032((ui,uj)) =\naobs.\n\u2022Add Node:An action leads to a UI that is not yet\nin the UTG. If uj/\u2208U, a new node and edge are\nadded to the graph: U\u2032=U\u222a{u j},T\u2032=T\u222a\n{(u i,uj)}, and \u03b5\u2032is extended with \u03b5\u2032((ui,uj)) =\naobs.\nThis approach allows AGENT+P to maintain a\nUTG that is both comprehensive and dynamically\naccurate, combining the breadth of static analysis\nwith the precision of real-time exploration.\n4.2 Node Selector\nA key challenge in AGENT+P is mapping an un-\nstructured, natural"}
{"file": "2510.06042v1.pdf", "section": "4.1 UTG Builder", "chunk": "UTG that is both comprehensive and dynamically\naccurate, combining the breadth of static analysis\nwith the precision of real-time exploration.\n4.2 Node Selector\nA key challenge in AGENT+P is mapping an un-\nstructured, natural language user goal (e.g., \u201cCreate\na playlist\u201d) to a specific, concrete UI state within\nthe app, i.e., utarget (Line 5 of Algorithm 1). Based\non existing techniques that represent UI data from\nsource code, screenshots, or text and convert it\ninto structured formats such as text (Baechler et al.,\n2024), binary vectors (Li et al., 2021), or graphs (Li\net al., 2025a), in AGENT+P, we implement two\ncomplementary strategies for this mapping process:\n1.Embedding-based Matching:We compute se-\nmantic embeddings for the user\u2019s input and for\nthe textual representations of all UI nodes in theAlgorithm 1:Workflow of AGENT+P\nInput:Natural language goalg nl, AppA, Max\nrunning stepsmaxStep\nOutput:Automation outcome:SuccessorFailure\nAliases:UB \u2190UtgBuilder;NS \u2190NodeSelector;PG\n\u2190PlanGenerator;UE\u2190UiExplorer\n1G\u2190UB.buildStaticUTG(A);\n2ucur\u2190getCurrentUI(A);\n3steps\u21900;\n4whilesteps\u2264maxStepdo\n5u target\u2190NS.selectTargetNode(g nl,G);\n6Plan\u2190PG.generatePlan(u cur,utarget ,G);\n7ifPlan is validthen\n8foreach action a iin Plando\n9u next\u2190UE.act(a i);\n10G\u2190UB.update(G,u cur,ai,unext);\n11u cur\u2190u next;\n12else\n13neighbors\u2190PG.getNeighbors(u cur,G);\n14a\u2190UE.decideAction(neighbors,u cur);\n15u next\u2190UE.act(a);\n16G\u2190UB.update(G,(u cur,a,u next));\n17u cur\u2190u next;\n18ifUE.evaluate(g nl))then\n19returnSuccess;\n20steps\u2190steps+1;\n21returnFailure;\nUTG. The node with the highest cosine similar-\nity to the input query is selected as the target.\n2.MLLM-based Identification:We query a Mul-\ntimodal Large Language Model (MLLM) with\nthe user\u2019s goal and representations of candidate\nUIs."}
{"file": "2510.06042v1.pdf", "section": "4.2 Node Selector", "chunk": "highest cosine similar-\nity to the input query is selected as the target.\n2.MLLM-based Identification:We query a Mul-\ntimodal Large Language Model (MLLM) with\nthe user\u2019s goal and representations of candidate\nUIs. The MLLM is prompted to directly identify\nwhich UI screen best corresponds to the user\u2019s\nobjective. The prompt template used for this\nprocess is detailed in Listing 3.\n4.3 Plan Generator\nOnce the target node utarget is identified, the task\nbecomes a classical planning problem: finding the\nshortest sequence of actions from the current UI\nstate to the target state. The Plan Generator or-\nchestrates this by first converting the UTG into the\nPDDL format. It then invokes a classical planner\nto solve for an optimal PDDL plan (Line 6).\nThis symbolic plan is subsequently translated\nback into a sequence of clear, natural language in-\nstructions for the UI Explorer to execute. An exam-\nple of a raw PDDL plan and its corresponding nat-\nural language translation are shown in Appendix in\nListing 4 and Table 5, respectively. In cases where\nthe classical planner fails to find a valid path (e.g.,\nif the target is unreachable), AGENT+P implements\na fallback strategy. Instead of a plan, it generates\na textual summary of the k-hop neighboring nodes\nfrom the current UI, providing contextual informa-\ntion to help the agent decide on its next steps (Line\nTable 3: Statistics of apps used in evaluation.\nApp Tasks Nodes Edges\nVLC 3 85 190\nSimple Calendar Pro 17 24 25\nTasks 6 86 127\nMarkor 14 14 17\nOsmAnd 3 152 508\n12-14).\n4.4 UI Explorer\nThe UI Explorer acts as the AGENT+P\u2019s execution\nengine, emulating user interactions with the app\n(Line 9, Line 14-15, and Line 18-19). It takes\nthe natural language plan and executes each step\nprogrammatically. After each action, it evaluates\nwhether the resulting UI state matches the expected\ngoal.\nA key design feature of the UI Explorer is its\nmodularity. It is a plug-and-play component, al-\nlowing AGENT+P to be integrated with various\ntypes of UI agents, including those based on L"}
{"file": "2510.06042v1.pdf", "section": "4.4 UI Explorer", "chunk": "expected\ngoal.\nA key design feature of the UI Explorer is its\nmodularity. It is a plug-and-play component, al-\nlowing AGENT+P to be integrated with various\ntypes of UI agents, including those based on LLMs,\nMLLMs, or other specialized models that incor-\nporate capabilities like reflection and grounding.\nThis flexibility is demonstrated in our evaluation\n(Section 5), where we integrate AGENT+P with\ndifferent UI agents to showcase its broad applica-\nbility.\n5 Evaluation\n5.1 Evaluation Setup\nDataset.We evaluate our AGENT+P on the An-\ndroidWorld benchmark (Rawles et al., 2024). As\nintroduced in Subsection 2.2, AndroidWorld is a\nwidely acknowledged benchmark for evaluating UI\nagents on real-world tasks and provides automatic\nmetrics to assess an agent\u2019s success rate.\nBaselines.We evaluate AGENT+P with agents\nfrom the official AndroidWorld leaderboard (An-\ndroid World, 2025): Droidrun, MobileUse, and the\nT3A agent included with the benchmark. We select\nthese agents as others are either not open-sourced\nor their released code is not functional. We in-\ntegrate each of these agents as the GUI Explorer\nmodule within AGENT+P for our evaluation.\nImplementation.At the time of our experiments,\nMobileUse claimed a success rate of 0.84, and\nDroidRun claimed 0.78 on the official Android-\nWorld leaderboard. However, the exact configura-\ntions used to obtain these results (e.g., backboneLLMs, enabling reasoning or vision capabilities)\nwere not publicly disclosed. To ensure a fair com-\nparison, we rerun each agent using three state-of-\nthe-art LLMs: GPT-5, Gemini-2.5 Pro, and Grok 4,\nunder multiple parameter configurations. We adopt\nthe configuration with the best performance for our\nevaluation.\nFor the classical planner, we employ Fast Down-\nward with an A\u2217search algorithm to find optimal\npaths. The static UTG is constructed using IC-\nCBot (Yan et al., 2022) and FlowDroid (Arzt et al.,\n2014). To ensure the reliability of our findings, all\nexperiments were conducted three times, and we\nreport the average values.\n5.2 Effectiveness\nTo"}
{"file": "2510.06042v1.pdf", "section": "5.1 Evaluation Setup", "chunk": "., 2022) and FlowDroid (Arzt et al.,\n2014). To ensure the reliability of our findings, all\nexperiments were conducted three times, and we\nreport the average values.\n5.2 Effectiveness\nTo demonstrate the effectiveness of AGENT+P in\nhelping UI agents navigate complex UIs, we con-\nduct an evaluation on a targeted subset of the An-\ndroidWorld benchmark. This subset consists of five\napps where baseline agents most frequently failed,\nas identified in our motivational study in Subsec-\ntion 2.2. Table 3 summarizes the characteristics of\nthese apps, including the number of tasks and the\ncomplexity of their UTGs (nodes and edges).\nAs illustrated in Figure 2, all baseline agents\nachieve higher success rates when integrated with\nAGENT+P compared to running independently.\nSpecifically, AGENT+P enhances the success rate\nof Droidrun by 12%, MobileUse by 2%, and\nT3A by mostly 14%. These gains confirm that\nAGENT+P effectively augments existing UI agents\nby providing them with global transition informa-\ntion of an app\u2019s UIs.\n5.3 Efficiency\nTo evaluate whether AGENT+P enables UI agents\nto reach target UIs more quickly, we design a spe-\ncific navigation task: \u201cGo to the privacy policy\npage\u201d. We select this task for two primary rea-\nsons. (1) It is simple enough for most agents to\naccomplish within a reasonable step budget, usu-\nally three successive correct actions (e.g., from the\nmain UI, click the \u201cAbout\u201d or \u201cSettings\u201d widget,\nscroll down, and then click \u201cPrivacy Policy\u201d). (2)\nthe app design, such as visual implementation of\nthe necessary widgets (e.g., settings icons, menu\nbuttons) varies significantly across apps, posing a\nchallenge for LLM-based agents navigating to the\ntarget UI (i.e., the privacy policy page) without er-\nrors. Therefore, this task provides a fair benchmark\nto compare agents\u2019 abilities to understand diverse\nWithout Agent+PWith Agent+P0.00.20.4Success Rate0.160.28(a) DroidRun\nWithout Agent+PWith Agent+P0.00.20.4Success Rate0.090.11 (b) MobileUse\nWithout Agent+PWith Agent+"}
{"file": "2510.06042v1.pdf", "section": "5.3 Efficiency", "chunk": ".4Success Rate0.160.28(a) DroidRun\nWithout Agent+PWith Agent+P0.00.20.4Success Rate0.090.11 (b) MobileUse\nWithout Agent+PWith Agent+P0.00.20.4Success Rate0.240.38 (c) T3A\nFigure 2: Comparison of agent performance across three baselines.\nTable 4: Comparison of agent performance on the spe-\ncific task \u201cGo to the privacy policy page of the current\napp\u201d.\nSuccess Rate Actions Time (s)\nw/ AGENT+P 0.60 3.80 16.90\nw/o AGENT+P 0.60 6.10 19.90\nUI structures and plan action sequences.\nFor this experiment, we use the Droidrun agent,\nwhich has a higher success rate compared to Mo-\nbileUse, while T3A is coupled only to the Android-\nWorld benchmark. We set a maximum limit of 10\nsteps per task. We compare the performance of\nthe standalone Droidrun agent against Droidrun\nintegrated with AGENT+P on all the apps in An-\ndroidWorld benchmark.\nAs shown in Table 4, both configurations achieve\nthe same success rate, indicating that the task is\ngenerally solvable for a capable agent within the\nstep limit. However, the efficiency gains with\nAGENT+P are substantial: the agent integrated\nwith AGENT+P required fewer steps on average\n(3.8) and less time ( 16.9s) to complete the task\ncompared to the standalone version. The reduc-\ntion in steps ( 37.7% fewer) is more pronounced\nthan the reduction in time, which is expected\nsince AGENT+P introduces additional computa-\ntional overhead for planning. Overall, these results\ndemonstrate that by incorporating global transition\ninformation, AGENT+P enables agents to navigate\nmore directly and complete tasks with higher effi-\nciency.\n6 Discussion\nSymbolic Planning as a Remedy for LLM Hal-\nlucination.Our study demonstrates that integrat-\ning classical symbolic planning with LLM-based\nagents substantially mitigates long-horizon plan-\nning failures. Unlike end-to-end LLM reasoning,\nwhich often suffers from hallucination and my-\nopic exploration, symbolic planners provideglobal"}
{"file": "2510.06042v1.pdf", "section": "6 Discussion", "chunk": "that integrat-\ning classical symbolic planning with LLM-based\nagents substantially mitigates long-horizon plan-\nning failures. Unlike end-to-end LLM reasoning,\nwhich often suffers from hallucination and my-\nopic exploration, symbolic planners provideglobalguaranteeson correctness and optimality. This\nsynergy leverages the complementary strengths of\nboth paradigms: the interpretability and reliability\nof symbolic reasoning, and the perception and lin-\nguistic versatility of LLMs. We believe that such\nhybrid architectures represent a promising direction\nfor future UI automation and broader embodied AI\nresearch.\nBroader Applicability and Generalization.Al-\nthough AGENT+P is designed for Android UI\nautomation, its methodology is domain-agnostic.\nAny environment that can be abstracted as a\nstate\u2013transition graph (e.g., desktop applications,\nweb navigation, robotic task planning) could po-\ntentially benefit from our symbolic-agentic design.\nIn particular, the UTG can be replaced by other\nforms of structured knowledge, such as the Doc-\nument Object Model (DOM) in web automation\nor state machines in robotic control. We antici-\npate that future systems can extend AGENT+P to\noperate across platforms and modalities, forming\na unified symbolic\u2013neural framework for general\nuser-interface reasoning. web-side utg techniques\nsuch as GUITAR (Nguyen et al., 2014).\nMulti-Goal Automation.Our current implemen-\ntation simplifies UI automation tasks into single-\ngoal navigation problems, where the objective is to\nreach a single target UI. However, many practical\nuser tasks are inherently multifaceted and require\nachieving a sequence of sub-goals. For instance,\na task like\u201cAdd an item to the shopping cart and\nthen proceed to checkout\u201dinvolves successfully\nreaching the item\u2019s page (goal 1) and subsequently\nnavigating to the checkout screen (goal 2).\nExtending AGENT+P to handle multi-goal sce-\nnarios would involve evolving the Node Selector\ninto a more sophisticated\u201cGoal Decomposer\u201dca-\npable of parsing a complex natural language in-\nstruction into an ordered list of target UI nodes\n{utarget_1 ,utarget_2 ,...,u target_n}. Subsequently, the\nPlan Generator would leverage the native ability\nof PDDL to support multiple goal predicates, en-\nabling the"}
{"file": "2510.06042v1.pdf", "section": "6 Discussion", "chunk": "an ordered list of target UI nodes\n{utarget_1 ,utarget_2 ,...,u target_n}. Subsequently, the\nPlan Generator would leverage the native ability\nof PDDL to support multiple goal predicates, en-\nabling the generation of a single, cohesive plan that\ntraverses the UTG to satisfy all sub-goals. This ef-\nfectively extends AGENT+P to a hierarchical plan-\nning paradigm. However, this approach also raises\nnew research questions regarding goal ordering and\ndependency resolution, especially when goals be-\ncome infeasible at runtime.\n7 Related Work\n7.1 UI Automation\nTraditionally, research in UI automation has cen-\ntered onautomated testing, where the primary ob-\njective is to systematically explore an app\u2019s UIs to\ndiscover bugs (Ran et al., 2024; Hu and Neamtiu,\n2011; Lai and Rubin, 2019), or security vulnerabil-\nities (Shahriar and Zulkernine, 2009; Moura et al.,\n2023; Liu et al., 2020). With the recent advent\nof LLMs, the focus has expanded totask-driven\nGUI agents, which aim to complete specific real-\nworld tasks rather than maximizing the exploration\ncoverage (Wen et al., 2024; Rawles et al., 2024).\n7.2 LLM+Classical Planner\nStudies using classical planners to enable more re-\nliable planning for LLMs by leveraging LLMs to\neither translate natural language problems into a\nformal language like PDDL for a classical planner\nto solve (Liu et al., 2023; Dagan et al., 2023; Guan\net al., 2023) or to generate an initial plan that the\nplanner then refines (Valmeekam et al., 2023). Bor-\nrowing from this insight, AGENT+P uses a clas-\nsical planner to enhance an LLM\u2019s capability in\nperforming complex GUI tasks.\n7.3 UI Transition Modeling\nExisting works construct UTGs (or similar con-\ncepts) using various methods across multiple plat-\nforms. One primary approach isdynamic analysis,\nwhere the application is executed to observe real\ntransitions. This is seen in early, multi-platform\n(e.g., web, mobile) systems like GUITAR (Nguyen"}
{"file": "2510.06042v1.pdf", "section": "7.3 UI Transition Modeling", "chunk": "across multiple plat-\nforms. One primary approach isdynamic analysis,\nwhere the application is executed to observe real\ntransitions. This is seen in early, multi-platform\n(e.g., web, mobile) systems like GUITAR (Nguyen\net al., 2014) and more recent tools such as GUI-\nXplore (Sun et al., 2025) and Autodroid (Wen et al.,\n2024). The other approach isstatic analysis, which\ninfers control flow from the app\u2019s code, such as\nA3E\u2019s activity transition graph (Azim and Neamtiu,\n2013) and Gator\u2019s window transition graph (Yang\net al., 2018). AGENT+P constructs the UTG to\nmodel UI transitions by synergistically combining\nboth static and dynamic techniques.8 Conclusion\nIn this paper, we introduced AGENT+P, a novel\nagentic framework designed to address the critical\nchallenge of long-horizon planning in UI automa-\ntion. By modeling an app\u2019s transition structure as\na UTG and leveraging an external symbolic plan-\nner, AGENT+P provides LLM-based agents with a\nglobally optimal, high-level plan, effectively miti-\ngating hallucination that causes common automa-\ntion failure. Our evaluation on the AndroidWorld\nbenchmark demonstrates that AGENT+P can be in-\ntegrated as a plug-and-play module to substantially\nimprove the success rates of state-of-the-art UI\nagents. We believe AGENT+P lays the foundation\nfor future research into neuro-symbolic planning\nparadigms for creating more robust and reliable UI\nagents for complex UI automation tasks.\nLimitations\nWhile AGENT+P effectively bridges global plan-\nning with local reasoning, several challenges re-\nmain. First, constructing accurate UTGs still de-\npends on program analysis quality. Static analysis\nmay include infeasible edges, while dynamic ex-\nploration may yield incomplete coverage. Second,\ntranslating between natural language and PDDL\nrepresentations introduces an additional layer of\nabstraction; errors in this translation can propagate\nthrough the pipeline. Finally, classical planners as-\nsume deterministic transitions, yet real-world GUIs\noften contain stochastic behaviors (e.g., pop-ups or\nasync UI updates), which may lead to plan diver-\ngence during execution. Addressing these issues\nrequires tighter integration between the"}
{"file": "2510.06042v1.pdf", "section": "7.3 UI Transition Modeling", "chunk": "planners as-\nsume deterministic transitions, yet real-world GUIs\noften contain stochastic behaviors (e.g., pop-ups or\nasync UI updates), which may lead to plan diver-\ngence during execution. Addressing these issues\nrequires tighter integration between the planner and\nthe environment to support real-time re-planning\nand uncertainty handling.\nEthical Considerations\nAutomated UI agents that can execute complex\ntasks on real applications must be deployed with\ncare. Potential misuse, such as automating sensitive\nor privacy-related operations without explicit con-\nsent, highlights the need for transparent auditing\nand permission control. In our implementation, all\nexperiments are conducted on benign benchmark\napps under controlled environments.\nReferences\nConstructions Aeronautiques, Adele Howe, Craig\nKnoblock, ISI Drew McDermott, Ashwin Ram,\nManuela Veloso, Daniel Weld, David Wilkins Sri,\nAnthony Barrett, Dave Christianson, and 1 others.\n1998. Pddl\u2014the planning domain definition lan-\nguage.Technical Report, Tech. Rep.\nAndroid World. 2025. Leaderboard of AndroidWorld.\nSteven Arzt, Siegfried Rasthofer, Christian Fritz, Eric\nBodden, Alexandre Bartel, Jacques Klein, Yves\nLe Traon, Damien Octeau, and Patrick McDaniel.\n2014. Flowdroid: Precise context, flow, field, object-\nsensitive and lifecycle-aware taint analysis for an-\ndroid apps.ACM sigplan notices, 49(6):259\u2013269.\nTanzirul Azim and Iulian Neamtiu. 2013. Targeted\nand depth-first exploration for systematic testing of\nandroid apps. InProceedings of the 2013 ACM SIG-\nPLAN International Conference on Object Oriented\nProgramming Systems Languages &amp; Applica-\ntions, OOPSLA \u201913, page 641\u2013660, New York, NY ,\nUSA. Association for Computing Machinery.\nGilles Baechler, Srinivas Sunkara, Maria Wang, Fedir\nZubach, Hassan Mansoor, Vincent Etter, Victor\nC\u02d8arbune, Jason Lin, Jindong Chen, and Abhanshu\nSharma. 2024. Screenai: A vision-language model\nfor ui and infographics understanding.arXiv preprint\narXiv:2402.04615"}
{"file": "2510.06042v1.pdf", "section": "7.3 UI Transition Modeling", "chunk": "Jason Lin, Jindong Chen, and Abhanshu\nSharma. 2024. Screenai: A vision-language model\nfor ui and infographics understanding.arXiv preprint\narXiv:2402.04615.\nGautier Dagan, Frank Keller, and Alex Lascarides.\n2023. Dynamic planning with a llm.arXiv preprint\narXiv:2308.06391.\nGaole Dai, Shiqi Jiang, Ting Cao, Yuanchun Li, Yuqing\nYang, Rui Tan, Mo Li, and Lili Qiu. 2025. Advancing\nmobile gui agents: A verifier-driven approach to prac-\ntical deployment.arXiv preprint arXiv:2503.15937.\nGoogle Developers. 2025. Introduction to activities.\nLin Guan, Karthik Valmeekam, Sarath Sreedharan,\nand Subbarao Kambhampati. 2023. Leveraging pre-\ntrained large language models to construct and utilize\nworld models for model-based task planning.Ad-\nvances in Neural Information Processing Systems,\n36:79081\u201379094.\nCuixiong Hu and Iulian Neamtiu. 2011. Automating\ngui testing for android applications. InProceedings\nof the 6th International Workshop on Automation of\nSoftware Test, pages 77\u201383.\nDuling Lai and Julia Rubin. 2019. Goal-driven ex-\nploration for android applications. In2019 34th\nIEEE/ACM International Conference on Automated\nSoftware Engineering (ASE), pages 115\u2013127. IEEE.\nJiawei Li, Jiahao Liu, Jian Mao, Jun Zeng, and Zhenkai\nLiang. 2025a. Ui-ctx: Understanding ui behaviors\nwith code contexts for mobile applications. InNDSS.\nNing Li, Xiangmou Qu, Jiamu Zhou, Jun Wang, Muning\nWen, Kounianhua Du, Xingyu Lou, Qiuying Peng,\nand Weinan Zhang. 2025b. Mobileuse: A gui agent\nwith hierarchical reflection for autonomous mobile\noperation.arXiv preprint arXiv:2507.16853.Toby Jia-Jun Li, Lindsay Popowski, Tom Mitchell, and\nBrad A Myers. 2021. Screen"}
{"file": "2510.06042v1.pdf", "section": "7.3 UI Transition Modeling", "chunk": "gui agent\nwith hierarchical reflection for autonomous mobile\noperation.arXiv preprint arXiv:2507.16853.Toby Jia-Jun Li, Lindsay Popowski, Tom Mitchell, and\nBrad A Myers. 2021. Screen2vec: Semantic embed-\nding of gui screens and gui components. InProceed-\nings of the 2021 CHI Conference on Human Factors\nin Computing Systems, pages 1\u201315.\nBo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu,\nShiqi Zhang, Joydeep Biswas, and Peter Stone.\n2023. Llm+ p: Empowering large language mod-\nels with optimal planning proficiency.arXiv preprint\narXiv:2304.11477.\nChanglin Liu, Hanlin Wang, Tianming Liu, Diandian\nGu, Yun Ma, Haoyu Wang, and Xusheng Xiao. 2022.\nPromal: precise window transition graphs for an-\ndroid via synergy of program analysis and machine\nlearning. InProceedings of the 44th International\nConference on Software Engineering, pages 1755\u2013\n1767.\nChenxu Liu, Zhiyu Gu, Guoquan Wu, Ying Zhang, Jun\nWei, and Tao Xie. 2025. Temac: Multi-agent collabo-\nration for automated web gui testing.arXiv preprint\narXiv:2506.00520.\nTianming Liu, Haoyu Wang, Li Li, Xiapu Luo, Feng\nDong, Yao Guo, Liu Wang, Tegawend\u00e9 Bissyand\u00e9,\nand Jacques Klein. 2020. MadDroid: Characterizing\nand detecting devious ad contents for android apps.\nInProceedings of The Web Conference 2020, pages\n1715\u20131726.\nShang Ma, Chaoran Chen, Shao Yang, Shifu Hou, Toby\nJia-Jun Li, Xusheng Xiao, Tao Xie, and Yanfang Ye.\n2025. Careful about what app promotion ads recom-\nmend! detecting and explaining malware promotion\nvia app promotion graph. InProceedings of the Net-\nwork and Distributed System Security Symposium\n(NDSS).\nThiago Santos de Moura, Everton LG Alves, Hugo\nFeitosa de Figueir\u00eado, and Cl\u00e1udio de Souza Bap-"}
{"file": "2510.06042v1.pdf", "section": "7.3 UI Transition Modeling", "chunk": "of the Net-\nwork and Distributed System Security Symposium\n(NDSS).\nThiago Santos de Moura, Everton LG Alves, Hugo\nFeitosa de Figueir\u00eado, and Cl\u00e1udio de Souza Bap-\ntista. 2023. Cytestion: Automated gui testing for web\napplications. InProceedings of the XXXVII Brazil-\nian Symposium on Software Engineering, pages 388\u2013\n397.\nBao N Nguyen, Bryan Robbins, Ishan Banerjee, and\nAtif Memon. 2014. Guitar: an innovative tool for\nautomated testing of gui-driven software.Automated\nsoftware engineering, 21(1):65\u2013105.\nGraziella Orr\u00f9, Andrea Piarulli, Ciro Conversano, and\nAngelo Gemignani. 2023. Human-like problem-\nsolving abilities in large language models using chat-\ngpt.Frontiers in artificial intelligence, 6:1199350.\nDezhi Ran, Hao Wang, Zihe Song, Mengzhou Wu, Yuan\nCao, Ying Zhang, Wei Yang, and Tao Xie. 2024.\nGuardian: A runtime framework for llm-based ui\nexploration. InProceedings of the 33rd ACM SIG-\nSOFT International Symposium on Software Testing\nand Analysis, pages 958\u2013970.\nChristopher Rawles, Sarah Clinckemaillie, Yifan Chang,\nJonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice\nLi, William Bishop, Wei Li, Folawiyo Campbell-\nAjala, and 1 others. 2024. Androidworld: A dynamic\nbenchmarking environment for autonomous agents.\narXiv preprint arXiv:2405.14573.\nHossain Shahriar and Mohammad Zulkernine. 2009.\nAutomatic testing of program security vulnerabilities.\nIn2009 33rd Annual IEEE International Computer\nSoftware and Applications Conference, volume 2,\npages 550\u2013555. IEEE.\nYuchen Sun, Shanhui Zhao, Tao Yu, Hao Wen, Samith\nVa, Mengwei Xu, Yuanchun Li, and Chongyang\nZhang. 2025. Gui-xplore: Empowering generaliz-\nable gui agents with one exploration. InProceedings\nof the Computer Vision and Pattern Recognition Con-\nference, pages 19477\u201319486"}
{"file": "2510.06042v1.pdf", "section": "7.3 UI Transition Modeling", "chunk": "and Chongyang\nZhang. 2025. Gui-xplore: Empowering generaliz-\nable gui agents with one exploration. InProceedings\nof the Computer Vision and Pattern Recognition Con-\nference, pages 19477\u201319486.\nAntti Valmari. 1996. The state explosion problem.\nInAdvanced Course on Petri Nets, pages 429\u2013528.\nSpringer.\nKarthik Valmeekam, Matthew Marquez, Sarath Sreed-\nharan, and Subbarao Kambhampati. 2023. On the\nplanning abilities of large language models-a criti-\ncal investigation.Advances in Neural Information\nProcessing Systems, 36:75993\u201376005.\nHui Wei, Zihao Zhang, Shenghua He, Tian Xia, Shijia\nPan, and Fei Liu. 2025. Plangenllms: A modern\nsurvey of llm planning capabilities.arXiv preprint\narXiv:2502.11221.\nHao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao,\nTao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu,\nYaqin Zhang, and Yunxin Liu. 2024. Autodroid: Llm-\npowered task automation in android. InProceedings\nof the 30th Annual International Conference on Mo-\nbile Computing and Networking, pages 543\u2013557.\nFrank Wilcoxon. 1945. Individual comparisons by rank-\ning methods.Biometrics bulletin, 1(6):80\u201383.\nZhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang,\nQiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen\nDing, Liheng Chen, Paul Pu Liang, and 1 others. Os-\natlas: A foundation action model for generalist gui\nagents, 2024c.\nYuquan Xie, Zaijing Li, Rui Shao, Gongwei Chen, Kai-\nwen Zhou, Yinchuan Li, Dongmei Jiang, and Liqiang\nNie. 2025. Mirage-1: Augmenting and updating\ngui agent with hierarchical multimodal skills.arXiv\npreprint arXiv:2506.10387.\nJiwei Yan, Shixin Zhang,"}
{"file": "2510.06042v1.pdf", "section": "7.3 UI Transition Modeling", "chunk": "iang\nNie. 2025. Mirage-1: Augmenting and updating\ngui agent with hierarchical multimodal skills.arXiv\npreprint arXiv:2506.10387.\nJiwei Yan, Shixin Zhang, Yepang Liu, Jun Yan, and Jian\nZhang. 2022. Iccbot: fragment-aware and context-\nsensitive icc resolution for android applications. In\nProceedings of the ACM/IEEE 44th international\nconference on software engineering: companion pro-\nceedings, pages 105\u2013109.\nShengqian Yang, Haowei Wu, Hailong Zhang, Yan\nWang, Chandrasekar Swaminathan, Dacong Yan, and\nAtanas Rountev. 2018. Static window transition\ngraphs for android.Automated Software Engineering,\n25:833\u2013873.Jiabo Ye, Xi Zhang, Haiyang Xu, Haowei Liu, Jun-\nyang Wang, Zhaoqing Zhu, Ziwei Zheng, Feiyu Gao,\nJunjie Cao, Zhengxi Lu, and 1 others. 2025. Mobile-\nagent-v3: Foundamental agents for gui automation.\narXiv preprint arXiv:2508.15144.\nChi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin\nChen, Zebiao Huang, Bin Fu, and Gang Yu. 2023.\nAppagent: Multimodal agents as smartphone users.\narXiv preprint arXiv:2312.13771.\nKangjia Zhao, Jiahui Song, Leigang Sha, Haozhan Shen,\nZhi Chen, Tiancheng Zhao, Xiubo Liang, and Jianwei\nYin. 2024. Gui testing arena: A unified benchmark\nfor advancing autonomous gui testing agent.arXiv\npreprint arXiv:2412.18426.\nA Additional Figures and Tables\nListing 2: Example problem PDDL for the Simple Cal-\nendar Pro app with the taskChange the time zone.\n1( define ( problem change -time - zone )\n2(: domain utg - automation )\n3\n4(: objects\n5SplashActivity - node\n6MainActivity - node\n7EventActivity - node\n8SettingsActivity - node\n9AboutActivity - node\n10TaskActivity - node\n11SelectTimeZoneActivity - node\n12ManageEvent"}
{"file": "2510.06042v1.pdf", "section": "7.3 UI Transition Modeling", "chunk": "4(: objects\n5SplashActivity - node\n6MainActivity - node\n7EventActivity - node\n8SettingsActivity - node\n9AboutActivity - node\n10TaskActivity - node\n11SelectTimeZoneActivity - node\n12ManageEventTypesActivity - node\n13WidgetListConfigureActivity - node\n14ContributorsActivity - node\n15FAQActivity - node\n16LicenseActivity - node\n17)\n18\n19(: init\n20(at SplashActivity )\n21\n22(goal - node SelectTimeZoneActivity )\n23\n24( connected SplashActivity MainActivity )\n25( connected MainActivity EventActivity )\n26( connected MainActivity SettingsActivity )\n27( connected MainActivity AboutActivity )\n28( connected MainActivity TaskActivity )\n29( connected EventActivity EventActivity )\n30( connected EventActivity SelectTimeZoneActivity )\n31( connected SettingsActivity\nManageEventTypesActivity )\n32( connected SettingsActivity\nWidgetListConfigureActivity )\n33( connected AboutActivity ContributorsActivity )\n34( connected AboutActivity FAQActivity )\n35( connected AboutActivity LicenseActivity )\n36( connected TaskActivity TaskActivity )\n37)\n38\n39(: goal\n40(goal - achieved SelectTimeZoneActivity )\n41)\n42)\nListing 3: Prompt template of Node Selector.\n1Given the user goal : \"{ user_goal }\".\n2\n3Select the most relevant UTG nodes to achieve this\ngoal .\n4\n5Available UTG nodes :\n6{ chr (10) . join ( nodes_information )}\n7\n8Return your response in JSON format :\n9{{\n10\" nodes \": [\" node1 \", \" node2 \", ...] ,\n11\" confidence \": 0.8 ,\n12\" reasoning \": \" brief explanation \"\n13}}\n14\n15For example ,\n16Available UTG nodes are :\n17\n18\n19Return ONLY the JSON , no other text .Listing 4: The plan generated by Fast Downward A\u2217for\nthe Simple Calendar Pro app with the taskAdd a new\nevent type named \u20191-on-1 meeting\u2019.\n1( navigate SplashActivity MainActivity )\n2( navigate MainActivity SettingsActivity )\n3( navigate SettingsActivity ManageEventTypesActivity )\n4; cost = 3 ( unit cost )\nEventActivitySelectTimeZoneActivity\nMainActivitySettingsActivity\nTaskActivityAboutActivityManageEventTypesActivity\nWidgetListCon\ufb01gureActivity\nSplashActivity\nContributorsActivity\nFAQActivity\nLicenseActivityFigure 3: Graphviz visualization of the UTG of Simple Calendar Pro in AndroidWorld. An activity is an"}
{"file": "2510.06042v1.pdf", "section": "7.3 UI Transition Modeling", "chunk": "ActivityManageEventTypesActivity\nWidgetListCon\ufb01gureActivity\nSplashActivity\nContributorsActivity\nFAQActivity\nLicenseActivityFigure 3: Graphviz visualization of the UTG of Simple Calendar Pro in AndroidWorld. An activity is an unit of\nAndroid UI (Google Developers, 2025). Edge labels are removed for visual clarity.\nTable 5: Example natural language prompt for the Simple Calendar Pro app with the taskAdd a new event type\nnamed \u20191-on-1 meeting\u2019.\nCategory Content\nNatural\nLanguage\nInstructions\u2014 UTG Navigation Guide \u2014\nCurrent UI: MainActivity\n\u2014 NA VIGATION PLAN FOR YOUR GOAL \u2014\nGoal Analysis: Based on your goal, the system identified these target destinations:\n\u2022 ManageEventTypesActivity\nConfidence: 95%\nOPTIMAL PATH (Follow these steps in order):\nStep 1: Navigate from MainActivity to SettingsActivity\nStep 2: Navigate from SettingsActivity to ManageEventTypesActivity\nIMMEDIATE NEXT ACTION:\n\u2192on click the ImageView widget with content-description \u201cmore options\u201d via API call\n\"virtualinvoke r0.<android.content.Context: void startActivity(android.content.Intent)>(r1)\u2019 ()\"\nThis will take you to: SettingsActivity\nTotal steps in optimal path: 2\nUsage TipsThis path was computed using PDDL planning for guaranteed optimality\n\u2014 USAGE TIPS \u2014\n\u2022 If a NA VIGATION PLAN is shown above, follow it step by step for optimal path\n\u2022 The plan was computed using formal PDDL planning algorithms\n\u2022 If no plan exists, the target is unreachable from current location\n\u2022 Some UI elements may not be visible - scroll if needed\nFallback\u2014 ALL A V AILABLE NA VIGATION OPTIONS FROM HERE \u2014\n\u2022 TO REACH: EventActivity\n\u2192on click ImageButton widget with context-description \u201cNew Event\u201d via API call\n\"virtualinvoke r0.<android.content.Context: void startActivity(android.content.Intent)>(r1)\u2019 ()\"\n\u2022 TO REACH: SettingsActivity\n\u2192on click Button widget with content-description \u201cSettings\u201d via API call\n\"virtualinvoke r0.<android.content.Context: void startActivity(android.content.Intent)>(r1)\u2019 ()\"\n\u2022 TO REACH: TaskActivity\n\u2192on click ImageButton widget via api call \"\u2019virtualinvoke\nr2.<android.content.Context: void startActivity(android.content.Intent)>(\nr3)\u2019 ()\"\n\u2014 End Navigation Guide \u2014"}
{"file": "2510.06042v1.pdf", "section": "7.3 UI Transition Modeling", "chunk": "TO REACH: TaskActivity\n\u2192on click ImageButton widget via api call \"\u2019virtualinvoke\nr2.<android.content.Context: void startActivity(android.content.Intent)>(\nr3)\u2019 ()\"\n\u2014 End Navigation Guide \u2014"}
{"file": "2510.06534v1.pdf", "section": "Introduction", "chunk": "BENEFICIALREASONINGBEHAVIORS INAGENTICSEARCH AND\nEFFECTIVEPOST-TRAINING TOOBTAINTHEM\nJiahe Jin Abhijay Paladugu Chenyan Xiong\nLanguage Technologies Institute, Carnegie Mellon University\n{jjiahe,apaladug,cx}@andrew.cmu.edu\nABSTRACT\nAgentic search leverages large language models (LLMs) to interpret complex user information needs\nand execute a multi-step process of planning, searching, and synthesizing information to provide\nanswers. This paradigm introduces unique challenges for LLMs\u2019 reasoning and agentic capabilities\nwhen interacting with retrieval systems and the broader web. In this paper, we propose a reasoning-\ndriven LLM-based pipeline to study effective reasoning behavior patterns in agentic search. Using\nthis pipeline, we analyze successful agentic search trajectories and identify four beneficial reasoning\nbehaviors: Information Verification, Authority Evaluation, Adaptive Search, and Error Recovery.\nBased on these findings, we propose a technique calledBehavior Primingto train more effective\nagentic search models. It synthesizes agentic search trajectories that exhibit these four behaviors\nand integrates them into the agentic search model through supervised fine-tuning (SFT), followed\nby standard reinforcement learning (RL). Experiments on three benchmarks (GAIA, WebWalker,\nand HLE) demonstrate that behavior priming yields over 35% gains in Llama3.2-3B and Qwen3-\n1.7B compared to directly training agentic search models with RL. Crucially, we demonstrate that\nthe desired reasoning behaviors in the SFT data, rather than the correctness of the final answer, is\nthe critical factor for achieving strong final performance after RL: fine-tuning on trajectories with\ndesirable reasoning behaviors but incorrect answers leads to better performance than fine-tuning\non trajectories with correct answers. Our analysis further reveals the underlying mechanism: the\nintroduced reasoning behaviors endow models with more effective exploration (higher pass@k and\nentropy) and test-time scaling (longer trajectories) capabilities, providing a strong foundation for RL.\nOur code will be released as open source.\n1 Introduction\nAgentic search [Jin et al., 2025a, Zheng et al., 2025, Li et al., 2025a, Moonshot AI, 2025] represents a new search\nparadigm in which search systems"}
{"file": "2510.06534v1.pdf", "section": "1 Introduction", "chunk": "entic search [Jin et al., 2025a, Zheng et al., 2025, Li et al., 2025a, Moonshot AI, 2025] represents a new search\nparadigm in which search systems leverage large language models (LLMs) to perform multi-step agentic actions to\ninvoke search tools for complex user information needs. This requires the reasoning process of decomposing tasks\ninto sub-queries, adapting search strategies dynamically, analyzing search results, and synthesizing the final answer.\nCommercial agentic search systems such as ChatGPT\u2019s Deep Research and Google Search\u2019s AI Mode have significantly\nenhanced user experiences and rapidly gained adoption [Zhou and Li, 2024, Business Insider, 2025, Verge, 2025]. The\nacademic and open-source communities have also made notable progress, especially in applying reinforcement learning\n(RL) to train more capable agentic search models [Jin et al., 2025b, Zheng et al., 2025].\nThe core enabler of agentic search is LLMs\u2019 reasoning ability for this agentic scenario. LLMs\u2019 reasoning abilities\nare mainly developed through large-scale post-training on reasoning-oriented tasks such as mathematics, coding, and\nscientific reasoning [DeepSeek-AI et al., 2025, Yang et al., 2025], and the reasoning patterns demonstrated by LLMs\nare crucial for their performance improvement through RL [Yeo et al., 2025, Gandhi et al., 2025]. For instance,\nbehaviors like verification, subgoal setting, backtracking, and backward chaining in initial models are crucial for\neffective RL training on math tasks Gandhi et al. [2025]. However, the reasoning behaviors beneficial for addressing\nchallenges unique to agentic search\u2014such as handling noisy search results or adapting strategies based on conflicting\ninformation\u2014remain uncertain.arXiv:2510.06534v1  [cs.AI]  8 Oct 2025\nA PREPRINT\nIn this paper, we first design an automatic LLM-based pipeline to study effective reasoning behaviors in agentic\nsearch. We collect the agentic search trajectories from multiple LLMs, and employ a reasoning LLM to analyze them,\nextracting behaviors that distinguish successful trajectories from failed ones. Through this process, we identify four\nreasoning behaviors critical for agentic search:Information Verification(validating results across sources),"}
{"file": "2510.06534v1.pdf", "section": "1 Introduction", "chunk": "LLMs, and employ a reasoning LLM to analyze them,\nextracting behaviors that distinguish successful trajectories from failed ones. Through this process, we identify four\nreasoning behaviors critical for agentic search:Information Verification(validating results across sources),Authority\nEvaluation(assessing reliability and resolving conflicts),Adaptive Search(modifying strategies dynamically), and\nError Recovery(detecting and correcting mistakes). The first two behaviors address challenges unique to information\nretrieval, while the latter two are fundamental for multi-step planning across all agentic tasks. We find that the frequency\nof these behaviors in a model\u2019s reasoning process strongly correlates with its performance in agentic search tasks across\nmultiple LLMs with varying capabilities.\nBuilding on these findings, we propose Behavior Priming, a technique that systematically instills these beneficial\nreasoning behaviors into agentic search models. We first curate a collection of trajectories that demonstrate these\ndesired behaviors by selecting from a large corpus of agentic search trajectories generated by LLMs. These curated\ntrajectories serve as supervision data to explicitly inject desired reasoning behaviors by fine-tuning models. After this,\nwe train the behavior-primed models with a standard reinforcement learning setup.\nTo investigate the impact of these reasoning behaviors on RL training, we designed a series of comparative experiments.\nWe compare our Behavior Priming method with (1) directly training base models using RL without the fine-tuning phase,\nas well as other common SFT-then-RL methods for training agentic models, including (2) fine-tuning on randomly\nselected trajectories distilled from a stronger model before RL, and (3) fine-tuning on trajectories with correct final\nanswers before RL. Experiment results demonstrate that Behavior Priming leads to significantly greater performance\nthan these methods. Compared with directly applying RL on the non-primed models, behavior priming boosted the\nfinal average performance of both Qwen3-1.7B and Llama-3.2-3B-Instruct by over 35% across three benchmarks\n(GAIA [Mialon et al., 2023], WebWalkerQA [Wu et al., 2025a], and HLE [Phan et al., 2025a]). Behavior priming also\nleads to better performance than other common SFT-then-RL methods that simply fine-tune on trajectories distilled\nfrom a stronger model or trajectories with correct final answers.\nCrucially, we"}
{"file": "2510.06534v1.pdf", "section": "1 Introduction", "chunk": "., 2025a]). Behavior priming also\nleads to better performance than other common SFT-then-RL methods that simply fine-tune on trajectories distilled\nfrom a stronger model or trajectories with correct final answers.\nCrucially, we conducted a targeted ablation study to disentangle the impact of reasoning behaviors from outcome\ncorrectness. We fine-tuned models on two sets of trajectories that both exhibited desirable reasoning behaviors: one set\nled to incorrect final answers, while the other led to correct ones. Both models were then trained with the same standard\nRL settings. While the initial performance after SFT was lower for the model trained on incorrect-answer trajectories,\nit ultimately achieved comparable performance with the correct-answer one after the subsequent RL training. This\nsurprising result strongly highlights the primacy of reasoning behaviors over outcome correctness for unlocking a\nmodel\u2019s potential through RL.\nOur analysis further clarifies the underlying mechanism. During the SFT phase of behavior priming, the frequency\nof all four behaviors in the model\u2019s reasoning process increases significantly, accompanied by a notable rise in both\npass@k accuracy and the average number of steps per trajectory. This establishes a robust foundation for exploration\nand effective test-time scaling for the subsequent RL training. During the subsequent RL phase, behavior-primed\nmodels maintain a high level of policy entropy, whereas models without priming start with lower entropy that declines\nsteeply, leading to premature policy convergence. Moreover, these non-primed models fail to cultivate the essential\nbehaviors during RL endogenously.\nIn summary, our key contributions are as follows:\n1.We identify four beneficial reasoning behaviors for agentic search by comparing and analyzing agentic\ntrajectories from multiple models with an LLM-based pipeline.\n2.We propose Behavior Priming, a method that instills these beneficial behaviors into models via SFT to enable\nhigher performance in the subsequent RL training phase.\n3.We empirically demonstrate that Behavior Priming significantly unlocks a model\u2019s potential in RL, enabling\nhigher final performance by establishing a robust foundation for exploration and test-time scaling capabilities.\n2 Related Work\nAgentic search is an emerging search paradigm where LLM-based systems autonomously and iteratively use web-related\ntools to gather external information for solving complex, fact-seeking tasks Xu and Peng [2025], OpenAI [2024],\nAnthropic [2025]. The development of agentic search systems can be broadly categorized into two approaches. The first"}
{"file": "2510.06534v1.pdf", "section": "2 Related Work", "chunk": "tools to gather external information for solving complex, fact-seeking tasks Xu and Peng [2025], OpenAI [2024],\nAnthropic [2025]. The development of agentic search systems can be broadly categorized into two approaches. The first\ninvolves multi-agent collaboration systems Alzubi et al. [2025], GPTResearcher [2025], Li et al. [2025b], Zhang et al.\n[2025] within a meticulously designed, pre-defined workflow. The second approach focuses on single-agent, end-to-end\nsystems Zheng et al. [2025], Jin et al. [2025a], Nguyen et al. [2025] where a single underlying LLM iteratively invokes\n2\nA PREPRINT\nweb search-related tools based on the context of previous steps. Research on agentic search training has predominantly\nconcentrated on the latter one for its simplicity.\nInfluenced by the success of Reinforcement Learning in reasoning tasks like mathematics DeepSeek-AI et al. [2025],\nJaech et al. [2024], many studies have adopted reinforcement learning for training agentic search models Jin et al.\n[2025a], Zheng et al. [2025], Li et al. [2025a], Moonshot AI [2025]. However, due to the scarcity of data that\ncontains tool use and real-world interaction, the foundational reasoning ability of LLMs for agentic tasks are relatively\nunderdeveloped Tao et al. [2025], Li et al. [2025c], Shi et al. [2025]. Consequently, a common way for training search\nagents with RL require a \"cold-start\" instruction-tuning phase Tao et al. [2025], Li et al. [2025a], Moonshot AI [2025]\nbefore applying RL to familiarize the model with the task. However, the specific abilities instilled during this phase and\ntheir impact on the subsequent RL stage requires further investigation.\nReinforcement learning has emerged as a powerful paradigm for enhancing complex reasoning in language models,\nwith the notable breakthrough of Deepseek-R1 DeepSeek-AI et al. [2025] demonstrating its ability to achieve significant\nimprovement in mathematics, coding, and scientific reasoning tasks. However, models\u2019 potential of improvement in RL\nhas a strong correlation with their initial characteristics. Certain models, notably the Qwen 2.5 series Yang et"}
{"file": "2510.06534v1.pdf", "section": "2 Related Work", "chunk": "to achieve significant\nimprovement in mathematics, coding, and scientific reasoning tasks. However, models\u2019 potential of improvement in RL\nhas a strong correlation with their initial characteristics. Certain models, notably the Qwen 2.5 series Yang et al. [2025],\nhave been widely shown to achieve substantial gains with RL. In contrast, some other models fail to show similar\nimprovement. Recent works Yeo et al. [2025], Wu et al. [2025b], Liu et al. [2025], Setlur et al. [2025] have shown that\nthe base model\u2019s ability to explore and extrapolate computation at test-time is pivotal for effective improvement in RL\ntraining. Models that benefit significantly from RL usually already exhibit some key behaviors such as self-reflection and\nverification before the RL training Gandhi et al. [2025], Liu et al. [2025], Yeo et al. [2025]. Furthermore, instilling these\nabilities through supervised-finetuning on models that initially lack them is effective in raising their performance ceiling\nin RL Yeo et al. [2025], Liu et al. [2025], Gandhi et al. [2025]. However, relevant research has been predominantly\nconcentrated on the mathematical domain. For broader applications, such as real-world agentic tasks, the critical\ncapabilities required for successful RL, and the methods to acquire them, remain open questions. In this paper, we\nexplore the nature of this gap by identifying the beneficial reasoning patterns for agentic search and exploring methods\nto obtain them.\n3 Identifying Beneficial Behaviors in Agentic Search\nTo identify beneficial behaviors in agentic search, we first develop a standard agentic search framework to enable our\nstudy across different LLMs, and then design an LLM-based analytical pipeline to systematically discover the key\nreasoning patterns from agentic search trajectories. We also validate the importance of these behaviors by investigating\nthe correlation between their frequency in trajectories from various LLMs and the final task performance.\n3.1 Standard Agenic Search Framework\nTo facilitate our study across different LLMs, we simplified the agent framework in [Chandrahasan et al., 2025] to\nbuild a standard end-to-end agentic search framework that easily integrates various LLMs as the underlying model,\nwith prompts shown in Appendix A.1. This standardized approach ensures our analysis focuses"}
{"file": "2510.06534v1.pdf", "section": "3.1 Standard Agenic Search Framework", "chunk": "andrahasan et al., 2025] to\nbuild a standard end-to-end agentic search framework that easily integrates various LLMs as the underlying model,\nwith prompts shown in Appendix A.1. This standardized approach ensures our analysis focuses on the model\u2019s core\ncapabilities rather than framework-specific artifacts. This framework operates in an iterative process: at step k, the\nmodel receives an input xk, which consists of a system prompt, user query, and the accumulated history context ctxk. It\nthen produces an output yk=< t k, ak>, where tkis the thinking process and akis the action. The entire interaction is\ncaptured as a trajectoryT= (\u27e8x 1, y1\u27e9,\u27e8x 2, y2\u27e9, . . . ,\u27e8x n, yn\u27e9).\nThe model\u2019s action akat each step is selected from a predefined set of three actions defined in its system prompt. Each\naction has a distinct effect on the iterative process:\n\u2022search: The model queries an external search API and receives search results as an observation,obs k.\n\u2022summary : The model condenses the current context to manage context length. This summary will replace the\nentire context for the next step.\n\u2022answer: The process terminates, anda kis provided as the final answer.\nThe history context ctxkserves as the model\u2019s memory, providing the necessary context from previous steps. After\neach step, the history context is updated based on the action taken. If the model chooses to perform a search action, its\nentire output ykand the search result obskwill be appended to the context. If the model performs a summary action, it\ncondenses the important information from the current context, and this summary is used as the context for the next\nstep. The summary action is introduced to prevent the context from becoming too long, thus making the framework\ncompatible with models that have different context length limits, and it also allows the model to explicitly reflect on\n3\nA PREPRINT\nprevious steps to help its decision in subsequent steps. Formally, the context is updated as:\nctxk+1 =(\nctxk+yk+obs k,ifa k=search\nak,ifa k=summary\n3.2 Identify Beneficial Reasoning Behaviors For Agentic Search\nTo identify beneficial reasoning behaviors, we leveraged our standardized agent framework to conduct a comparative\nanalysis of model"}
{"file": "2510.06534v1.pdf", "section": "3.1 Standard Agenic Search Framework", "chunk": "obs k,ifa k=search\nak,ifa k=summary\n3.2 Identify Beneficial Reasoning Behaviors For Agentic Search\nTo identify beneficial reasoning behaviors, we leveraged our standardized agent framework to conduct a comparative\nanalysis of model trajectories. Specifically, we separately integrated a strong model (Gemini 2.5 Flash) and a weak\nmodel (Qwen3-1.7B) into the framework, and tasked them with answering an identical set of questions. After evaluation,\nwe extract a set of questions where Gemini 2.5 Flash succeeded but Qwen3-1.7B failed. This approach allows for a\ncontrolled comparison of the models\u2019 reasoning behaviors when faced with the same problem and operating within the\nsame agent framework, thereby isolating the key differences that lead to divergent outcomes.\nInspired by an automated rule generation method [Wang and Xiong, 2025], we developed an automatic LLM-based\npipeline to identify effective reasoning behaviors that lead to the successful completion of agentic search tasks. Prompts\ncan be found in Appendix A.2. Our process involved three steps. First, we prompted a reasoning LLM to analyze\npairs of successful and failed trajectories for individual questions. The LLM was provided with the initial question\nand the trajectory pair, and instructed to analyze why one attempt succeeded while the other failed. Second, we\ngathered the analyses for 200 randomly selected trajectory pairs and prompted the LLM to extract the key reasoning\nbehaviors observed in the successful trajectories. Finally, we tasked the LLM with consolidating and deduplicating\nthese summaries to produce the final set of common behaviors. We then manually reviewed these behaviors to confirm\nthat the identified behaviors were indeed recurring patterns across multiple tasks.\nThrough this analysis pipeline, we identified four essential reasoning behaviors for agentic search. These can be divided\ninto two main categories: the first two focus oninformation retrieval, and the latter two are more general strategies for\nthemulti-step agentic planningability. These behaviors are:Information Verification, which involves validating search\nresults across multiple sources, performing cross-referencing, and explicitly citing evidence in the reasoning process;\nAuthority Evaluation, the ability to identify conflicts among different search results and analyze source credibility to\nprioritize the most authoritative information;Adaptive Search, the practice of dynamically modifying search strategies\nbased on previous outcomes; andError Recovery, the capacity for recognizing and correcting mistakes"}
{"file": "2510.06534v1.pdf", "section": "3.2 Identify Beneficial Reasoning Behaviors For Agentic Search", "chunk": "the ability to identify conflicts among different search results and analyze source credibility to\nprioritize the most authoritative information;Adaptive Search, the practice of dynamically modifying search strategies\nbased on previous outcomes; andError Recovery, the capacity for recognizing and correcting mistakes made in prior\nsteps. The following box provides examples of these behaviors in Gemini-2.5-Flash\u2019s trajectories.\nExamples of four behaviors\nInformation Verification\u2014 Cross-source confirmation and exact-quote checking:\n\u201cMy task is clear: verify if the quoted text exactly matches Greetham\u2019s article. . . accuracy is paramount; I\u2019ll use\n\u2018uncoupled\u2019, \u2018authors\u2019, \u2018mis-transmission\u2019, and \u2018veil\u2019 to zero in on the relevant section. \u201d\nAuthority Evaluation\u2014 Narrow focus to official data source:\n\u201cI\u2019m aiming for the USGS\u2019s own reports or databases, like the \u2018Nonindigenous Aquatic Species\u2019 page, to get the\nmost reliable data. \u201d\nAdaptive Search\u2014 Methodical refinement based on previous results:\n\u2018The search engine may not have indexed the quote perfectly, or the user\u2019s quote may differ slightly. . . I\u2019ll refine\nmy strategy. \u201d\nError Recovery\u2014 Recognize previous mistake and correct:\n\u201cI realize I added an irrelevant keyword, \"Teresa Teng\" to my previous search... I should remove it as it is\nunrelated to my task. \u201d\n3.3 Validate the Importance of Beneficial Behaviors\nWe then developed an automatic LLM-based method to measure the frequency of these four behaviors in agent\ntrajectories. Specifically, we provide the full agentic search trajectories to an LLM and prompt it as an automated\nevaluator to identify the presence of each behavior within these trajectories. Detail prompts are provided in Appendix A.3.\nWe definebehavior frequencyas the proportion of trajectories that exhibit a specific behavior. Using this analysis\nframework, we deployed Gemini 2.5 Flash, DeepSeekR1, Llama3.2-3B-Instruct, and Qwen3-1.7B in our agent\nframework and evaluated their behavior frequency on three widely accepted benchmarks for agenic search.\nAs illustrated in Figure 1, results across different models demonstrate a strong correlation between model performance\nand the frequency of these beneficial behaviors. The performance ranking of the four models directly corresponds to\n4\nA PREPRINT\nFigure 1: Comparison of different LLMs as the underlying agentic search model of our agent framework"}
{"file": "2510.06534v1.pdf", "section": "3.3 Validate the Importance of Beneficial Behaviors", "chunk": "between model performance\nand the frequency of these beneficial behaviors. The performance ranking of the four models directly corresponds to\n4\nA PREPRINT\nFigure 1: Comparison of different LLMs as the underlying agentic search model of our agent framework across four\nbenchmarks. (a): the frequency of four behaviors in trajectories. (b): scores on benchmarks.Abbreviations:IV=\nInformation Verification,AE= Authority Evaluation,AS= Adaptive Search,ER= Error Recovery.\ntheir ranking in behavior frequency. This result across different model families and model sizes strongly validates the\nimportance of these behaviors for successful agentic search.\n4 Behavior Priming: Unlocking the RL Potential for Agentic Search\n4.1 Instilling Behaviors via Supervised Fine-tuning\nHaving identified the essential behaviors for agentic search, we now investigate the effect of instilling these behaviors\non model performance during subsequent reinforcement learning. The common SFT-then-RL approach for training\nagentic search models typically fine-tunes them on trajectories from strong models for distillation or on trajectories that\nyield a correct outcome. However, we hypothesize thatthe underlying critical factor enabling these methods is the\nreasoning behavior within these trajectories. Therefore, a more effective fine-tuning strategy should directly focus on\nselecting trajectories with the desired reasoning behaviors, a method we term \u201cBehavior Priming\u201d.\nTo test this hypothesis, we curated a suite of specialized datasets. First, we generated a large corpus of trajectories\nusing Gemini 2.5 Flash, a strong model in agentic search. Each trajectory in this corpus was then analyzed for two\ncriteria: the correctness of its final answer and the presence of the four beneficial behaviors in the thinking process of\nintermediate steps. From this corpus, we filtered and created several equally-sized datasets:\n\u2022SFT (Random):Trajectories randomly selected from the corpus, serving as an SFT distillation baseline.\n\u2022SFT (Correct):Trajectories with a correct final answer, irrespective of the behaviors exhibited.\n\u2022Behavior Prime:Trajectories exhibiting all four beneficial behaviors, regardless of the final outcome.\n\u2022Behavior Prime (Incorrect):Trajectories exhibiting all four behaviors but resulting in incorrect final answer.\n\u2022Behavior Prime (Correct):Trajectories exhibiting all four behaviors and resulting in correct final answer.\nSince we generated the corpus with a capable model that naturally has these behavior patterns, the trajectories in the\nSFT (Random)andSFT (Correct"}
{"file": "2510.06534v1.pdf", "section": "4.1 Instilling Behaviors via Supervised Fine-tuning", "chunk": "\u2022Behavior Prime (Correct):Trajectories exhibiting all four behaviors and resulting in correct final answer.\nSince we generated the corpus with a capable model that naturally has these behavior patterns, the trajectories in the\nSFT (Random)andSFT (Correct)datasets also contain a certain number of behaviors. They serve as strong baselines,\nallowing us to measure the added value of our targeted curation methods. TheBehavior Primedataset is the unique\ncase constructed by our method. Although the trajectories in this dataset do not always yield the correct answer, all\ntrajectories contain the four behaviors, thus allowing for the concentrated instillation of these reasoning behaviors.\nTo disentangle the influence of the reasoning process from the final outcome, we use theBehavior Prime (Incorrect)\ndataset to verify that the reasoning behavior itself, rather than a correct outcome, is the key to providing a strong\nfoundation for subsequent RL training. In parallel, theBehavior Prime (Correct)dataset was created to assess the\nsynergistic effect of combining a high-quality reasoning process with a successful outcome.\nWith the datasets prepared, we instill the target behaviors into a base model via supervised fine-tuning. We train models\non multi-step trajectories by treating each step as an independent training sample. Formally, given a trajectory dataset\nDT={T i}N\ni=1, where each trajectory is a sequence Ti= (\u27e8xi\n1, yi\n1\u27e9,\u27e8xi\n2, yi\n2\u27e9, . . . ,\u27e8xi\nLi, yi\nLi\u27e9)with step number Li, the\ndataset for SFTD SFT is constructed by aggregating all input-output pairs from these trajectories:\nDSFT={\u27e8xi\nk, yi\nk\u27e9 | Ti\u2208 DT,1\u2264k\u2264L i}\n5\nA PREPRINT\nSince we use each step as an individual training sample rather than concatenating the full trajectory, it is unnecessary\nto mask the loss on environment observations. The model \u03c0\u03b8is trained on DSFT with the standard loss function for\nautoregressive sequence generation:\nLSFT(\u03b8) =E \u27e8x,y\u27e9\u223cD SFT\uf8ee\n\uf8f0\u2212|y|X\nj=1log\u03c0 \u03b8(yj|y<j, x)\uf8f9\n\uf8fb\n4.2 Reinforcement Learning\nAfter the SFT phase, each of"}
{"file": "2510.06534v1.pdf", "section": "4.1 Instilling Behaviors via Supervised Fine-tuning", "chunk": "\ufffd\ufffd\n\uf8f0\u2212|y|X\nj=1log\u03c0 \u03b8(yj|y<j, x)\uf8f9\n\uf8fb\n4.2 Reinforcement Learning\nAfter the SFT phase, each of these SFT-checkpoints, along with the original base model (as a no-SFT baseline),\nundergoes the same RL training process for an identical number of steps. Similarly, in this RL phase, each step of a\ntrajectory is treated as an independent training sample.\nWe optimize the policy \u03c0\u03b8using the widely used GRPO [Shao et al., 2024] algorithm. For each question q, we sample a\ngroup of Gtrajectories rollout {Ti}G\ni=1. The policy is then updated by aggregating over all individual steps from the\nsampled trajectories. The loss function is defined as:\nJGRPO (\u03b8) =Eq\u223cD,{T i}G\ni=1\u223c\u03c0\u03b8old(\u00b7|q)\n\"GX\ni=1LiX\nk=1|yk|X\nt=11\n|yk|min\u0010\nri,k,t(\u03b8)\u02c6Ai,clip\u0010\nri,k,t(\u03b8),1\u00b1\u03b5\u0011\n\u02c6Ai\u0011#(1)\nwhere ri,k,t(\u03b8)is the importance sampling ratio, xi\nkandyi\nkare the input and output at step kof trajectory i, and \u02c6Aiis\nthe advantage estimate for that trajectory.\nWe employ an outcome-based reward signal to guide the training. For each completed trajectory Ti, an LLM-judge\nevaluates the final answer aiagainst the ground-truth solution, assigning a binary reward Riof 1 for a correct answer\nand 0 otherwise. This reward Riis used as reward for every step within that trajectory, and the advantage estimate \u02c6Aiis\nalso constant for all steps, defined as \u02c6Ai=Ri\u2212mean({R k})\nstd({R k}).\n5 Experiments\n5.1 Experimental Setup\nDatasetWe use the SFT and RL dataset from the web agent dataset of Li et al. [2025c]. For SFT, we utilize only the\nquestions and ground truth answers to generate a trajectory corpus and evaluate outcomes. We sample 10 trajectories\nfor each question, and randomly select data samples that meet specific criteria to create each SFT"}
{"file": "2510.06534v1.pdf", "section": "5.1 Experimental Setup", "chunk": "5c]. For SFT, we utilize only the\nquestions and ground truth answers to generate a trajectory corpus and evaluate outcomes. We sample 10 trajectories\nfor each question, and randomly select data samples that meet specific criteria to create each SFT dataset. Detailed\nstatistics for these datasets are presented in Table 1. For the RL phase, we utilized the complete 10427 instance in the\noriginal dataset.\nBehavior AnalysisFor the behavior identification pipeline, we used Gemini 2.5 Flash for the first two steps and used\nGemini 2.5 Pro for the final step of consolidating and deduplicating behaviors. For behavior frequency measurement,\nwe also used Gemini 2.5 Flash.\nTraining DetailsWe adopt Qwen3-1.7B Yang et al. [2025] and Llama3.2-3B-Instruct Meta [2024] as our base models.\nFor SFT, each model was fine-tuned for three epochs with a batch size of 8. The RL training was conducted on the\nverl-agent Feng et al. [2025] framework for 300 steps, with a batch size of 32 and a group size of 8 for GRPO.\nBenchmarks and EvaluationWe evaluate on three widely-accepted benchmarks for search agents: WebWalk-\nerQA Wu et al. [2025a], GAIA Mialon et al. [2023], and Humanity\u2019s Exam (HLE) Phan et al. [2025b]. For the GAIA\nbenchmark, we follow prior work Li et al. [2025b], Wu et al. [2025c], Li et al. [2025c] and use the subset of 103\ntext-based examples. To assess performance, we adopt an LLM-as-Judge approach, using GPT-4o-mini to score the\nfinal answers. During this evaluation, each trajectory is limited to a maximum of 25 steps. For our main evaluation, the\nmodel\u2019s temperature is set to 0.0, whereas for the pass@k evaluation, it is set to 1.0.\n5.2 Main Results\nBehavior Priming Effectively Increases Headroom for RLAs shown in Table 2, the behavior-primed models\n(trained on theBehavior Primedataset) consistently achieve significantly higher performance after RL compared to\n6\nA PREPRINT\nDatasetInformation\nVerificationAuthority\nEvaluation"}
{"file": "2510.06534v1.pdf", "section": "5.2 Main Results", "chunk": "Headroom for RLAs shown in Table 2, the behavior-primed models\n(trained on theBehavior Primedataset) consistently achieve significantly higher performance after RL compared to\n6\nA PREPRINT\nDatasetInformation\nVerificationAuthority\nEvaluationAdaptive\nSearchError\nRecoveryOutcome\nAccuracyAvg. Steps\n/ Traj.# Traj. # Toal Steps\nSFT (Random) 71.7 42.2 52.3 36.2 40.0% 4.6 4.3k 20k\nSFT (Correct) 85.7 52.2 53.2 28.0 100.0% 3.9 5.1k 20k\nBehavior Prime 100.0 100.0 100.0 100.0 49.8% 6.8 2.9k 20k\nBehavior Prime (Incorrect) 100.0 100.0 100.0 100.0 0.0% 7.6 2.6k 20k\nBehavior Prime (Correct) 100.0 100.0 100.0 100.0 100.0% 5.9 3.4k 20k\nTable 1: The behavior frequencies, outcome accuracy, and trajectory statistics for SFT datasets.\nTable 2: Overall performance(in accuracy %)of Qwen3-1.7B and Llama3.2-3B-Instruct before and after RL fine-tuning.\nBold numbers indicate the maximum score.\nGAIA WebWalker HLE Overall\nMethod Level 1 Level 2 Level 3 Avg. Avg. Avg. Avg.\nBefore RL\nQwen3-1.7B 7.7 1.9 0.0 3.9 15.3 4.0 7.7\nQwen3-1.7B + SFT (Random) 12.8 7.7 0.0 8.7 23.5 5.4 12.5\nQwen3-1.7B + SFT (Correct) 10.3 9.6 0.0 8.7 24.3 4.8 12.6\nQwen3-1.7B + Behavior Prime 12.8"}
{"file": "2510.06534v1.pdf", "section": "5.2 Main Results", "chunk": "+ SFT (Correct) 10.3 9.6 0.0 8.7 24.3 4.8 12.6\nQwen3-1.7B + Behavior Prime 12.8 7.7 0.0 8.7 22.0 4.6 11.8\nQwen3-1.7B + Behavior Prime (Incorrect) 7.7 11.5 0.0 8.7 15.3 3.5 9.3\nQwen3-1.7B + Behavior Prime (Correct) 10.3 9.6 0.0 8.7 19.1 6.2 11.3\nLlama3.2-3B-Instruct 2.6 3.8 0.0 3.0 3.9 2.1 3.0\nLlama3.2-3B-Instruct + Behavior Prime 20.5 9.6 0.012.6 26.7 5.2 14.8\nAfter RL\nQwen3-1.7B + RL 15.4 11.5 0.0 11.7 26.1 3.9 13.9\nQwen3-1.7B + SFT (Random) + RL 18.0 11.5 16.7 14.6 33.2 7.4 18.4\nQwen3-1.7B + SFT (Correct) + RL 23.1 17.3 0.0 17.5 36.8 5.8 20.0\nQwen3-1.7B + Behavior Prime + RL 28.2 21.2 0.0 21.4 37.2 7.8 22.3\nQwen3-1.7B + Behavior Prime (Incorrect) + RL 30.826.98.327.2 35.5 7.8 23.5\nQwen3-1.7B + Behavior Prime (Correct) + RL30.823.116.725.2 37.8 7.8 23.6\nLlama3.2-3B-In"}
{"file": "2510.06534v1.pdf", "section": "5.2 Main Results", "chunk": ".5\nQwen3-1.7B + Behavior Prime (Correct) + RL30.823.116.725.2 37.8 7.8 23.6\nLlama3.2-3B-Instruct + RL 12.8 11.5 8.3 11.7 23.8 7.4 14.3\nLlama3.2-3B-Instruct + Behavior Prime + RL 25.6 11.5 8.316.5 34.0 7.5 19.3\nthe non-behavior-primed base models. The scores of behavior-primed Llama-3.2-Instruct and Qwen3-1.7B after RL\nare over 35% higher than those of base models trained with direct RL (19.3 vs. 14.3 and 22.3 vs. 13.9, respectively).\nFurthermore, Behavior Priming significantly outperforms the simple distillation baseline (trained on theSFT (random)\ndataset) and the result-correctness driven baseline (trained on theSFT (Correct)dataset). These results demonstrate that\nBehavior Priming establishes a robust foundation through behavior-centric SFT, which in turn enables more effective\nand substantial gains from subsequent RL training.\nReasoning Behavior is More Important than Outcome CorrectnessWhile fine-tuning on theSFT (Correct)\ndataset yields the strongest performance before RL, this advantage does not lead to the best ultimate performance after\nRL. During the subsequent RL training, the model trained on theSFT (Correct)dataset is surpassed by those trained\nwith our behavior priming method (fine-tuned on theBehavior Prime,Behavior Prime (Incorrect), orBehavior\nPrime (Correct)datasets). This finding strongly suggests that although outcome correctness provides a powerful initial\nboost, focusing on reasoning behaviors lays a more solid foundation for RL, which is the key to achieving superior final\nperformance.\nFurthermore, this conclusion is powerfully supported by the results from theBehavior Prime (Incorrect)dataset.\nDespite achieving suboptimal improvement after SFT, this model ultimately acquired the largest gains during RL\ntraining and achieved comparable final performance with the model fine-tuned on theBehavior Prime (Correct)dataset.\nThis result provides strong evidence that a model can still benefit from reasoning behaviors even if the trajectories used\nfor training all lead to incorrect outcomes"}
{"file": "2510.06534v1.pdf", "section": "5.2 Main Results", "chunk": "RL\ntraining and achieved comparable final performance with the model fine-tuned on theBehavior Prime (Correct)dataset.\nThis result provides strong evidence that a model can still benefit from reasoning behaviors even if the trajectories used\nfor training all lead to incorrect outcomes. It demonstrates that the reasoning process itself, independent of the outcome,\nis the most critical factor for building a strong foundation for subsequent RL training.\n5.3 Analysis with Training Dynamics\nTo better understand the effect of Behavior Priming, we analyze the model\u2019s properties during both the supervised\nfine-tuning phase and the subsequent RL phase. Our analysis reveals that priming instills essential capabilities for both\nexploration and extending test-time compute, which unlock greater potential for the RL phase.\n7\nA PREPRINT\nFigure 2: Qwen3 1.7B +SFT (Random)and Qwen3 1.7B +Behavior Prime\u2019s behavior frequencies, Pass@8 accuracy,\nand trajectories statistics (average step number and average search action number per trajectory) on the WebWalkerQA\nbenchmark during thesupervised fine-tuningprocess.\nBehavior Priming Encourages ExplorationWe first examine how model capabilities evolve during the supervised\nfine-tuning process. We monitored models\u2019 behavior frequencies, Pass@8 performance, and average step number and\nsearch action number in trajectories, as SFT progressed on theSFT (Random)andBehavior Primedatasets. The\nresults are shown in Figure 2. In both models, the pass@8 performance and trajectory length increase as behavior\nfrequencies rise. However, the growth is substantially more pronounced when training on theBehavior Primedataset,\nwith all metrics increasing to a much greater extent. This indicates that the focused instilling of reasoning behaviors\nenables the model to learn to explore more diverse paths (pass@8) and allocate more test-time compute resources\nby performing more steps (trajectory length). This process equips the model with crucial abilities, laying a stronger\nfoundation for the subsequent RL stage.\nFigure 3: The entropy, validation accuracy, and valid action ratio trend during the RL process of Qwen3-1.7B and\nQwen3-1.7B with behavior priming (SFT on theBehavior Primedataset). The valid action ratio is the percentage of\nsteps in which the model generates a syntactically valid action.\nSustained Exploration Translates to a Higher RL CeilingWe then analyzed the training dynamics"}
{"file": "2510.06534v1.pdf", "section": "5.3 Analysis with Training Dynamics", "chunk": "ing (SFT on theBehavior Primedataset). The valid action ratio is the percentage of\nsteps in which the model generates a syntactically valid action.\nSustained Exploration Translates to a Higher RL CeilingWe then analyzed the training dynamics during the RL\nphase. We compared the policy entropy and validation performance of the behavior-primed model against those of\nthe base model. Results demonstrated that the behavior-primed model consistently maintains a high policy entropy\nthroughout RL training, showing little tendency to collapse. In contrast, the base model begins with a lower entropy that\nrapidly collapses. This signifies that the behavior priming brings the model with richer exploratory ability, continuously\nseeking diverse strategies rather than prematurely converging to a suboptimal policy. This sustained exploration\ntranslates directly to performance. The validation curve shows that while the base model converges more quickly to a\nlow plateau, the behavior-primed model converges more slowly and reaches a higher performance ceiling.\nDisentangling Reasoning Behaviors from Format LearningNotably, results from the RL training process also\ndemonstrate that unfamiliarity with the task\u2019s required output format is not the primary barrier preventing the base\nmodel from improving during RL. As shown in Figure 3, although the non-behavior-primed Qwen3-1.7B model initially\nhas a lower valid action ratio (the percentage of steps in which the model generates a syntactically valid action) than\nthe behavior-primed model, it successfully masters the action format within just 20 steps and maintains a high valid\nratio thereafter. In contrast, the valid action ratio for the behavior-primed model is even less stable. This phenomenon\nprovides strong evidence that the performance gains from behavior priming stem from the targeted reasoning behaviors\nthemselves, rather than from simple format familiarization (e.g., learning tool-use syntax).\n8\nA PREPRINT\n5.4 Effect of SFT Data Size on Behavior Priming\nWe also investigated how the scale of the SFT data affects the efficacy of Behavior Priming. To this end, we fine-tuned\nQwen3-1.7B on subsets of ourBehavior Primedataset of varying sizes: 5k, 10k, and the full 20k samples. We then\nsubjected each of these behavior-primed models to RL training and compared their final performance against a baseline"}
{"file": "2510.06534v1.pdf", "section": "5.4 Effect of SFT Data Size on Behavior Priming", "chunk": "of ourBehavior Primedataset of varying sizes: 5k, 10k, and the full 20k samples. We then\nsubjected each of these behavior-primed models to RL training and compared their final performance against a baseline\nthat received no behavior-priming process (denoted as 0k).\nResults in Figure 4 reveal a clear scaling trend: as the size of the SFT dataset increases, the model\u2019s final performance\nafter RL consistently improves. This demonstrates that the benefits of Behavior Priming are scalable, with performance\nceiling increasing by leveraging larger-scale data for the initial priming phase.\nFigure 4: Qwen3-1.7B\u2019s performance after fine-tuning on different sizes of Behavior Priming\u2019s subset, and the\ncorresponding performance after the subsequent RL training.\n6 Discussion: Is Process Rewards an Alternative for Behavior Guidance?\nIn this section, we explore alternative ways to use these beneficial behaviors to improve RL performance. Inspired by\nprior works [Yeo et al., 2025, Gandhi et al., 2025, Wu et al., 2025b] suggesting that the spontaneous emergence of\nreasoning behaviors during RL is not guaranteed for models that do not exhibit them, we measured the change in behavior\nfrequencies for a non-behavior-primed model before and after RL training. The results in Table 3 demonstrate that\nthe base model Qwen3-1.7B fails to spontaneously acquire these reasoning behaviors, with most behavior frequencies\neven decreasing after RL. Considering that our behavior frequency analysis is a process-level evaluation of the entire\ntrajectory, we investigate whether we could guide the emergence of these behaviors with an additional process reward\nsignal. Specifically, we combined the standard outcome-based reward with a process-based reward that encourages\nthe exhibition of reasoning behaviors. For a rollout trajectory Ti, we measure the number of behaviors present in this\ntrajectoryN, and define the final reward asR i=R outcome + 0.1\u00d7N.\nAs shown in Table 3, this reward-shaping approach successfully increases the behavior frequency, but this increase did\nnot translate into better performance. In contrast, the final task scores were even worse compared to standard RL with an\noutcome-only reward. This result aligns with the findings of previous work on math and code reasoning DeepSeek-AI\net al. [2025], suggesting that the model learns to \"reward hack"}
{"file": "2510.06534v1.pdf", "section": "6 Discussion: Is Process Rewards an Alternative for Behavior Guidance?", "chunk": "were even worse compared to standard RL with an\noutcome-only reward. This result aligns with the findings of previous work on math and code reasoning DeepSeek-AI\net al. [2025], suggesting that the model learns to \"reward hack\"\u2014it mimics the surface-level patterns of the behaviors\nto maximize the process reward but fails to grasp their functional essence for effective problem-solving. This finding\nreveals that our SFT-based Behavior Priming method is a more effective way to improve model performance, which\ninstills a deeper, more grounded understanding of the reasoning process that direct reward shaping cannot achieve.\nTable 3: Results of behavior frequency (averages across three benchmarks) and benchmark performance for standard\nRL and behavior-guided RL with process rewards on Qwen3-1.7B.\nBehavior Frequency Benchmark Performance\nModelInformation Authority Adaptive Error WebWalkerQA GAIA HLE\nVerification Evaluation Search Recovery\nQwen3-1.7B 28.3 18.1 18.2 39.7 15.3 3.9 4.0\nQwen3-1.7B + RL 25.2 (-3.1) 9.8 (-8.3) 21.5 (+3.3) 11.2 (-28.5)26.1(+10.8)16.3(+12.4) 3.9 (-0.1)\nQwen3-1.7B + behavior guided RL 67.3 (+39.0) 50.8 (+32.7) 82.8 (+64.6) 86.5 (+46.8) 15.4 (+0.1) 7.8 (+3.9)4.4(+0.4)\n9\nA PREPRINT\n7 Conclusion\nIn this work, we address the critical question of how to best prepare LLMs for reinforcement learning in agentic search.\nUnlike in domains such as mathematics, foundational reasoning capabilities for these real-world agentic tasks are\noften underdeveloped, making this preparatory stage particularly important. To this end, we identified four beneficial\nreasoning behaviors for agentic search: Information Verification, Authority Evaluation, Adaptive Search, and Error\nRecovery. We then proposed Behavior Priming, a method that uses supervised fine-tuning to instill these behaviors\nbefore the RL stage. Our findings reveal that the presence of"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": "search: Information Verification, Authority Evaluation, Adaptive Search, and Error\nRecovery. We then proposed Behavior Priming, a method that uses supervised fine-tuning to instill these behaviors\nbefore the RL stage. Our findings reveal that the presence of these reasoning behaviors within SFT trajectory data is\nmore important than the correctness of the final outcome. Behavior Priming builds a strong reasoning foundation for\nunlocking a model\u2019s potential for self-improvement during RL training. Our analysis reveals that this method fosters a\nrobust exploratory capability that translates to sustained high policy entropy during RL, preventing premature policy\ncollapse. This work highlights a promising direction for exploring how targeted post-training can systematically unlock\nthe reinforcement learning potential to train more capable agentic models to handle complex real-world tasks.\nReferences\nBowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. Search-\nr1: Training llms to reason and leverage search engines with reinforcement learning.arXiv preprint arXiv:2503.09516,\n2025a.\nYuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher:\nScaling deep research via reinforcement learning in real-world environments, 2025. URL https://arxiv.org/\nabs/2504.03160.\nKuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao,\nXinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent.arXiv preprint arXiv:2507.02592,\n2025a.\nMoonshot AI. End-to-end rl training for emerging agentic capabilities, 2025. URL https://moonshotai.github.\nio/Kimi-Researcher/.\nTao Zhou and Songtao Li. Understanding user switch of information seeking: From search engines to generative ai.\nJournal of librarianship and information science, page 09610006241244800, 2024.\nBusiness Insider. Apple and google disagree on ai cutting into search.Business Insider, May 2025. URL\nhttps://www.businessinsider"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": "Journal of librarianship and information science, page 09610006241244800, 2024.\nBusiness Insider. Apple and google disagree on ai cutting into search.Business Insider, May 2025. URL\nhttps://www.businessinsider.com/apple-google-disagree-ai-cutting-into-search-2025-5?\nutm_source=chatgpt.com. Accessed: 2025-09-22.\nVerge. Google searches are falling in safari for the first time ever \u2014 probably because of ai.The Verge, May 2025. URL\nhttps://www.theverge.com/news/662725/google-search-safari-ai-apple-eddy-cue-testimony?\nutm_source=chatgpt.com. Accessed: 2025-09-22.\nBowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han.\nSearch-r1: Training llms to reason and leverage search engines with reinforcement learning, 2025b. URL https:\n//arxiv.org/abs/2503.09516.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong\nMa, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li,\nZiyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi\nDeng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli\nLuo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding,\nHuajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan,\nJunjie Qiu, Junlong Li, J. L. Cai,"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": "uo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan,\nJunjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin\nHuang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan\nZhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng\nZhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen,\nR. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu,\nShunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun,\nT. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L.\nXiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao\nLiu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen,\nXiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y . K. Li, Y . Q. Wang, Y . X.\nWei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi,\nYiliang Xiong, Ying He"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": "X.\nWei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi,\nYiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou,\n10\nA PREPRINT\nYuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang\nZhou, Y . X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun\nZha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao,\nZhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng\nPan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability\nin llms via reinforcement learning, 2025. URLhttps://arxiv.org/abs/2501.12948.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang,\nChenxu Lv, et al. Qwen3 technical report.arXiv preprint arXiv:2505.09388, 2025.\nEdward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-of-thought reasoning\nin llms.arXiv preprint arXiv:2502.03373, 2025.\nKanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D Goodman. Cognitive behaviors that\nenable self-improving reasoners, or, four habits"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": "73, 2025.\nKanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D Goodman. Cognitive behaviors that\nenable self-improving reasoners, or, four habits of highly effective stars.arXiv preprint arXiv:2503.01307, 2025.\nGr\u00e9goire Mialon, Cl\u00e9mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. Gaia: a benchmark for\ngeneral ai assistants. InThe Twelfth International Conference on Learning Representations, 2023.\nJialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu\nZhou, Pengjun Xie, and Fei Huang. Webwalker: Benchmarking llms in web traversal, 2025a. URL https:\n//arxiv.org/abs/2501.07572.\nLong Phan, Alice Gatti, Ziwen Han, Nathaniel Li, Josephina Hu, Hugh Zhang, Chen Bo Calvin Zhang, Mohamed\nShaaban, John Ling, Sean Shi, et al. Humanity\u2019s last exam.arXiv preprint arXiv:2501.14249, 2025a.\nRenjun Xu and Jingwen Peng. A comprehensive survey of deep research: Systems, methodologies, and applications.\narXiv preprint arXiv:2506.12594, 2025.\nOpenAI. Introducing chatgpt search, 2024. URL https://openai.com/index/introducing-chatgpt-search/ .\nAnthropic. Claude can now search the web, 2025. URLhttps://www.anthropic.com/news/web-search.\nSalaheddin Alzubi, Creston Brooks, Purva Chiniya, Edoardo Contente, Chiara von Gerlach, Lucas Irwin, Yihan Jiang,\nArda Kaz, Windsor Nguyen, Sewoong Oh, et al. Open deep search: Democratizing search with open-source reasoning\nagents.arXiv preprint arXiv:2503.20201, 2025.\nGPTResearcher. Say goodbye to hours of research, 2025. URLhttps://gptr.dev/.\nXiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": "3.20201, 2025.\nGPTResearcher. Say goodbye to hours of research, 2025. URLhttps://gptr.dev/.\nXiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou.\nWebthinker: Empowering large reasoning models with deep research capability.arXiv preprint arXiv:2504.21776,\n2025b.\nWentao Zhang, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, and Bo An. Agentorchestra: A hierarchical\nmulti-agent framework for general-purpose task solving.arXiv preprint arXiv:2506.12508, 2025.\nXuan-Phi Nguyen, Shrey Pandit, Revanth Gangi Reddy, Austin Xu, Silvio Savarese, Caiming Xiong, and Shafiq Joty.\nSfr-deepresearch: Towards effective reinforcement learning for autonomously reasoning single agents.arXiv preprint\narXiv:2509.06283, 2025.\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander\nMadry, Alex Beutel, Alex Carney, et al. Openai o1 system card.arXiv preprint arXiv:2412.16720, 2024.\nZhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu\nWang, Yong Jiang, et al. Webshaper: Agentically data synthesizing via information-seeking formalization.arXiv\npreprint arXiv:2507.15061, 2025.\nWeizhen Li, Jianbo Lin, Zhuosong Jiang, Jingyi Cao, Xinpeng Liu, Jiayu Zhang, Zhenqiang Huang, Qianben Chen,\nWeichen Sun, Qiexiang Wang, et al. Chain-of-agents: End-to-end agent foundation models via multi-agent distillation\nand agentic rl.arXiv preprint arXiv:2508.13167, 2025c.\nDingf"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": "iang Wang, et al. Chain-of-agents: End-to-end agent foundation models via multi-agent distillation\nand agentic rl.arXiv preprint arXiv:2508.13167, 2025c.\nDingfeng Shi, Jingyi Cao, Qianben Chen, Weichen Sun, Weizhen Li, Hongxuan Lu, Fangchen Dong, Tianrui Qin, King\nZhu, Minghao Liu, Jian Yang, Ge Zhang, Jiaheng Liu, Changwang Zhang, Jun Wang, Yuchen Eleanor Jiang, and\nWangchunshu Zhou. Taskcraft: Automated generation of agentic tasks, 2025. URL https://arxiv.org/abs/\n2506.10055.\nHaoze Wu, Cheng Wang, Wenshuo Zhao, and Junxian He. Model-task alignment drives distinct rl outcomes.arXiv\npreprint arXiv:2508.21188, 2025b.\nZichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin. Understanding\nr1-zero-like training: A critical perspective.arXiv preprint arXiv:2503.20783, 2025.\n11\nA PREPRINT\nAmrith Setlur, Matthew YR Yang, Charlie Snell, Jeremy Greer, Ian Wu, Virginia Smith, Max Simchowitz, and Aviral\nKumar. e3: Learning to explore enables extrapolation of test-time compute for llms.arXiv preprint arXiv:2506.09026,\n2025.\nPrahaladh Chandrahasan, Jiahe Jin, Zhihan Zhang, Tevin Wang, Andy Tang, Lucy Mo, Morteza Ziyadi, Leonardo F. R.\nRibeiro, Zimeng Qiu, Markus Dreyer, Akari Asai, and Chenyan Xiong. Deep research comparator: A platform for\nfine-grained human annotations of deep research agents, 2025. URLhttps://arxiv.org/abs/2507.05495.\nTevin Wang and Chenyan Xiong. Autorule: Reasoning chain-of-thought extracted rule-based rewards improve\npreference learning, 2025. URLhttps://arxiv.org/abs/2506.15651"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": "05495.\nTevin Wang and Chenyan Xiong. Autorule: Reasoning chain-of-thought extracted rule-based rewards improve\npreference learning, 2025. URLhttps://arxiv.org/abs/2506.15651.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li,\nYang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models.arXiv\npreprint arXiv:2402.03300, 2024.\nMeta. Introducing meta llama 3: The most capable openly available llm to date, 2024. URL https://ai.meta.com/\nblog/meta-llama-3/.\nLang Feng, Zhenghai Xue, Tingcong Liu, and Bo An. Group-in-group policy optimization for llm agent training, 2025.\nURLhttps://arxiv.org/abs/2505.10978.\nLong Phan et al. Humanity\u2019s Last Exam.ArXiv, abs/2501.14249, 2025b.\nJialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu,\nYong Jiang, et al. Webdancer: Towards autonomous information seeking agency.arXiv preprint arXiv:2505.22648,\n2025c.\n12\nA PREPRINT\nA Prompts\nA.1 Prompt For Agent Framework\nWe support the integration of both models with internal thinking(like Gemini 2.5 series, Qwen3 series, DeepSeek R1)\nand models without internal thinking. For models without internal thinking, we explicitly prompt them first to generate\na thinking process and then the action. The prompts for both models as shown below:\nAgent Framework Prompt for Models with Internal Thinking\nYour are a research assistant with the ability to perform web searches to answer questions.\nYou can answer a question with many turns of search and reasoning.\nBased on the history information, you need to suggest the next action to complete the task.\nYou will be provided with:\n1. Your history search attempts: query in format <search> query </search> and the returned\nsearch results in <information> and </information>.\n2."}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": "need to suggest the next action to complete the task.\nYou will be provided with:\n1. Your history search attempts: query in format <search> query </search> and the returned\nsearch results in <information> and </information>.\n2. The question to answer.\nIMPORTANT: You must strictly adhere to the following rules:\n1. Choose ONLY ONE action from the list below for each response, DO NOT perform more than\none action per step.\n2. Follow the exact syntax format for the selected action, DO NOT create or use any actions\nother than those listed.\n3. **Don't do duplicate search.** Pay attention to the history search results.\nValid actions:\n1. <search> query </search>: search the web for information if you consider you lack some\nknowledge.\n2. <answer> answer </answer>: output the final answer if you consider you are able to answer\nthe question. The answer should be short and concise. No justification is needed.\n3. <summary> important parts of the history turns </summary>: summarize the history turns.\nReflect the search queries and search results in you history turns, and keep the information\nyou consider important for answering the question and generating your report. Still keep the\ntag structure, keep search queries between <search> and </search>, and keep search results\nbetween <information> and </information>. The history turn information for your subsequent\nturns will be updated accoring to this summary action.\nFormat:\nYou should pay attention to the format of your output. You can choose **ONLY ONE** of the\nfollowing actions:\n- If You want to search, You should put the query between <search> and </search>.\n- If You want to summarize the history turns, You should put the summary between\n<summary> and </summary>.\n- If You want to give the final answer, You should put the answer between <answer> and\n</answer>.\nYou can only use ONE action per response.\nNote: text between <information></information> is the search results from search engine\nafter you perform a search action, **DO NOT** include any information in\n<information></information> in your output.\nQuestion: {question}\nHistory Turns: (empty if this is the first turn)\nAgent Framework Prompt for Models without Internal Thinking\nYour are a research assistant with the ability to perform web searches to answer questions.\nYou can answer a question with many turns of search and reasoning.\n13\nA"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": "(empty if this is the first turn)\nAgent Framework Prompt for Models without Internal Thinking\nYour are a research assistant with the ability to perform web searches to answer questions.\nYou can answer a question with many turns of search and reasoning.\n13\nA PREPRINT\nBased on the history information, you need to suggest the next action to complete the task.\nYou will be provided with:\n1. Your history search attempts: query in format <search> query </search> and the returned\nsearch results in <information> and </information>.\n2. The question to answer.\nIMPORTANT: You must strictly adhere to the following rules:\n1. Choose ONLY ONE action from the list below for each response, DO NOT perform more than\none action per step.\n2. Follow the exact syntax format for the selected action, DO NOT create or use any actions\nother than those listed.\n3. **Don't do duplicate search.** Pay attention to the history search results.\nValid actions:\n1. <search> query </search>: search the web for information if you consider you lack some\nknowledge.\n2. <answer> answer </answer>: output the final answer if you consider you are able to answer\nthe question. The answer should be short and concise. No justification is needed.\n3. <summary> important parts of the history turns </summary>: summarize the history turns.\nReflect the search queries and search results in you history turns, and keep the information\nyou consider important for answering the question and generating your report. Still keep the\ntag structure, keep search queries between <search> and </search>, and keep search results\nbetween <information> and </information>. The history turn information for your subsequent\nturns will be updated accoring to this summary action.\nFormat:\nYou should pay attention to the format of your output. You can choose **ONLY ONE** of the\nfollowing actions:\n- If You want to search, You should put the query between <search> and </search>.\n- If You want to summarize the history turns, You should put the summary between\n<summary> and </summary>.\n- If You want to give the final answer, You should put the answer between <answer> and\n</answer>.\nYou can only use ONE action per response.\nFormat:\n<think> thinking process </think>\n[your action output]\nExample:\n<think> I need to answer the question, so I need to... </think>\n<search> query </"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": ">.\nYou can only use ONE action per response.\nFormat:\n<think> thinking process </think>\n[your action output]\nExample:\n<think> I need to answer the question, so I need to... </think>\n<search> query </search>\nNote: text between <information></information> is the search results from search engine\nafter you perform a search action, **DO NOT** include any information in\n<information></information> in your search action.\nQuestion: {question}\nQuestion: {question}\nHistory Turns: (empty if this is the first turn)\nA.2 Prompt For Behavior Identification\nPrompt for Trajectory Analysis\n[Instruction]\n14\nA PREPRINT\nYou are tasked with analyzing multi-step trajectories of a search agent's two attempts for\nanswering the same question using search tools. One of the attempts correctly answers the\nquestion, and another attempt does not. Based on the content, please provide a detailed\nexplanation of why one attempt succeeds and the other fails.\nThere are two parts in each step of the trajectory:\n1. Agent output: The agent's output in this step, consists of it's thinking process and the\nfinal action.\n2. Environment feedback: The feedback from the environment, including the search results\nwrapped in <information> and </information> tags when the agent performs a search action in\nthis step.\nThe agent could perform one of the following actions in each step:\n1. <search> query </search>: search the web for information\n2. <answer> answer </answer>: output the final answer\n3. <summary> important parts of the history turns </summary>: summarize the history turns to\nkeep valuable information for solving the question.\nPlease analyze the agent's behavior in each step and provide a detailed explanation of why\none attempt succeeds and the other fails.\n[Question]\n{question}\n[Trajectory 1]\n{trajectory_1}\n[Evaluation Results 1]\n{evaluation_results_1}\n[Trajectory 2]\n{trajectory_2}\n[Evaluation Results 2]\n{evaluation_results_2}\n[Your Explanation]\nPrompt for Key Reasoning Behavior Extraction\nYou are an expert in analyzing the behavior of a search agent. You will be provided with an\nexplanation about a search agent's two attempts to answer the same question using search\ntools. The first attempt correctly answers the question, while the second attempt fails.\nBased on the explanation of why trajectory 1"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": ". You will be provided with an\nexplanation about a search agent's two attempts to answer the same question using search\ntools. The first attempt correctly answers the question, while the second attempt fails.\nBased on the explanation of why trajectory 1 succeeds while trajectory 2 fails, extract the\nkey reasoning behaviors statements implied by the explanation that lead to the success of\ntrajectory 1. These should be clear, objective, and unambiguously verifiable.\nReturn the list as a JSON array of strings. Do not include markdown code fences. If there\nare no rule-like statements, return an empty JSON array.\n[Reasoning]\n{reasoning_text}\nPrompt for Behavior Summarization\nYou are an expert in analyzing the behavior of a search agent. You are provided with a set\nof behaviors describing the the reasoning process and actions of the agent.\n15\nA PREPRINT\nBelow is a list of behaviors regarding the behavior of the search agent. Some behaviors may\nbe duplicates or express very similar meanings. Please merge them by removing duplicates and\nconsolidating similar behaviors, while keeping only the most essential information. When\nmerging, discard narrow or overly specific restrictions, and retain only general behaviors\nthat are broadly applicable.\nThe final rules should be clear, objective, and unambiguous, so they can be reliably used to\nevaluate the agent's reasoning and interaction trajectory.\nReturn the merged list as a JSON array of strings. Do not include markdown code fences.\n[Behaviors]\n{behaviors_text}\nA.3 Prompt For Behavior Analysis Framework\nPrompt For Behavior Frequency Analysis\n[Instruction]\nYou are tasked with analyzing a multi-step trajectory of a search agent's attempt for\nanswering a question using search tools.\nThe agent can perform one of the following actions in each step:\n1. <search> query </search>: search the web for information\n2. <answer> answer </answer>: output the final answer\n3. <summary> important parts of the history turns </summary>: summarize the history turns to\nkeep valuable information for solving the question.\nThere are two parts in each step of the trajectory:\n1. Agent output: The agent's output in this step, consists of it's thinking process and the\nfinal action.\n2. Environment feedback: The feedback from the environment, including the search results\nwrapped in <information> and </information> tags when the agent performs a search action in\nthis step.\nPlease act as"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": "'s thinking process and the\nfinal action.\n2. Environment feedback: The feedback from the environment, including the search results\nwrapped in <information> and </information> tags when the agent performs a search action in\nthis step.\nPlease act as an judge to evaluate whether the agent's thinking process and actions in this\ntrajectory demonstrated any of following behaviors:\n**behavior1: Information Verification**\nThe agent validates information across multiple reliable sources to ensure its conclusions\nare well-founded.\n* **Cross-Referencing:** Actively seeking out and comparing multiple sources to confirm\ncritical facts, or performing additional searches to verify the information.\n* **Citing Evidence:** Explicitly basing its reasoning and conclusions on the information\nfound, rather than making unsupported claims.\n**behavior2: Authority Evaluation**\nThe agent assesses the reliability of its sources and resolves conflicting information.\n* **Detecting Conflicts:** Identifying when different sources provide conflicting\ninformation and attempting to resolve the discrepancy.\n* **Prioritizing Authority:** Giving more weight to official documentation, academic papers,\nand reputable news outlets over forums, blogs, or less reliable sources.\n**behavior3: Adaptive Search**\nThe agent intelligently modifies its search strategy based on the information and challenges\nencountered in previous steps.\n* **Narrowing Focus:** Using initial broad search results to identify more specific and\neffective keywords for subsequent searches.\n* **Broadening Scope:** Widening the search terms or approach when initial queries are too\nnarrow and yield no useful results.\n**behavior4: Error Recovery**\n16\nA PREPRINT\nThe agent recognizes previous errors and takes actions to correct its course.\n* **Acknowledging Failure:** Explicitly noting when a search query or an entire strategy is\nnot yielding useful information, or some mistakes are made.\n* **Strategic Pivoting:** Decisively abandoning a failed approach and formulating a new plan\nto achieve the user's goal, or taking actions to correct the mistakes.\nBe as objective as possible when evaluating the behaviors and do not evaluate other\ncharacteristics of the response. If the behavior is not applicable for this task, treat it\nas if the behavior is not demonstrated.\nYou must provide your answer with the following json format without markdown code fences:\n{{\n\"behavior1\": \"<'Yes'or'No'>\",\n\"behavior2\": \"<'Yes'or'No'>\",\n\"behavior3\": \"<'Yes'or'No'>\",\n\"behavior"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": "json format without markdown code fences:\n{{\n\"behavior1\": \"<'Yes'or'No'>\",\n\"behavior2\": \"<'Yes'or'No'>\",\n\"behavior3\": \"<'Yes'or'No'>\",\n\"behavior4\": \"<'Yes'or'No'>\",\n...\n}}\n[Question]\n{question}\n[Trajectory]\n{trajectory}\n[Your Answer]\nA.4 Prompt for LLM-Judge Evaluation\nPrompt For LLM-Judge Evaluation\nPlease determine if the predicted answer is SEMANTICALLY equivalent to the labeled answer.\nQuestion: {question}\nLabeled Answer: {gt_answer}\nPredicted Answer: {pred_answer}\n{{\n\"rationale\": \"your rationale for the judgement, as a text\",\n\"judgement\": \"your judgement result, can only be'correct'or'incorrect'\"\n}}\nB Case Study\nHere is a case study for how the behavior-primed model (Qwen3 1.7B + Behavior Prime + RL) successfully solves a\nquestion, but the base model fails (Qwen3 1.7B + RL). The question is from the GAIA dataset. The trajectory for both\nmodels is provided below.\nQuestion\u201cWhat was the actual enrollment count of the clinical trial on H. pylori in acne vulgaris patients from\nJan\u2013May 2018 as listed on the NIH website?\u201d\n17\nA PREPRINT\nB.1 Successful Trajectory\nQwen3 1.7B + Behavior Prime + RL\nStep 1:\n<think>\nOkay, so the user's looking for the enrollment count of a specific clinical trial on\nH. pylori and acne vulgaris, but they're zeroing in on the period from January to May of\n2018,\nand they're specifically mentioning the NIH website. ...\n[Behavior 1: Information Verification]\nMy task is clear: verify if the quoted text exactly matches Greetham\u2019s article.\nAccuracy is paramount; I\u2019ll use \u2019uncoupled\u2019, \u2019authors\u2019, \u2019mis-transmission\u2019, and\n\u2019veil\u2019 to zero in on the relevant section.\n</think>\n<search>\nclinical trial H. pylori acne vulgaris enrollment Jan-May 2018 NIH website\n<search>\n<information> retrieved documents </information>\nStep 2:\n<think>\nI've got a few hits, though. Result #2 ... looks promising because it's linked to\nClinicalTrials.gov"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": "-May 2018 NIH website\n<search>\n<information> retrieved documents </information>\nStep 2:\n<think>\nI've got a few hits, though. Result #2 ... looks promising because it's linked to\nClinicalTrials.gov,\nwhich is where I'd expect to find this kind of information. ...\n[Behavior 2: Authority Evaluation]\nI\u2019m aiming for the USGS\u2019s own reports or databases, like the \u2018Nonindigenous Aquatic\nSpecies\u2019 page, to get the most reliable data. </think>\n<search>\nNCT03411733 enrollment count Jan-May 2018 NIH\n</search>\n<information> retrieved documents </information>\nStep 3:\n<think>\nThe problem is that the search engine is not returning the number directly.\nIt's likely that the information is not readily available in the snippets.\nSo, I need to try to get to the page for NCT03411733 ...\n[Behavior 3: Adaptive Search]\nThe search engine may not have indexed the quote perfectly, or the user\u2019s quote may\ndiffer slightly. I\u2019ll refine my strategy.\n</think>\n<search>\nNCT03411733 trial start date and recruitment numbers Jan-May 2018 NIH\n</search>\n<information> retrieved documents </information>\nStep 4:\n<think>\nThe last query didn\u2019t bring me directly to the ClinicalTrials.gov record.\nI need to correct my approach and focus on the trial ID itself.\n[Behavior 4: Error Recovery]\nMy initial search didn\u2019t deliver the goods... I need to get more precise.\n</think>\n<search>\nNCT03411733 actual enrollment count Jan-May 2018 NIH\n</search>\n18\nA PREPRINT\n<information> retrieved documents </information>\nStep 5:\n<think>\n...\n</think>\n<answer>\nThe actual enrollment count is 90 patients.\n</answer>\nResult: Correct.\n19\nA PREPRINT\nB.2 Unsuccessful Trajectory\nQwen3 1.7B + RL\nTurn 1\n<think>\nOkay, let's tackle this question. The user is asking for the actual enrollment count of a\nclinical trial on\nH. pylori in acne vulgaris patients from January to May 2018, specifically as listed on the\nNIH website.\nSince the history is empty, I should start by searching for the clinical trial.\nThe valid action here is to perform a search.\nThe"}
{"file": "2510.06534v1.pdf", "section": "7 Conclusion", "chunk": "in acne vulgaris patients from January to May 2018, specifically as listed on the\nNIH website.\nSince the history is empty, I should start by searching for the clinical trial.\nThe valid action here is to perform a search.\nThe query would be something like \"H. pylori clinical trial acne vulgaris enrollment Jan-May\n2018 NIH\".\nI'll use the <search> action with that query. The search results might show the trial's\ndetails, including the enrollment numbers.\nI need to make sure the NIH website has this information.\n</think>\n<search>\nH. pylori clinical trial acne vulgaris enrollment Jan-May 2018 NIH\n</search>\n<information> ... Found NCT03411733 and related links ... </information>\nNo attempt to cross-check, only one generic search.\nTurn 2\n<think>\n[No reasoning; model attempts an invalid summary action.]\n</think>\n<summary>\n<search> H. pylori clinical trial acne vulgaris enrollment Jan-May 2018 NIH </search>\n<information>[All 10 results copied verbatim]</information>\n</summary>\nModel generated an invalid action format. Instead of correcting, it collapsed\nhistory incorrectly.\nTurn 3\n<think>\n[No new reasoning. Model repeats identical search query from Turn 1.]\n</think>\n<search>\nH. pylori clinical trial acne vulgaris enrollment Jan-May 2018 NIH\n</search>\n<information> ... Same results again ... </information>\nQuery was repeated with no refinement.\nTurn 4\n<think>\nPrematurely concludes the answer without verifying enrollment numbers.\n</think>\n<answer>\nThe NIH clinical trial on H. pylori and acne vulgaris from January to May 2018 (NCT03411733)\nhad an enrollment count of 100 participants.\n</answer>\n20\nA PREPRINT\nResult: Incorrect. Extracted \"100\" without checking against the authoritative NIH record. NIH/Clinical-\nTrials.gov was named, but not validated.\n21"}
{"file": "2510.06664v1.pdf", "section": "Introduction", "chunk": "TOOLMEM: Enhancing Multimodal Agents with\nLearnable Tool Capability Memory\nYunzhong Xiao1Yangmin Li1Hewei Wang1Yunlong Tang2Zora Zhiruo Wang1\n1Carnegie Mellon University2University of Rochester\nyunzhonx@alumni.cmu.edu, yangmin2@alumni.cmu.edu, heweiw@alumni.cmu.edu,\nyunlong.tang@rochester.edu, zhiruow@andrew.cmu.edu\nAbstract\nAgents utilizing tools powered by large language models (LLMs) or vision-\nlanguage models (VLMs) have demonstrated remarkable progress in diverse tasks\nacross text and visual modalities. Unlike traditional tools such as calculators, which\ngive deterministic outputs, neural tools perform uncertainly across task scenarios.\nWhile different tools for a task may excel in varied scenarios, existing agents typi-\ncally rely on fixed tools, thus limiting the flexibility in selecting the most suitable\ntool for specific tasks. In contrast, humans snowball their understanding of the\ncapabilities of different tools by interacting with them, and apply this knowledge\nto select the optimal tool when solving a future task. To build agents that similarly\nbenefit from this process, we propose TOOLMEMthat enables agents to develop\nmemories of tool capabilities from previous interactions, by summarizing their\nstrengths and weaknesses and storing them in memory; at inference, the agent\ncan retrieve relevant entries from TOOLMEM, and select the best tool to solve\nindividual tasks more accurately. We evaluate TOOLMEMon learning varied text\ngeneration and text-to-image generation neural tools. Compared to no-memory,\ngeneric agents, we find TOOLMEM-augmented agents predict tool performance\n14.8% and 28.7% more accurately across text and multimodal generation scenarios.\nMoreover, TOOLMEMfacilitates optimal tool selection among multiple choices by\n21% and 24% absolute increases in respective scenarios.\n1 Introduction\nRecent advances in agents have drastically reshaped the landscape of generative tasks, especially by\nutilizing powerful tools supported by large language models (LLMs) [Schick et al., 2023] or vision-\nlanguage models (VLMs) [Gao et al., 2025, Carrasco et al., 2025, Radford et al., 2021]. These agents\nsupported with task-specific neural tools have demonstrated impressive capabilities"}
{"file": "2510.06664v1.pdf", "section": "1 Introduction", "chunk": "vision-\nlanguage models (VLMs) [Gao et al., 2025, Carrasco et al., 2025, Radford et al., 2021]. These agents\nsupported with task-specific neural tools have demonstrated impressive capabilities across various\ndownstream applications such as instruction following and reasoning in text modality [Kim et al.,\n2025, Ouyang et al., 2022], as well as visual understanding and editing in cross-modal scenarios [Shen\net al., 2023, Wang et al., 2024a, Tang et al., 2025a]. Unlike traditional tools such as calculator or\nsqlexecutor that always produce deterministic outputs for a given input, tools supported by neural\nmodels vastly expand the spectrum of tasks that cannot be tackled by deterministic tools, such as\nanswering ad-hoc questions [Chen et al., 2017, Karpukhin et al., 2020] or search relevant news on the\nweb [Nakano et al., 2022, Dunn et al., 2017]. Nonetheless, what are brought along with this wider\napplicability of neural tools is the uncertainty of their outputs\u2014there is no guarantee that a QAtool\ncan answer an arbitrary question correctly, especially when the answer to that question is open-ended\n[Lewis et al., 2021] or changes over time [Guu et al., 2020].\nMoreover, current agents typically solve tasks using a pre-designated set of tools by human experts,\nand only get to learn to use these tools at test time by contextualizing on static textual descriptions\nPreprint. Under review.arXiv:2510.06664v1  [cs.CL]  8 Oct 2025\nLifestyle theme\nGreat at conveying a lively\natmosphere in urban settings Render text\nUnable to accurately render\nreadable text \u201cEco Market\u201d Generate Image with this prompt:\n'Eco Market' on a banner above a green lifestyle fair.\nQuery\nImage Generated ToolMem Tool: Midjourney\nLifestyle theme\nE\ufb00ectively showing lively\nand active environments Render text\nReally good at rendering\nshort texts (\u201cEco Market\u201d)\nImage Generated ToolMem Tool: SDXL-Base\nSelect Tool Figure 1: Example of identifying better-performing tools based on learned TOOLMEM. For example,\ncompared to the image generation tool"}
{"file": "2510.06664v1.pdf", "section": "1 Introduction", "chunk": "rendering\nshort texts (\u201cEco Market\u201d)\nImage Generated ToolMem Tool: SDXL-Base\nSelect Tool Figure 1: Example of identifying better-performing tools based on learned TOOLMEM. For example,\ncompared to the image generation tool instantiated byMidjourney,SDXL-Basetool is better at\nrendering short text such as the queried \u201cEco Market\u201d. We thus store this information in TOOLMEM\nand prioritize selectingSDXL-basein later tasks that ask to render short texts.\nfor tool functionalities [Qin et al., 2023b, Guo et al., 2024]. More often than not, the agent may be\nprovided with multiple tools possessing similar functionalities [Tang et al., 2023, Li et al., 2023],\nthus making it harder to distinguish the best tool to use for a certain task, because the agent has no\nprior knowledge about the individual expertise of these seemingly similar tools [Yao et al., 2023].\nTaking Figure 1 as an example, the two text-to-image tools have different expertise: while both might\nbe good at representing lively environment, the later one may excel at rendering text. Given the task\nof generating image with text, the agent better pick the later tool to ensure better performance.\nHuman gradually learn to distinguish and select between tools by interacting with tools and gathering\nknowledge about their individual properties through past experiences. Recent methods have explored\nsome techniques to gather explicit knowledge from experiences by reflecting on [Shinn et al., 2023]\nand refining [Madaan et al., 2023] generated solutions, or constructing short-term workflows [Wang\net al., 2024c] and long-term memory [Xu et al., 2025], and even curating new tools to assist target\ndownstream tasks [Wang et al., 2023, 2024b]. Besides explicitly storing the knowledge, approaches\nlike Toolformer [Schick et al., 2023] and ToolLLM [Qin et al., 2023a] also explored parametric\nupdates via self -supervised fine -tuning and instruction tuning on tool-using experiences. However,\nthese methods focus on learning task-specific procedures with a fixed tool, instead of distinguishing\nbetween a group of functionally similar tools to optimize downstream tool selection and performance.\nMore concretely, agents lack a mechanism to build and update an internal,"}
{"file": "2510.06664v1.pdf", "section": "1 Introduction", "chunk": ",\nthese methods focus on learning task-specific procedures with a fixed tool, instead of distinguishing\nbetween a group of functionally similar tools to optimize downstream tool selection and performance.\nMore concretely, agents lack a mechanism to build and update an internal, dynamic memory that\nencapsulates the strengths and weaknesses of diverse generative tools.\nTo bridge this gap, we introduce TOOLMEM, a framework that empowers agents to learn and\napply tool-specific capability memories (\u00a72). TOOLMEMis built upon three core components:\n(1) a structured capability memory initialized with a taxonomy that categorizes tool behaviors by\nproficiency levels, enabling consistent updates and targeted retrieval; (2) a feedback generation\nprocess that evaluates tool outputs using either human annotations or automated metrics\u2014such\nas LLM-based judgment scores\u2014to extract fine-grained insights into tool performance; and (3) a\ndynamic memory update mechanism that incorporates new experiences through retrieval-augmented\ngeneration (RAG), allowing the agent to refine, revise, and expand its tool knowledge over time.\nDuring inference, TOOLMEM-augmented agents retrieve relevant memory entries based on the\ncurrent task and inject them into the input context, enabling more accurate tool selection and solution\ngeneration.\nWe first study:Can agents learn and estimate tool capabilities through interaction?(\u00a73) We\nconstruct TOOLMEMabout image generation tools on GENAI-BENCH[Li et al., 2024] and various\ntext-oriented tools on BIGGENBENCH[Kim et al., 2025], then evaluate if TOOLMEM-augmented\nagent can accurately estimate the performance of these tools. We find that TOOLMEMcan predict\nsolution quality using varied tools 14.8\u201328.7% more accurately than GENERICagent without prior\nknowledge about tools.\nNext, we ask:Can agents leverageTOOLMEMto optimize performance through tool selection?(\u00a74)\nWe examine TOOLMEM-augmented agents on selecting the best-performing tool for individual tasks\n2\namong multiple tool candidates, and find it outperforming GENERICagent approach by 21% and\n24% on GENAI-BENCHand BIGGENBENCHbenchmarks (\u00a74).\nIn short, our work builds memory-adaptive tool-using agents, and uniquely addresses the dynamic\nnature of neural tools by enabling agents to grasp an evolving understanding of accessible tools.\n2 TOOLMEM: Learning Tool Capability Memory\nIn this section, we introduce our TOOLMEMframework that initializes a"}
{"file": "2510.06664v1.pdf", "section": "1 Introduction", "chunk": "-adaptive tool-using agents, and uniquely addresses the dynamic\nnature of neural tools by enabling agents to grasp an evolving understanding of accessible tools.\n2 TOOLMEM: Learning Tool Capability Memory\nIn this section, we introduce our TOOLMEMframework that initializes a structured memory (\u00a72.1),\nlearns and updates tool capability knowledge through experiences (\u00a72.2), and eventually solves new\ntasks according to retrieved memory entries (\u00a72.3). Overall, TOOLMEMequips agents to build\ntool capability memories progressively learned from past tool usage experiences, and leverages this\ndynamic knowledge repository to enhance its decision-making.\n2.1 Structured Memory Initialization\nThe evolution of TOOLMEMis an iterative process that builds upon previous states. As such, the\ninitial memory plays a critical role in guiding subsequent developments.\nOur goal is for agents \u03c0to learn the capabilities of a set of tools T={t} with similar functionalities\nto solve a particular task (e.g., text-to-image generation, instruction following), where each tool tis\ninstantiated with a neural model (e.g., Midjourney, GPT). For each tool t, we initialize its capability\nmemory Mt=\u2205and aim to collect a set of knowledge entries Mt={m t}, each describing some\nproperties of tool behaviors in natural language (NL). While a tool tcan be good at certain scenarios\nand bad at others, we categorize memory entries based on the varied proficiency levels the agent\npossesses, namely C={proficient at(p),good at(g),bad at(b),weak at(w)}. Moreover, these\nproficiency-aware categories can be associated with concrete numerical measures of tool performance,\n+2,+1,\u22121,\u22122, when more accurate measures are useful. We thus denote TOOLMEMas the union\nof all memory categories M=\u222a c\u2208CMc. This structured memory initialization offers flexibility in\nmemory update and retrieval, which we will provide more details on next.\n2.2 Learning Tool Capabilities from Experiences\nConstructing Memory from ExperiencesAgents learn more about the tool capabilities by\ninteracting with them in task-solving experiences. More concretely, the agent begins by receiving a\ntaskq(e.g., an NL instruction), as well as a corresponding solution stusing a designated tool t(e.g.,\nan image generated by a text-to-image tool t, an answer generated by a text generation tool t). As an"}
{"file": "2510.06664v1.pdf", "section": "2.2 Learning Tool Capabilities from Experiences", "chunk": "a\ntaskq(e.g., an NL instruction), as well as a corresponding solution stusing a designated tool t(e.g.,\nan image generated by a text-to-image tool t, an answer generated by a text generation tool t). As an\ninformative learning signal of this task, a reward system Rprovides feedback rt=R(q, s t)on the\nquality of the solution stin tackling the task q, which could be numerical quality measures (e.g., 1-5\nLikert scale) or NL feedback (as exemplified in Figure 2). Together, the task q, tool-using solution\nst, and its quality feedback rtform an experience et= (q, s t, rt)from interaction with tool t, upon\nwhich we can construct capability memories about. In our study, we focus on building TOOLMEM\nfrom experiences annotated by human experts for image generation tools and by llm-as-a-judge\nfor text generation tools. These annotations are provided as part of the training datasets, typically\naccompanied by high-quality solutions and reliable feedback. We introduce a memory induction\nmodule ILMinstantiated with an LM, to summarize the capabilities of tool tfrom a pertaining\nexperiencee tviaI LM(et)\u2192m t.\nUpdating Agent MemoryGiven a new memory entry mderived from an experience e, a naive\nmemory update would directly augment the existing memory repository by M \u2190 M \u222a {m} .\nHowever, this simplistic approach often introduces redundancy and inflates computational overhead.\nSpecifically, entries that are highly similar or overlapping may accumulate without being appropriately\nconsolidated, and prior incomplete or suboptimal entries might persist without receiving necessary\nupdates.\nTo enable flexible memory refinement capabilities, we introduce a structured and refined memory\nupdating procedure. Formally, we denote the agent\u2019s current memory as M=\u222a c\u2208CMcpartitioned\nintoCcategories introduced in \u00a72.1. Given the newly constructed experience e= (q, s, r) , we first\nperform a targeted retrieval within each memory category Mcto identify the top- kmost semantically\n3\n[NL feedback]   While correctly translated French into English,\nbut it included an extra system prompt sentence\u2026 ToolMem\nConstruction\nTranslate the given sentence to English: La pomme est color\u00e9e en rouge\u2026 Task q\nFeedback r\nscore Task q ToolMem -Augmented\nr etrieve\nSolution s"}
{"file": "2510.06664v1.pdf", "section": "2.2 Learning Tool Capabilities from Experiences", "chunk": "it included an extra system prompt sentence\u2026 ToolMem\nConstruction\nTranslate the given sentence to English: La pomme est color\u00e9e en rouge\u2026 Task q\nFeedback r\nscore Task q ToolMem -Augmented\nr etrieve\nSolution s Task Solving Capability Analysis\nTask q Solution\ns\nFeedback r\nSolution s \u2026Pro\ufb01cient +2\n\u2026\nTranslating\nFrench to\nEnglish Good At +1\n\u2026\nIncluding\nunrelated\ncomments\u2026 Bad At -1\u2026Poor At -2\nTRANSLATION : The apple is \u2026 I am a helpful, respectful and honest assistant.\n[Score] 4 Figure 2: Illustration of the TOOLMEMconstruction process from past experiences (left), as well as\nTOOLMEM-augmented task-solving and capability analysis at downstream (right).\nrelevant memory entries. Mc\nretrieved =Retriever(e,Mc),\u2200c\u2208C where Retriever(\u00b7,\u00b7) calculates\nsome similarity measure (e.g., cosine similarity) computed between embedded memory entries.\nNext, we construct a condensed contextual set from these retrieved entries across all categories:\nMretrieved =S\nc\u2208CMc\nretrieved . Subsequently, the condensed contextual set Mcontext along with the\nnew experience e= (q, s, r) is passed into the TOOLMEMinduction module Ito for entry refinement\nMupdated =I(M retrieved , e)then memory update M \u2190(M \\ M context)\u222a M updated The refinement\nprocess involves adding novel insights, updating incomplete entries, removing redundant information,\nand merging semantically related entries (see exact prompts in \u00a7B.\n2.3 Solving Tasks with Retrieved Memory\nAnalogous to how humans draw upon past experiences when confronted with a new task, for a\ngiven task q\u2032at test time, our task-solving agents can (i) retrieve top- krelevant memory entries\nfrom TOOLMEMutilizing a retriever model Retriever(q\u2032,M)\u2192 M q\u2032, and then (ii) augmented\nthese relevant memory entries to input contexts to generate solutions \u03c0(q\u2032,Mq\u2032)\u2192s\u2032or estimate\ntool performance \u03c0(q\u2032,Mq\u2032)\u2192r\u2032. As TOOLMEMcontains multiple categories M=\u222a cMc, we\nseparately retrieve top- krelevant entries from each category Mcand provide all of them in"}
{"file": "2510.06664v1.pdf", "section": "2.3 Solving Tasks with Retrieved Memory", "chunk": "estimate\ntool performance \u03c0(q\u2032,Mq\u2032)\u2192r\u2032. As TOOLMEMcontains multiple categories M=\u222a cMc, we\nseparately retrieve top- krelevant entries from each category Mcand provide all of them in context.\nThis RAG strategy addresses two key challenges: (1) relevance filtering: focusing on contextually\nuseful entries and discarding irrelevant ones; (2) scalability: preventing computation overload by\nrestricting retrieval to a small number of entries per category.\n3 Experiments: Tool Performance Prediction\nIn this section, we first introduce the baseline approaches in comparison to our TOOLMEM(\u00a73.1),\nthen demonstrate TOOLMEM\u2019s effectiveness on two representative tasks \u2014 text generation (\u00a73.2) and\ntext-to-image generation evaluation (\u00a73.3). We focus on TOOLMEMusage under theperformance\npredictionscenario in this section. Later in \u00a74, we further demonstrate its utility when extrapolating\ntomodel selectionunder accuracy and efficiency considerations.\n3.1 Baseline and TOOLMEMApproaches\nWe introduce the two baseline approaches and our TOOLMEMmethod for experimentation.\nGeneric AgentOur first baseline is a vanilla agent without tool-specific memory beyond general\ninformation (e.g., tool\u2019s name, basic skills, input/output) at inference. This setup is similar to the\n4\ncurrent tool-selection approach adopted by state-of-the-art generative agents [Wang et al., 2024a].\nAgents must rely heavily on the parametric knowledge acquired through the training of their backbone\nLM to estimate tool capabilities.\nFew-Shot MemoryWithout the need to induce TOOLMEM, we propose another stronger baseline\non top of GENERICthat also retrieves top-12 training examples, i.e., (task, score, rubric) triplets, that\nare most relevant to the test example at hand. We denote this the FEW-SHOTsetting and compare it\nto our TOOLMEM-augmented LM, to show the effectiveness of derived capability memory against\nimplicit information from raw examples.\nTOOLMEMInstead of employing memory with generic or raw experiences, our TOOLMEM\ncontains carefully induced and updated memory from past experiences. Particularly, we induce\nmemory entries from the same set of training examples used for retrieval in FEW-SHOTsetting, and\ntest TOOLMEM-augmented LM on the same test set.\nFor generations, we use GPT-4o (the gpt-4o"}
{"file": "2510.06664v1.pdf", "section": "3.1 Baseline and TOOLMEMApproaches", "chunk": "from the same set of training examples used for retrieval in FEW-SHOTsetting, and\ntest TOOLMEM-augmented LM on the same test set.\nFor generations, we use GPT-4o (the gpt-4o-mini-2024-07-18 checkpoint) for all approaches by\ndefault; we use a temperature t= 0.0 and sample n= 1 output. For retrieval, we use OpenAI\u2019s\ntext-embedding-ada-0021model to generate embeddings and implement RAG using ChromaDB2\nthat measures task-memory similarity using (negative) cosine distance. During TOOLMEMconstruc-\ntion, we retrieve k= 6 entries from each memory category and perform memory update on them.\nAt test time, the agent retrieves k= 12 entries from each memory category and grounds on them to\ngenerate task solutions; this setting scores the best in our ablation study onkvalue choices in \u00a7A.\n3.2 Text Generation\nDataset: BIGGENBENCHWe evaluate TOOLMEMon diverse text-generation tasks from the\nBIGGENBENCHbenchmark [Kim et al., 2025], which includes 696 task instances spanning 8\ncapability dimensions, such as reasoning, safety, and tool usage, evaluated across 103 language\nmodels. We split the data into 211 training and 485 testing examples for our experiments.\nEach task qconsists of a natural language instruction composed of a system prompt and user input.\nThe solution stis the textual response generated by a specific language model (tool) t. The quality\nfeedback rt=R(q, s t)includes a numerical score (on a 5-point Likert scale), a detailed score rubric,\nand NL feedback by GPT-4. We use feedback generated by GPT-4 to construct TOOLMEMfor each\ntool, which are then used to improve downstream score prediction accuracy for new prompts.\nWe focus on six language model tools with varying capabilities and accessibility levels: (1) Proprietary\nmodels:Claude-3-Sonnet,GPT-3.5-Turbo;3(2) Top-tier open-source models:Meta-Llama-3-70B-\nInstruct,Qwen-110B-Chat; (3) Smaller models: the strongest sub-7B model,Gemma-1.1-2B-It, and\nthe smallest chat-capable model,Qwen1"}
{"file": "2510.06664v1.pdf", "section": "3.2 Text Generation", "chunk": "70B-\nInstruct,Qwen-110B-Chat; (3) Smaller models: the strongest sub-7B model,Gemma-1.1-2B-It, and\nthe smallest chat-capable model,Qwen1.5-0.5B-Chat.\nEvaluation: Score PredictionWe evaluate TOOLMEM-augmented LMs under a score prediction\nevaluation setup on how accurately they measure the tool generation quality. In our vanilla evaluation\nsetup, each method is tasked with predicting a quality score for the generated image or text given a task\nprompt. The goal is to predict the quality score of a prompt-response pair q, using the task-specific\nrubrics provided in the dataset, also rated on a 1\u20135 Likert scale. We take the gpt4-04-turbo-score\nas ground-truth, as no human scores are provided.\nWe measure the consistency of predicted scores spredto the ground-truth scores sby (i) the average\nmean absolute error (MAE) across all Ntest examples MAE=1\nNPN\ni=1|s(i)\npred\u2212s(i)|, (ii) the root\nmean squared error by RMSE=q\n1\nNPN\ni=1(s(i)\npred\u2212s(i))2, (iii) the Pearson correlation coefficient\nbetween predicted and true scores: Pearson=PN\ni=1(s(i)\npred\u2212\u00afspred)(s(i)\u2212\u00afs)qPN\ni=1(s(i)\npred\u2212\u00afspred)2\u221aPN\ni=1(s(i)\u2212\u00afs)2, where \u00afspredand\n\u00afsare the means of the predicted and ground-truth scores, respectively.\n1https://platform.openai.com/docs/models/text-embedding-ada-002\n2https://www.trychroma.com/\n3Specific checkpoints are:claude-3-sonnet-20240229,gpt-3.5-turbo-1106.\n5\nResults and AnalysisAs shown in Table 1, TOOLMEMreduces MAE by 14.8% and RMSE\nby 14.5% on average than GENERICbaseline; and improves ranking fidelity\u2014Pearson correlation\nincreases by 76.7%.\nTools with varied sizesFor the two smallest and least capable tools (Qwen1.5-0.5B-"}
{"file": "2510.06664v1.pdf", "section": "3.2 Text Generation", "chunk": "5% on average than GENERICbaseline; and improves ranking fidelity\u2014Pearson correlation\nincreases by 76.7%.\nTools with varied sizesFor the two smallest and least capable tools (Qwen1.5-0.5B-Chatand\ngemma-1.1-2B-it), TOOLMEMboosts Pearson correlation significantly, lifting it from near-random\n(\u22120.007 and0.120 ) to0.405 and0.324 , highlighting the valuable, tool-specific priors that are initially\nlack for these lightweight models and are augmented through TOOLMEM. Even for the strongest\nopen-weight tool (Meta-Llama-3-70B-Instruct), TOOLMEMachieves a notable 38.9% increase in\nPearson correlation, from0.175to0.243, while slightly reducing RMSE.\nTable 1: Performance prediction of text-generation tools.\nTool Method MAE\u2193RMSE\u2193Pearson\u2191\nLlama3 70BGENERIC 0.5711.036 0.175\nFEW-SHOT 1.115 1.566 0.163\nTOOLMEM 0.6101.025 0.243\nQwen 110BGENERIC 0.602 0.991 0.228\nFEW-SHOT 1.155 1.605 0.146\nTOOLMEM 0.600 0.988 0.246\nGemma 2BGENERIC 1.004 1.528 0.120\nFEW-SHOT 1.008 1.429 0.271\nTOOLMEM 0.932 1.252 0.324\nQwen1.5 0.5BGENERIC 1.769 2.136 -0.007\nFEW-SHOT 1.381 1.762 0.102\nTOOLMEM 1.080 1.400 0.405\nClaude3GENERIC 0.5460.9830.313\nFEW-SHOT 1.085 1.555 0.140\nTOOLMEM 0.5510.9790.310\nGPT-3.5GENERIC 0.625 1.095 0.224\nFEW-SHOT 0.924 1.392"}
{"file": "2510.06664v1.pdf", "section": "3.2 Text Generation", "chunk": "140\nTOOLMEM 0.5510.9790.310\nGPT-3.5GENERIC 0.625 1.095 0.224\nFEW-SHOT 0.924 1.392 0.204\nTOOLMEM 0.588 0.998 0.332Top closed tools have di-\nminishing returnsForgpt-\n3.5-turbo-1106andclaude-\n3-sonnet, baseline predic-\ntions already achieve rela-\ntively strong Pearson corre-\nlations ( \u22650.224 ), while\nTOOLMEMstill manages to\nenhance correlation for GPT\nby 48.2%.\nInstability of Raw Experi-\nencesAcross all tools, the\nFEW-SHOTstrategy underper-\nforms GENERICacross met-\nrics and often doubles the\nerror for the two top mod-\nels (e.g., MAE +95% on\nMeta-Llama-3-70B). In con-\ntrast, TOOLMEMdelivers ro-\nbust improvements without\nsuch regressions, underscor-\ning the value of a compact,\ninduced capability memory\nover raw example reference.\n3.3 Text-to-Image Evaluation\nDataset: GENAI-BENCHWe evaluate text-to-image generation tools using the GENAI-BENCH\nbenchmark [Li et al., 2024], which contains 1,600 diverse natural language instructions spanning\ncompositional scenarios such as object counting, spatial relations, and negation. We randomly sample\n200 training and 800 testing examples for experiments.\nEach instruction is paired with six images generated by different tools, includingMidJourney,\nDALL\u00b7E 3,SDXL Turbo,DeepFloyd I-XL v1,SDXL 2.1, andSDXL Base. Each text-image pair is\nannotated by three human raters on a 1\u20135 Likert scale, reflecting how well the image aligns with\nthe prompt. As multiple human annotators can introduce variance in prediction scores, we take the\nfirst annotator\u2019s rating as the ground-truth score to ensure label consistency. Our goal is to predict\nthe quality score rtbased on the task input q(image generation prompt) and the solution st(the\ngenerated image using toolt) with agents having no memory, few-shot examples, or TOOLMEM.\nEvaluation: Score & Description PredictionWe"}
{"file": "2510.06664v1.pdf", "section": "3.3 Text-to-Image Evaluation", "chunk": "to predict\nthe quality score rtbased on the task input q(image generation prompt) and the solution st(the\ngenerated image using toolt) with agents having no memory, few-shot examples, or TOOLMEM.\nEvaluation: Score & Description PredictionWe similarly evaluate TOOLMEM-augmented LM\nto predict the alignment score of a prompt-image pairq, rated on a 5-point Likert scale.\nWhile score prediction may be subject to individual human judgment and less robust, we alternatively\nevaluate image quality through text-image alignment: given a text prompt q(e.g. \u201cThree birds flying\nin the sky\u201d), we ask agent to predict a text description dpredfor the image simagegenerated by tool t.\nWe then evaluate the alignment between the image and the predicted description using the VQA score\n[Li et al., 2024], computed as svqa=VQA(simage, dpred). A higher VQA score indicates stronger\nalignment between the image and the textual description. We report average VQA Score in Table 2.\n6\nResults and AnalysisTable 2 lists the six image\u2013generation tools in descending order of capability.\nOverall, TOOLMEMconsistently improves over the GENERICbaseline by decreasing MAE by 28.7%\nand RMSE by 26.6% across tools. The gains, however, are not uniform:\nTop-tier closed models (DALLE 3, Midjourney 6).TOOLMEMbeats the GENERICagent (\u20137.4%\nand \u201312.2% MAE, respectively) but trails FEW-SHOTby 3.7% to 6.7%. This suggests that when\nthe underlying generator is highly capable, large in-context exemplars are sufficient and an external\ncapability memory brings limited extra benefit. However, TOOLMEMdelivers consistent gains\nin VQA scores, with improvements of +2.1% for DALLE 3 and +2.0% for Midjourney 6. These\nsteady gains highlight TOOLMEM\u2019s ability to fine-tune descriptions even for top-performing models,\neffectively capturing subtle yet crucial details that baseline methods might overlook.\nMid/low-tier open models (DeepFloyd IXLv1, SDXL-Base, SDXL-Turbo, SD-2-1).Here\nTOOLMEMclearly shows its strength: it achieves the lowest MAE and RMSE on all four models,\nreducing MA"}
{"file": "2510.06664v1.pdf", "section": "3.3 Text-to-Image Evaluation", "chunk": "XLv1, SDXL-Base, SDXL-Turbo, SD-2-1).Here\nTOOLMEMclearly shows its strength: it achieves the lowest MAE and RMSE on all four models,\nreducing MAE by 26.1\u201342.6% and RMSE by 25.5\u201336.6% relative to GENERIC, and outperforming\nFEW-SHOTby 6.7\u201318.0% MAE.\nTable 2: Predicting text-to-image tool performance on GENAI-BENCH.\nTool MethodScore Prediction Desc. Pred.\nMAE\u2193RMSE\u2193VQA Score (%)\u2191\nDALLE 3GENERIC 1.009 1.405 81.30\nFEW-SHOT 0.875 1.18081.64\nTOOLMEM 0.934 1.20482.98\nMidjourney 6GENERIC 1.085 1.434 81.19\nFEW-SHOT 0.9191.227 82.21\nTOOLMEM 0.9531.212 82.84\nDeepFloydGENERIC 1.334 1.703 79.17\nFEW-SHOT 1.058 1.393 79.74\nTOOLMEM 0.986 1.268 81.30\nSDXL-BaseGENERIC 1.481 1.841 80.47\nFEW-SHOT 1.111 1.422 81.37\nTOOLMEM 0.978 1.285 82.03\nSDXL-TurboGENERIC 1.566 1.932 79.22\nFEW-SHOT 1.101 1.400 80.01\nTOOLMEM 0.996 1.331 81.73\nSDXL-2-1GENERIC 1.645 1.981 75.70\nFEW-SHOT 1.151 1.477 75.91\nTOOLMEM 0.944 1.255 78.90TOOLMEMalso exhibits\nsignificant improvements\nfor VQA scores, particu-\nlarly in mid- and low-tier\nmodels where gains reach\n+3.17% for SDXL-Turbo\nand +4"}
{"file": "2510.06664v1.pdf", "section": "3.3 Text-to-Image Evaluation", "chunk": "78.90TOOLMEMalso exhibits\nsignificant improvements\nfor VQA scores, particu-\nlarly in mid- and low-tier\nmodels where gains reach\n+3.17% for SDXL-Turbo\nand +4.23% for SDXL-\n2-1. This pattern re-\nflects TOOLMEM\u2019s ability\nto adapt to each model\u2019s\nspecific strengths and weak-\nnesses, resulting in more\naccurate and context-aware\ndescriptions. The learn-\nable memory is especially\neffective for weaker tools\nthat lack robust prior knowl-\nedge, helping narrow the\ngap between less capa-\nble generators and their\nstronger counterparts.\nIn summary, TOOLMEM\noffers the greatest ben-\nefit where it is most\nneeded\u2014narrowing the gap between weaker generators and their stronger peers\u2014while remaining\ncompetitive on state-of-the-art closed models. Its dual effectiveness in both score prediction and\nprecise description prediction highlights its capacity to enhance model evaluation by leveraging\nlearned memory, ultimately providing higher VQA scores and more precise model-specific\ndescriptions.\n4 Experiments: Performant Tool Selection\nIn addition to predicting a single tool\u2019s performance, we can select the better-performing tool among\nmultiple choices based on the TOOLMEMlearned for these tools. We construct TOOLMEMand\nevaluate on the same datasets on GENAI-BENCHand BIGGEN-BENCH, and compare the same three\nmethods (GENERIC, FEW-SHOT, TOOLMEM) as in \u00a73.\nEvaluation MetricsLet sAandsBdenote the ground-truth scores of tools A and B, and spred,A\nandspred,B be their predicted scores. Since for equal pairs selecting either tool is equally acceptable,\nwe evaluate tool selection performance over the subset ofunequalpairs D={i:si\nA\u0338=si\nB}. For each\ni\u2208D , we define TP<=|{i\u2208D:si\nA< si\nB\u2227si\npred,A < si\npred,B}|,P<=|{i\u2208D:si\npred,A <\nsi\npred,B}|, and R<=|{i\u2208D:si\nA< si\nB}|. We define TP>,P>, and R>analogously for the \u201c >\u201d\n7\nTable 3: Selecting better text generation tools. Best results for"}
{"file": "2510.06664v1.pdf", "section": "4 Experiments: Performant Tool Selection", "chunk": "<=|{i\u2208D:si\nA< si\nB}|. We define TP>,P>, and R>analogously for the \u201c >\u201d\n7\nTable 3: Selecting better text generation tools. Best results for each tool pair are bold.\nTool PairGENERICFEW-SHOTTOOLMEM\nF1< F1> Acc. F1< F1> Acc. F1< F1> Acc.\nLlama3 Claude3 0.00 0.09 0.03 0.20 0.06 0.07 0.28 0.10 0.11\nClaude3 GPT-3.5-Turbo 0.03 0.06 0.03 0.210.22 0.13 0.060.30 0.15\nQwen 110B Qwen 0.5B 0.00 0.13 0.07 0.00 0.09 0.05 0.000.77 0.62\nLlama3 Qwen 110B 0.04 0.19 0.07 0.120.170.08 0.160.140.09\nGemma 2B Qwen 0.5B 0.10 0.04 0.03 0.05 0.12 0.06 0.06 0.39 0.23\nGPT-3.5-Turbo Gemma 2B 0.00 0.24 0.11 0.080.27 0.14 0.000.62 0.39\nAverage 0.03 0.12 0.06 0.110.16 0.09 0.090.39 0.27\ncase. Then we compute F1<=2TP<\nP<+R<,F1>=2TP>\nP>+R>, and Acc =TP<+TP >\n|D|, where F1<(resp.\nF1>) is the F 1score for predicting sA< sB(resp. sA> sB) over the non-equal set D, and Acc is\nthe proportion of cases where the predicted and ground-truth preference directions match.\nText Generation EvaluationOn BIGGENBENCH, we"}
{"file": "2510.06664v1.pdf", "section": "4 Experiments: Performant Tool Selection", "chunk": "sA< sB(resp. sA> sB) over the non-equal set D, and Acc is\nthe proportion of cases where the predicted and ground-truth preference directions match.\nText Generation EvaluationOn BIGGENBENCH, we evaluate six tool pairs as listed in Table 3.\nThe selected pairs include comparisons between top open-source and closed-source models (e.g.,\nMeta-Llama-3-70B vs. Claude-3-Sonnet), similarly strong closed models (Claude-3-Sonnet vs. GPT-\n3.5-Turbo), and strong open models (Meta-Llama-3-70B vs. Qwen110B), models from the same\nfamily with large capacity gaps (Qwen110B vs. Qwen0.5B), and small-scale model comparisons\n(e.g., Gemma-2B vs. Qwen0.5B). These combinations cover diverse settings across model families,\nsizes, and access types.\nAs shown in Table 3, we find that TOOLMEMoffers clear and consistent accuracy advantages over\nGENERICand FEW-SHOTsettings by 21% and 18% on average. TOOLMEMincreases the F 1<score,\nwhich measures the ability to identify the cases where weaker tool excels, increases from 0.00 to 0.28\nfrom GENERICin the Meta-Llama-3-70B vs. Claude-3-Sonnet comparison, on average it improves\nfrom GENERIC\u2019s 0.03 to 0.09. F 1>improves more significantly from 0.12 to 0.39 on average, with\nTOOLMEMachieving the highest score on five of the six tool pairs. TOOLMEMboosts performance\nmost substantially when tool pairs exhibit large capability gaps, e.g., TOOLMEMimproves accuracy\nby +0.55 for Qwen110B vs. Qwen0.5B and +0.28 for GPT-3.5-Turbo vs. Gemma-2B. Even for\ntool pairs with smaller capability gaps (e.g., Meta-Llama-3-70B vs. Claude-3-Sonnet), TOOLMEM\nmaintains an edge over both baselines.\nText-to-Image Generation EvaluationOn GENAI-BENCH, we examine six representative tool\npairs (in Table 4) to test whether TOOLMEM-augmented"}
{"file": "2510.06664v1.pdf", "section": "4 Experiments: Performant Tool Selection", "chunk": "3-Sonnet), TOOLMEM\nmaintains an edge over both baselines.\nText-to-Image Generation EvaluationOn GENAI-BENCH, we examine six representative tool\npairs (in Table 4) to test whether TOOLMEM-augmented LM can accurately identify the superior tool\nfor individual tasks. More capable model is on the left side of each pair. To showcase TOOLMEM\nutility in varied scenarios, we choose varied types of tool pairs. Specifically: (i) distinguishing\ncomparably top-performing proprietary tools (DALL \u00b7E 3, Midjourney) and against open-weight tools\n(DALL \u00b7E 3, DeepFloyd), (Midjourney, DeepFloyd); (ii) comparing tools with huge gaps from the\nsame (SDXL-Base, SDXL-Turbo) and different model families (DALL\u00b7E 3, SDXL-2.1).\nTable 4 shows the consistent edge of TOOLMEMacross varied tool pairs. Averaged over five tool\npairs, accuracy on unequal pairs climbs from 0.09 (GENERIC) and 0.27 (FEW-SHOT) to 0.33 with\nTOOLMEM, an absolute gain of +0.24 over GENERIC(+266%) and +0.06 over FEW-SHOT(+22%).\nThe hardest class F 1<, which measures the situation to find worse cases for a more capable model,\nimproves most dramatically\u2014rising from 0.09 to 0.32 compared to GENERICand FEW-SHOT\napproaches. Meanwhile, F 1>also edges up from 0.15 to 0.46 when augmented with TOOLMEM.\nThe benefit brought by TOOLMEMshows more prominently as the capability gap of the tool pair\ngrows. For instance, between comparable DALL \u00b7E 3 and Midjourney tools, the gain of Acc. for\nTOOLMEMcompare to GENERICis relatively small (+0.12), yet comparing DALL \u00b7E 3 to the weaker\nSDXL-2.1, TOOLMEMboosts accuracy from 0.08 to 0.53, achieves much larger gains.\nEven within the same model family, TOOLMEMcan effectively tell tools\u2019 capability differences,\nshowcased by the +0.18 gain in the SDXL-Base vs. SDXL-Turbo comparison. These results confirm\nthat"}
{"file": "2510.06664v1.pdf", "section": "4 Experiments: Performant Tool Selection", "chunk": "much larger gains.\nEven within the same model family, TOOLMEMcan effectively tell tools\u2019 capability differences,\nshowcased by the +0.18 gain in the SDXL-Base vs. SDXL-Turbo comparison. These results confirm\nthat TOOLMEMconsistently chooses better tools when the tool choices exhibit different properties.\n8\nTable 4: Selecting better text-to-image generation tools. Best results for each tool pair are bold.\nTool PairGENERIC FEW-SHOT TOOLMEM\nF1< F1> Acc. F1< F1> Acc. F1< F1> Acc.\nDALLE 3 DeepFloyd 0.07 0.19 0.10 0.190.520.31 0.31 0.52 0.35\nDALLE 3 Midjourney 0.00 0.31 0.14 0.17 0.42 0.23 0.26 0.43 0.26\nDALLE 3 SD-2-1 0.00 0.16 0.08 0.20 0.66 0.47 0.32 0.72 0.53\nSDXL-Base SDXL-Turbo 0.14 0.03 0.05 0.240.290.18 0.360.260.23\nMidjourney DeepFloyd 0.23 0.07 0.08 0.26 0.28 0.18 0.35 0.37 0.26\nAverage 0.09 0.15 0.09 0.21 0.43 0.27 0.32 0.46 0.33\n5 Related Work\nTool-Using Multimodal AgentsMultimodal agents combine LLMs and LVLMs to solve complex\ntasks across modalities [Xie et al., 2024]. Early systems like Visual Programming [Gupta and\nKembhavi, 2022] and ViperGPT [Sur \u00b4\u0131s et al., 2023] use LLMs to compose expert modules for visual\nreasoning. Visual ChatGPT [Wu et al., 2023] and HuggingGPT [Shen et al., 2023] improve agent\nflex"}
{"file": "2510.06664v1.pdf", "section": "5 Related Work", "chunk": "2023] use LLMs to compose expert modules for visual\nreasoning. Visual ChatGPT [Wu et al., 2023] and HuggingGPT [Shen et al., 2023] improve agent\nflexibility via prompt routing and tool selection from a VFM pool. Later methods such as Toolformer\n[Schick et al., 2023], AssistGPT [Gao et al., 2023], and ToolLLM [Qin et al., 2023b] learn to invoke\ntools through self-supervision or in-context examples. While effective, these approaches assume\nfixed tool behaviors or rely on limited, static memory. In contrast, TOOLMEMenables adaptive,\ncontext-aware tool selection among choices, by building capability memories that capture tools\u2019\nempirical strengths and weaknesses through experiences.\nGenerative AI ToolsUnlike deterministic tools that produce predictable outputs, tools supported by\ngenerative neural models (e.g.,DALLE-3,GPT) exhibit varied performances across task specifications\n[Tang et al., 2025b]. Some tools may excel at object counting, while others are better at spatial\nunderstanding [Sim et al., 2024]. While prior work has relied on human expertise to navigate these\nnuances, current agent systems typically neglect such variance during tool selection [Wang et al.,\n2024a]. In contrast, TOOLMEMrecords empirical performance signals for each tool, enabling agents\nto make accuracy-aware decisions during task solving.\nMemory-Augmented AgentsEffective memory systems enable agents to operate over extended\ntasks. Foundational work like neural Turing machines [Graves et al., 2014] introduced external\nmemory modules for querying past states. Recent systems such as Reflexion [Shinn et al., 2023]\nstore in-session NL-formatted feedback as memory, while AWM [Wang et al., 2024c] captures\nreusable workflows across tasks. MemGPT [Packer et al., 2024] and MemoryBank [Zhong et al.,\n2023] separate memory into short- and long-term slots with dynamic update strategies. While these\nmethods enhance general reasoning and task procedures of agents, they do not model the behavior of\nexternal tools, especially those with functionalities unknown prior to testing. In contrast, TOOLMEM\nexplicitly tracks tool-specific strengths and weaknesses, facilitating smarter tool selection for use.\n6 Conclusion and Future Work\nWe"}
{"file": "2510.06664v1.pdf", "section": "5 Related Work", "chunk": ", they do not model the behavior of\nexternal tools, especially those with functionalities unknown prior to testing. In contrast, TOOLMEM\nexplicitly tracks tool-specific strengths and weaknesses, facilitating smarter tool selection for use.\n6 Conclusion and Future Work\nWe introduced TOOLMEM, a closed-loop framework that equips multimodal agents with a learnable\nand evolving memory of tool capabilities. By integrating structured memory initialization based\non a proficiency-level taxonomy, feedback-driven learning from LLM-generated critiques, and\nretrieval-augmented generation for memory refinement, TOOLMEMenables agents to continually\nimprove their understanding of tool behaviors. Our experiments demonstrate that agents augmented\nwith TOOLMEMachieve more accurate tool performance estimation and make better-informed tool\nchoices, leading to improved outcomes in both text and image generation tasks.\nThis work opens up several promising directions for future exploration. One compelling avenue is\nextending ToolMem to supporta wider array of generative and decision-making toolsacross domains\nsuch as multimodal editing and code synthesis. Another direction is developingmore advanced\nautomated feedbackmechanisms to support autonomous memory construction. Additionally, ana-\nlyzing thetheoretical underpinnings of memorymaintenance can offer new insights into long-term\nmemory scalability. Finally, incorporatinghuman-in-the-loopandprincipled memory consolidation\n9\nstrategies may enhance learning from sparse or noisy experiences. We hope TOOLMEMcan serve as a\nfoundation for building adaptive agents that learn from prior interactions without repeated retraining,\npaving the way for more efficient, resilient, and generalizable AI systems.\nReferences\nAlejandro Carrasco, Marco Nedungadi, Victor Rodriguez-Fernandez, and Richard Linares. Visual\nlanguage models as operator agents in the space domain. InAIAA SCITECH 2025 Forum. American\nInstitute of Aeronautics and Astronautics, January 2025. doi: 10.2514/6.2025-1543. URL\nhttp://dx.doi.org/10.2514/6.2025-1543.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-\ndomain questions, 2017. URLhttps://arxiv.org/abs/1704.00051.\nMatthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyungh"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "domain questions, 2017. URLhttps://arxiv.org/abs/1704.00051.\nMatthew Dunn, Levent Sagun, Mike Higgins, V . Ugur Guney, V olkan Cirik, and Kyunghyun Cho.\nSearchqa: A new q&a dataset augmented with context from a search engine, 2017. URL https:\n//arxiv.org/abs/1704.05179.\nDifei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou.\nAssistgpt: A general multi-modal assistant that can plan, execute, inspect, and learn, 2023. URL\nhttps://arxiv.org/abs/2306.08640.\nZhi Gao, Bofei Zhang, Pengxiang Li, Xiaojian Ma, Tao Yuan, Yue Fan, Yuwei Wu, Yunde Jia,\nSong-Chun Zhu, and Qing Li. Multi-modal agent tuning: Building a vlm-driven agent for efficient\ntool usage, 2025. URLhttps://arxiv.org/abs/2412.15606.\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014. URL https:\n//arxiv.org/abs/1410.5401.\nZhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong\nSun, and Yang Liu. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of\nlarge language models, 2024.\nTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning\nwithout training, 2022. URLhttps://arxiv.org/abs/2211.11559.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Realm: Retrieval-\naugmented language model pre-training, 2020. URLhttps://arxiv.org/abs/2002.08909.\nVladimir Karpukhin, Barlas O \u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen tau Yih. Dense passage retrieval for"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": ".\nVladimir Karpukhin, Barlas O \u02d8guz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen tau Yih. Dense passage retrieval for open-domain question answering, 2020. URL\nhttps://arxiv.org/abs/2004.04906.\nSeungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin\nSon, Yejin Cho, Sheikh Shafayat, Jinheon Baek, Sue Hyun Park, Hyeonbin Hwang, Jinkyung Jo,\nHyowon Cho, Haebin Shin, Seongyun Lee, Hanseok Oh, Noah Lee, Namgyu Ho, Se June Joo,\nMiyoung Ko, Yoonjoo Lee, Hyungjoo Chae, Jamin Shin, Joel Jang, Seonghyeon Ye, Bill Yuchen\nLin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. The biggen\nbench: A principled benchmark for fine-grained evaluation of language models with language\nmodels, 2025. URLhttps://arxiv.org/abs/2406.05761.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal,\nHeinrich K \u00a8uttler, Mike Lewis, Wen tau Yih, Tim Rockt \u00a8aschel, Sebastian Riedel, and Douwe\nKiela. Retrieval-augmented generation for knowledge-intensive nlp tasks, 2021. URL https:\n//arxiv.org/abs/2005.11401.\nBaiqi Li, Zhiqiu Lin, Deepak Pathak, Jiayao Li, Yixin Fei, Kewen Wu, Tiffany Ling, Xide Xia,\nPengchuan Zhang, Graham Neubig, and Deva Ramanan. Genai-bench: Evaluating and improving\ncompositional text-to-visual generation, 2024. URLhttps://arxiv.org/abs/2406.13743.\nMinghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li,"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "positional text-to-visual generation, 2024. URLhttps://arxiv.org/abs/2406.13743.\nMinghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei\nHuang, and Yongbin Li. API-bank: A comprehensive benchmark for tool-augmented LLMs. In\nProceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023.\nURLhttps://aclanthology.org/2023.emnlp-main.187/.\n10\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang, Shashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. Self-refine: Iterative\nrefinement with self-feedback, 2023. URLhttps://arxiv.org/abs/2303.17651.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William Saunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\nGretchen Krueger, Kevin Button, Matthew Knight, Benjamin Chess, and John Schulman. Webgpt:\nBrowser-assisted question-answering with human feedback, 2022. URL https://arxiv.org/\nabs/2112.09332.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,\nLuke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and\nRyan Lowe. Training language models to follow instructions with human feedback, 2022. URL\nhttps://arxiv.org/abs/2203.02155.\nCharles Packer, Sarah Wooders, Kevin Lin, Vivian Fang,"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "and\nRyan Lowe. Training language models to follow instructions with human feedback, 2022. URL\nhttps://arxiv.org/abs/2203.02155.\nCharles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, and Joseph E.\nGonzalez. Memgpt: Towards llms as operating systems, 2024. URL https://arxiv.org/abs/\n2310.08560.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein,\nDahai Li, Zhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master\n16000+ real-world apis, 2023a. URLhttps://arxiv.org/abs/2307.16789.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li,\nZhiyuan Liu, and Maosong Sun. Toolllm: Facilitating large language models to master 16000+\nreal-world apis, 2023b.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.\nLearning transferable visual models from natural language supervision, 2021. URL https:\n//arxiv.org/abs/2103.00020.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess `\u0131, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools, 2023. URLhttps://ar"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "a Raileanu, Maria Lomeli, Luke Zettlemoyer,\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\nuse tools, 2023. URLhttps://arxiv.org/abs/2302.04761.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt:\nSolving ai tasks with chatgpt and its friends in hugging face, 2023. URL https://arxiv.org/\nabs/2303.17580.\nNoah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu\nYao. Reflexion: Language agents with verbal reinforcement learning, 2023.\nShang Hong Sim, Clarence Lee, Alvin Tan, and Cheston Tan. Evaluating the generation of spatial\nrelations in text and image generative models, 2024. URL https://arxiv.org/abs/2411.\n07664.\nD\u00b4\u0131dac Sur \u00b4\u0131s, Sachit Menon, and Carl V ondrick. Vipergpt: Visual inference via python execution for\nreasoning, 2023. URLhttps://arxiv.org/abs/2303.08128.\nQiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. Toolal-\npaca: Generalized tool learning for language models with 3000 simulated cases.arXiv preprint\narXiv:2306.05301, 2023.\nYunlong Tang, Jing Bi, Chao Huang, Susan Liang, Daiki Shimada, Hang Hua, Yunzhong Xiao,\nYizhi Song, Pinxin Liu, Mingqian Feng, Junjia Guo, Zhuo Liu, Luchuan Song, Ali V osoughi,\nJinxi He, Liu He, Zeliang Zhang, Jiebo Luo, and Chenliang Xu. Caption anything in video:\nFine-grained object-centric captioning via spatiotemporal multimodal prompting, 2025a. URL\nhttps://arxiv.org/abs/2504.05541.\n11\nYunlong Tang,"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "Caption anything in video:\nFine-grained object-centric captioning via spatiotemporal multimodal prompting, 2025a. URL\nhttps://arxiv.org/abs/2504.05541.\n11\nYunlong Tang, Junjia Guo, Pinxin Liu, Zhiyuan Wang, Hang Hua, Jia-Xing Zhong, Yunzhong Xiao,\nChao Huang, Luchuan Song, Susan Liang, Yizhi Song, Liu He, Jing Bi, Mingqian Feng, Xinyang\nLi, Zeliang Zhang, and Chenliang Xu. Generative ai for cel-animation: A survey, 2025b. URL\nhttps://arxiv.org/abs/2501.06250.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and\nAnima Anandkumar. V oyager: An open-ended embodied agent with large language models, 2023.\nURLhttps://arxiv.org/abs/2305.16291.\nZhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent for\nunified image generation and editing, 2024a. URLhttps://arxiv.org/abs/2407.05600.\nZhiruo Wang, Daniel Fried, and Graham Neubig. Trove: Inducing verifiable and efficient toolboxes\nfor solving programmatic tasks, 2024b. URLhttps://arxiv.org/abs/2401.12869.\nZora Zhiruo Wang, Jiayuan Mao, Daniel Fried, and Graham Neubig. Agent workflow memory, 2024c.\nURLhttps://arxiv.org/abs/2409.07429.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual\nchatgpt: Talking, drawing and editing with visual foundation models, 2023. URL https://\narxiv.org/abs/2303.04671.\nJunlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. Large multimodal agents: A\nsurvey, 2024. URLhttps"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "/abs/2303.04671.\nJunlin Xie, Zhihong Chen, Ruifei Zhang, Xiang Wan, and Guanbin Li. Large multimodal agents: A\nsurvey, 2024. URLhttps://arxiv.org/abs/2402.15116.\nWujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, and Yongfeng Zhang. A-mem: Agentic\nmemory for llm agents, 2025. URLhttps://arxiv.org/abs/2502.12110.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models, 2023. URL https://arxiv.org/\nabs/2210.03629.\nWanjun Zhong, Lianghong Guo, Qiqi Gao, He Ye, and Yanlin Wang. Memorybank: Enhancing large\nlanguage models with long-term memory, 2023. URLhttps://arxiv.org/abs/2305.10250.\n12\nA Analysis: Number of Memory Entries to Retrieve\nWe further conduct an in-depth analysis on top- kretrieval granularity, by retrieving varying numbers\nof memory entries from TOOLMEM.\nAs shown in Figure 3, We also compare performance when using varyingtop-kvalues (0\u201324)4.\nFigure 3 shows that largertop-kvalues generally reduce MAE and RMSE but eventually plateau or\ndegrade due to noise from excessive retrieval. Accuracy follows a similar trend, often peaking around\ntop-k= 10\u201314. For example, TOOLMEM\u2019s predictions for Midjourney 6 improve from an MAE of\n0.98 attop-k= 0 to 0.91 attop-k= 18; SDXL Turbo\u2019s MAE falls from 1.24 to 1.06 over the same\nrange. These results suggest that leveraging a moderate number of high-relevance memory items\nhelps balance predictive accuracy and computational efficiency.\n0 5 10 15 20 25\nTop-K0.91.01.11.21.31.4Score\nMae\n0 5 10 15 20 25\nTop-K1.11"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": ".\n0 5 10 15 20 25\nTop-K0.91.01.11.21.31.4Score\nMae\n0 5 10 15 20 25\nTop-K1.11.21.31.41.51.6\nRmse\n0 5 10 15 20 25\nTop-K0.100.150.200.250.300.350.40\nAccuracy\nDALLE_3 DeepFloyd_I_XL_v1 Midjourney_6 SDXL_2_1 SDXL_Base SDXL_Turbo\nFigure 3: TOOLMEM\u2019s score prediction accuracy on six text-to-image models across different top- k\nmemory retrieval sizes, evaluated on 100 samples. The dashed line at k= 12 indicates most models\u2019\nempirically optimal memory size.\nB Prompt Templates\nB.1 Text to Image Tools\nPrompt for Feedback Generation\nThe text prompt used to generate the image is \u201c {prompt} \u201d. The Scoring metric used here is\na 5-point Likert scale:\n1 (Does not match at all): The generated image or video completely fails to align with the\ntext prompt. It either lacks the required elements or depicts a scenario that contradicts the\nprompt.\n2 (Has significant discrepancies): There are major mismatches between the text prompt\nand the visual output. Key elements may be missing, or relationships and attributes are\nsignificantly misrepresented.\n3 (Has several minor discrepancies): The output aligns with the prompt in a general sense\nbut has noticeable errors or omissions in key details. While the main elements are present,\nfiner aspects are not accurately rendered.\n4 (Has a few minor discrepancies): The output is mostly aligned with the prompt, with only\na few small and less critical details being inaccurate or missing. The visual matches the\ndescription well but not perfectly.\n5 (Matches exactly): The generated image or video perfectly matches the text prompt. All\nelements, relationships, attributes, and details align seamlessly with the description, with no\nnoticeable discrepancies.\nThis image scored {score} based on the human ratings. Now tell me why you think the\nimage is or isn\u2019t following the text prompt.\n4Whentop-k= 0, predictions are made using only the tool name and its overview, without retrieving any\nmemory items.\n13\nPrompt for Memory Refinement\nYou are"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "the\nimage is or isn\u2019t following the text prompt.\n4Whentop-k= 0, predictions are made using only the tool name and its overview, without retrieving any\nmemory items.\n13\nPrompt for Memory Refinement\nYou are an assistant responsible for maintaining an incremental memory of the tool\u2019s\ncapabilities, limitations, and use cases.\nYou will receive a current memory and new feedback. Your job is to update the memory by\nintegrating the new feedback without losing previously stored information.\nRules:\n1. Start with the current memory.\n2. Consider the new prompt and feedback, but do not simply repeat the feedback, think deep\nand get general insights.\n3. Incrementally refine and update the memory to reflect any new insights.\n4. Do not remove previously known capabilities unless the new feedback explicitly\ncontradicts them.\n5. If no new insights are provided, leave the memory unchanged.\nInstructions:\nFollow this procedure strictly:\n- Review the current memory for model{tool name}:\n\u201c{current memory}\u201d\n- Input task prompt to the tool was:\n\u201c{task prompt}\u201d\n- Tool\u2019s answer was:\n\u201c{response}\u201d\n- Score rubric was:\n\u201c{score rubric}\u201d\n- The score for tool\u2019s response was:\n\u201c{score}\u201d\n- Feedback for this answer received:\n\u201c{feedback}\u201d\nIntegrate the key insights of the tool\u2019s limitations and capabilities from the feedback into\nthe existing memory. Use complete sentences starting with phrases like good at ,bad at ,\nproficient at, orpoor atto describe your findings.\n- If the feedback suggests new limitations, capabilities, or clarifications, add them.\n- If the feedback suggests that something previously stated is incorrect, correct it.\n- If the feedback suggests the tool struggles with a certain aspect (like counting multiple\nobjects), note it.\n- If no contradiction or new insight is provided, keep the memory as is.\n- Think deeply and determine what the feedback actually means for the model\u2019s capability,\nrather than staying on the surface of a specific example.\nIn your final answer, outputONLYthe updated overall memory of the tool. Do not include\nextra explanations.\nPredict Image Description - GENERIC\nYou are familiar with how the model{model name}generates images.\nYour current memory of{model name}is: \u201cA text to image model\u201d\nBased on this memory, predict how an image generated by {model name} for the following\nprompt would look"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "how the model{model name}generates images.\nYour current memory of{model name}is: \u201cA text to image model\u201d\nBased on this memory, predict how an image generated by {model name} for the following\nprompt would look:\nPrompt: \u201c{prompt}\u201d\nDescribe the expected image in no more than 50 English words. Be concise and specific.\nOnly return the description text.\n14\nPredict Image Description - FEW-SHOT\nYou are familiar with how the model{self.model name}generates images.\nYour current memory of{self.model name}is: \u201c{current memory}\u201d\nHere are some examples of the model\u2019s performance on specific prompts:\n{few shot memory}\nThe scoring metric used here is a 5-point Likert scale:\n1 (Does not match at all): The generated image or video completely fails to align with the\ntext prompt. It either lacks the required elements or depicts a scenario that contradicts the\nprompt.\n2 (Has significant discrepancies): There are major mismatches between the text prompt\nand the visual output. Key elements may be missing, or relationships and attributes are\nsignificantly misrepresented.\n3 (Has several minor discrepancies): The output aligns with the prompt in a general sense\nbut has noticeable errors or omissions in key details. While the main elements are present,\nfiner aspects are not accurately rendered.\n4 (Has a few minor discrepancies): The output is mostly aligned with the prompt, with only\na few small and less critical details being inaccurate or missing. The visual matches the\ndescription well but not perfectly.\n5 (Matches exactly): The generated image or video perfectly matches the text prompt. All\nelements, relationships, attributes, and details align seamlessly with the description, with no\nnoticeable discrepancies.\nBased on these examples and your memory of {self.model name} , predict how an image\ngenerated by{self.model name}for the following prompt would look:\nPrompt: \u201c{prompt}\u201d\nDescribe the expected image in no more than 50 English words. Be concise and specific.\nOnly return the description text.\nPredict Image Description - TOOLMEM\nYou are familiar with how the model{model name}generates images.\nYour current memory of{model name}is: \u201c{current memory}\u201d\nBased on this memory, predict how an image generated by {model name} for the following\nprompt would look:\nPrompt: \u201c{prompt}\u201d\nDescribe the expected image in"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "current memory of{model name}is: \u201c{current memory}\u201d\nBased on this memory, predict how an image generated by {model name} for the following\nprompt would look:\nPrompt: \u201c{prompt}\u201d\nDescribe the expected image in no more than 50 English words. Be concise and specific.\nOnly return the description text.\nPredict Score - GENERIC\nYour current memory (knowledge) of the tool{model name}is: \u201cA text to image model\u201d\nBased on your memory of model {model name} , predict the score for the following prompt:\n\u201c{prompt}\u201d.\nThe scoring metric used here is a 5-point Likert scale:\n1 (Does not match at all): The generated image or video completely fails to align with the\ntext prompt. It either lacks the required elements or depicts a scenario that contradicts the\nprompt.\n2 (Has significant discrepancies): There are major mismatches between the text prompt\nand the visual output. Key elements may be missing, or relationships and attributes are\n15\nsignificantly misrepresented.\n3 (Has several minor discrepancies): The output aligns with the prompt in a general sense\nbut has noticeable errors or omissions in key details. While the main elements are present,\nfiner aspects are not accurately rendered.\n4 (Has a few minor discrepancies): The output is mostly aligned with the prompt, with only\na few small and less critical details being inaccurate or missing. The visual matches the\ndescription well but not perfectly.\n5 (Matches exactly): The generated image or video perfectly matches the text prompt. All\nelements, relationships, attributes, and details align seamlessly with the description, with no\nnoticeable discrepancies.\nNow return a score between 1 and 5 for prompt {prompt} , based on your memory of the\ntool{model name}. Return a single number only, nothing else.\nPredict Score - FEW-SHOT\nYour current memory (knowledge) of the tool {self.model name} is: \u201cA text to image\nmodel\u201d.\nHere are some examples of the model\u2019s performance on specific prompts:\n{samples prompt}.\nBased on these examples and your memory of {self.model name} , predict the score this\nmodel will get for answering the following task prompt: \u201c{prompt}\u201d.\nThe scoring metric used here is a 5-point Likert scale:\n1 (Does not match at all): The model\u2019s response completely fails to align with the task prompt.\nIt"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "get for answering the following task prompt: \u201c{prompt}\u201d.\nThe scoring metric used here is a 5-point Likert scale:\n1 (Does not match at all): The model\u2019s response completely fails to align with the task prompt.\nIt either lacks the required elements or presents information that contradicts the prompt.\n2 (Has significant discrepancies): There are major mismatches between the task prompt\nand the model\u2019s response. Key elements may be missing, or relationships and attributes are\nsignificantly misrepresented.\n3 (Has several minor discrepancies): The response aligns with the prompt in a general sense\nbut has noticeable errors or omissions in key details. While the main elements are present,\nfiner aspects are not accurately rendered.\n4 (Has a few minor discrepancies): The response is mostly aligned with the prompt, with\nonly a few small and less critical details being inaccurate or missing. The response matches\nthe description well but not perfectly.\n5 (Matches exactly): The model\u2019s response perfectly matches the task prompt. All elements,\nrelationships, attributes, and details align seamlessly with the description, with no noticeable\ndiscrepancies.\nNow, return a score between 1 and 5 based on your memory of {self.model name} \u2019s\ncapabilities and limitations. Return a single number only, nothing else.\nPredict Score - TOOLMEM\nYour current memory (knowledge) of the tool{model name}is: \u201c{current memory}\u201d\nBased on your memory of model {model name} , predict the score for the following prompt:\n\u201c{prompt}\u201d.\nThe scoring metric used here is a 5-point Likert scale:\n1 (Does not match at all): The generated image or video completely fails to align with the\ntext prompt. It either lacks the required elements or depicts a scenario that contradicts the\nprompt.\n2 (Has significant discrepancies): There are major mismatches between the text prompt\nand the visual output. Key elements may be missing, or relationships and attributes are\nsignificantly misrepresented.\n3 (Has several minor discrepancies): The output aligns with the prompt in a general sense\nbut has noticeable errors or omissions in key details. While the main elements are present,\n16\nfiner aspects are not accurately rendered.\n4 (Has a few minor discrepancies): The output is mostly aligned with the prompt, with only\na few small and less critical details being inaccurate or missing. The visual matches the\ndescription well but not perfectly.\n5"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "aspects are not accurately rendered.\n4 (Has a few minor discrepancies): The output is mostly aligned with the prompt, with only\na few small and less critical details being inaccurate or missing. The visual matches the\ndescription well but not perfectly.\n5 (Matches exactly): The generated image or video perfectly matches the text prompt. All\nelements, relationships, attributes, and details align seamlessly with the description, with no\nnoticeable discrepancies.\nNow return a score between 1 and 5 for prompt {prompt} , based on your memory of the\ntool{model name}. Return a single number only, nothing else.\nB.2 Text Generation Tools\nPrompt for Memory Refinement\nYou are an assistant responsible for maintaining an incremental memory of the tool\u2019s\ncapabilities, limitations, and use cases.\nYou will receive a current memory and new feedback. Your job is to update the memory by\nintegrating the new feedback without losing previously stored information.\nRules:\n1. Start with the current memory.\n2. Consider the new prompt and feedback, but do not simply repeat the feedback, think deep\nand get general insights.\n3. Incrementally refine and update the memory to reflect any new insights.\n4. Do not remove previously known capabilities unless the new feedback explicitly\ncontradicts them.\n5. If no new insights are provided, leave the memory unchanged.\nInstructions:\nFollow this procedure strictly:\n- Review the current memory for model{tool name}:\n\u201c{current memory}\u201d\n- Input task prompt to the tool was:\n\u201c{task prompt}\u201d\n- Tool\u2019s answer was:\n\u201c{response}\u201d\n- Score rubric was:\n\u201c{score rubric}\u201d\n- The score for tool\u2019s response was:\n\u201c{score}\u201d\n- Feedback for this answer received:\n\u201c{feedback}\u201d\nIntegrate the key insights of the tool\u2019s limitations and capabilities from the feedback into\nthe existing memory. Use complete sentences starting with phrases like good at ,bad at ,\nproficient at, orpoor atto describe your findings.\n- If the feedback suggests new limitations, capabilities, or clarifications, add them.\n- If the feedback suggests that something previously stated is incorrect, correct it.\n- If the feedback suggests the tool struggles with a certain aspect (like counting multiple\nobjects), note it.\n- If no contradiction or new insight is provided, keep the memory as is.\n- Think deeply and determine what the feedback actually means for the model\u2019s capability,\nrather than staying"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "struggles with a certain aspect (like counting multiple\nobjects), note it.\n- If no contradiction or new insight is provided, keep the memory as is.\n- Think deeply and determine what the feedback actually means for the model\u2019s capability,\nrather than staying on the surface of a specific example.\n17\nIn your final answer, outputONLYthe updated overall memory of the tool. Do not include\nextra explanations.\nPredict Score - GENERIC\nYour current memory (knowledge) of the tool{model name}is: \u201cA large language model\u201d\nBased on your memory of model {model name} , predict the score this model will get for\nanswering the following task prompt: \u201c{prompt}\u201d.\nThe scoring metric used here is a 5-point Likert scale:\n{rubric}\nNow return a score between 1 and 5, based on your memory of {model name} \u2019s capabilities\nand limitations.\nReturn a single number only, nothing else.\nPredict Score - FEW-SHOT\nYour current memory (knowledge) of the tool{model name}is: A large language model.\nHere are some examples of the model\u2019s performance on some tasks:\nTask Prompt:{task prompt}\nRubric:{rubric}\nModel\u2019s Score:{score}\nTask Prompt:{task prompt}\nRubric:{rubric}\nModel\u2019s Score:{score}\nTask Prompt:{task prompt}\nRubric:{rubric}\nModel\u2019s Score:{score}\n...\nBased on those samples and your memory of {model name} , predict the score this model\nwill get for answering the following task prompt: \u201c{task prompt}\u201d.\nThe scoring metric used here is a 5-point Likert scale:\n{rubric}\nNow return a score between 1 and 5, based on your memory of {model name} \u2019s capabilities\nand limitations.\nReturn a single number only, nothing else.\nPredict Score - TOOLMEM\nYour current memory (knowledge) of the tool{model name}is:{current memory}\nBased on your memory of model {model name} , predict the score this model will get for\nanswering the following task prompt: \u201c{prompt}\u201d.\nThe scoring metric used here is a 5-point Likert scale:\n{rubric}\n18\nNow return a score between 1 and 5, based on your memory of {model name} \u2019s capabilities\nand limitations.\nReturn a single number only, nothing else.\nFigure 4:"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "Likert scale:\n{rubric}\n18\nNow return a score between 1 and 5, based on your memory of {model name} \u2019s capabilities\nand limitations.\nReturn a single number only, nothing else.\nFigure 4: Qualitative examples for TOOLMEM. The left image illustrates the prediction for \u201cAn oil\npainting of a majestic lion hanging above the fireplace,\u201d while the right example shows the prediction\nfor \u201cA pair of socks, one is striped, but the other is not.\u201d\nC Case Study\nTo better understand TOOLMEM\u2019s capability in performance prediction, we present two representative\nexamples in Figure 4, where TOOLMEMis prompted to explicitly forecast the quality and fidelity of\nthe output images generated by the SDXL-Turbo tool.\nExample 1: Majestic Lion Above the Fireplace.For the prompt \u201cAn oil painting of a majestic\nlion hanging above the fireplace,\u201d TOOLMEMassigns a predicted score of 4 (minor discrepancies). In\npractice, SDXL-Turbo successfully produces a visually appealing scene with a detailed lion portrait.\nHowever, our system anticipates that fine spatial alignment may not be perfectly captured (e.g., the\nfireplace might be cropped or misaligned). Indeed, the actual generated image exhibits a slightly off\npositioning of the lion, confirming TOOLMEM\u2019s prediction. Notably, the final score aligns closely\nwith human evaluations, underlining TOOLMEM\u2019s proficiency in assessing model outputs.\nExample 2: Pair of Socks with Different Patterns.For the prompt \u201cA pair of socks, one is\nstriped, but the other is not,\u201d TOOLMEMpredicts a score of 3 (several minor discrepancies). While\nSDXL-Turbo generally handles simple object prompts and textures adequately, TOOLMEMnotes the\nmodel\u2019s recurring difficulty in distinguishing subtle visual contrasts between highly similar objects.\nIn the actual output, both socks often appear identical (both striped), matching TOOLMEM\u2019s forecast.\nHowever, human evaluators deemed these inaccuracies more severe, giving a lower score of 2. This\ndiscrepancy suggests that although TOOLMEMdetects the correct type of error, it occasionally assigns\nhigher tolerance to such mismatches than human annotators.\nDiscussion.These examples highlight both the strengths and limitations of TOOLMEM. On one\nhand, TOOLMEMdemonstrates strong alignment with human judgments when identifying whether\nthe main elements of a prompt are preserved or substantially distorted. On"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "annotators.\nDiscussion.These examples highlight both the strengths and limitations of TOOLMEM. On one\nhand, TOOLMEMdemonstrates strong alignment with human judgments when identifying whether\nthe main elements of a prompt are preserved or substantially distorted. On the other hand, its scoring\n19\nmechanism sometimes diverges from that of human evaluators, underscoring the subjective nature of\nvisual quality assessments.\nD Compute Resource and Reproducibility Details\nThe core computation in our experiments is conducted via OpenAI\u2019s API, GPT-4o-mini serves as\nthe backbone language model for TOOLMEM, and text-embedding-ada-002 for RAG tasks. As our\nframework relies on API calls, it does not require any local GPU hardware for inferences.\nAll other models involved\u2014such as Claude-3, DALL\u00b7E 3, Midjourney, SDXL, and others\u2014are part\nof the precomputed outputs provided by the GENAI-BENCHand BIGGENBENCHdatasets. We do\nnot rerun these tools; we only analyze and build memory around their existing outputs.\nThe only locally executed model in our pipeline is the clip-flant5-xl model used to calculate VQA\nScore, which evaluates image-description alignment. This model is run on an NVIDIA A100 GPU,\nand is highly efficient\u2014each VQA score can be computed in about 0.2 seconds. As such, the compute\nrequirement is minimal, and large batches can be processed in real time.\nOverall, our pipeline is lightweight and fully reproducible. All heavy model inference is handled\nexternally via OpenAI APIs, and the only local requirement is for lightweight VQA scoring, making\nTOOLMEMpractical to deploy and evaluate with modest resources.\nE Broader Impact and Limitation\nThis paper investigates a new capability for agents powered by Large Language Models (LLMs):\nbuilding and utilizing structured memory representations of tool capabilities to improve tool selection\nand task-solving performance. By enabling agents to systematically record and retrieve performance\nfeedback from past tool interactions, our proposed TOOLMEMframework pushes toward more\nautonomous and adaptive decision-making systems.\nThe broader impact of this line of work lies in its potential to make complex multi-tool reasoning more\nrobust, transparent, and efficient. Applications that involve numerous generative or decision-support\ntools\u2014such as multimodal assistants, educational platforms, or creative design systems\u2014could\nbenefit from better memory"}
{"file": "2510.06664v1.pdf", "section": "6 Conclusion and Future Work", "chunk": "potential to make complex multi-tool reasoning more\nrobust, transparent, and efficient. Applications that involve numerous generative or decision-support\ntools\u2014such as multimodal assistants, educational platforms, or creative design systems\u2014could\nbenefit from better memory-guided tool selection, reducing trial-and-error usage and model waste.\nTOOLMEMmay also lower the barrier for non-experts to leverage advanced model capabilities, as\nthe system autonomously adapts based on prior observations.\nHowever, several limitations and risks must be acknowledged. First, the memory quality heavily\ndepends on the accuracy and reliability of the feedback used to construct it. In this work, we\nrely on GPT-4-generated scores and rubrics, which, although effective, may introduce biases or\ninconsistencies not present in human-grounded evaluations. Moreover, the framework assumes tool\ncapabilities remain relatively stable over time. In real-world deployments, frequent model updates or\nusage context shifts may render historical memory entries stale or misleading.\nAnother concern is over-reliance on memorized preferences, which may lead to suboptimal or unfair\ntool choices in edge cases not well-represented in the agent\u2019s prior experiences. Without proper\nmonitoring, such systems might reinforce early biases or miss the opportunity to explore potentially\nbetter alternatives.\nLastly, our experiments are conducted in controlled benchmark environments with known tool outputs\nand synthetic feedback. Generalizing TOOLMEMto open-ended tasks, dynamic tool ecosystems, or\nsafety-critical applications requires additional safeguards and validation, which we leave for future\nwork.\nWhile promising, TOOLMEMrepresents an early step toward memory-augmented tool reasoning,\nand further exploration is needed to ensure both its robustness and its alignment with human-centered\nvalues in broader deployment contexts.\n20"}
{"file": "2510.06911v1.pdf", "section": "Introduction", "chunk": "LLM-Assisted Modeling of Semantic\nWeb-Enabled Multi-Agents Systems with AJAN\nHacane Hechehouche1, Andr\u00e9 Antakli2, and Matthias Klusch2\n1Daimler Buses GmbH\nCarl-Zeiss-Stra\u00dfe 2, Neu-Ulm, Germany\nhacane.hechehouche@daimlertruck.com\n2German Research Center for Artificial Intelligence (DFKI)\nSaarland Informatics Campus, Saarbruecken, Germany\n{andre.antakli, matthias.klusch}@dfki.de\nAbstract.There are many established semantic Web standards for im-\nplementingmulti-agentdrivenapplications.TheAJANframeworkallows\nto engineer multi-agent systems based on these standards. In particular,\nagent knowledge is represented in RDF/RDFS and OWL, while agent\nbehavior models are defined with Behavior Trees and SPARQL to ac-\ncess and manipulate this knowledge. However, the appropriate definition\nof RDF/RDFS and SPARQL-based agent behaviors still remains a ma-\njor hurdle not only for agent modelers in practice. For example, dealing\nwith URIs is very error-prone regarding typos and dealing with complex\nSPARQL queries in large-scale environments requires a high learning\ncurve. In this paper, we present an integrated development environment\nto overcome such hurdles of modeling AJAN agents and at the same time\nto extend the user community for AJAN by the possibility to leverage\nLarge Language Models for agent engineering.\n1 Introduction\nThe AJAN (Accessible Java Agent Nucleus) multi-agent engineering framework\n[1] has already been applied across a wide range of domains, demonstrating the\nversatility of its approach to modeling agents entirely in RDF. A central element\nof AJAN is its use ofSPARQL Behavior Trees (SBTs)to describe agent be-\nhavior. This approach leverages SPARQL\u2014a powerful query language for RDF\ndata\u2014to access and manipulate agent knowledge, while also relying on the mod-\nularstructureofBehaviorTrees,originallydevelopedinthegamingindustryand\nnow widely adopted in robotics for high-level behavior programming. The use of\nSBTs enables not only reusability of reactive behavior modules but also seam-\nless integration of additional AI techniques. However, defining agents and their\nbehaviors entirely in RDF and SPARQL poses a significant challenge\u2014even for\nexperts in the Semantic Web field\u2014"}
{"file": "2510.06911v1.pdf", "section": "1 Introduction", "chunk": "only reusability of reactive behavior modules but also seam-\nless integration of additional AI techniques. However, defining agents and their\nbehaviors entirely in RDF and SPARQL poses a significant challenge\u2014even for\nexperts in the Semantic Web field\u2014due to the technical complexity and lack of\nintuitive tooling. Recent developments in generative AI, particularly with Large\nLanguage Models (LLM), enable the use of the most natural form of communi-\ncation\u2014natural language\u2014for the creation of MAS-driven applications. WhenarXiv:2510.06911v1  [cs.AI]  8 Oct 2025\n2 H. Hechehouche et al.\nconsidering AJAN behaviors described in RDF while combining Behavior Trees\n(LLM approaches: [3,7]) and SPARQL (LLM approaches: [10,8]), a holistic so-\nlution must be found that is modular and applicable to dynamic domains.\nTo address this challenge, we developed aweb-based graphical editor for AJAN\nthat combines graphical and tabular interfaces with core features of modern\nintegrated development environments (IDEs). In particular, recent advances in\nthe use of LLMs have been incorporated into the editor to offer a natural, user-\nfriendly interface for modeling agent behavior and interacting directly with an\nagentactinginadynamicenvironment.Thatenablesevennon-expertstoengage\nwith the AJAN framework for agent modeling more effectively and lowers the\nentry barrier for agent modeling. This paper is structured as follows: Section\n2 introduces the AJAN-Editor, followed by its LLM-based NLP extension in\nSection 3. Section 4 presents an application example. Related work is discussed\nin Section 5, and conclusions are drawn in Section 6.\n2 Hypermedia Agent Modeling\nFig.1: Behavior editor to model SPARQL-BTs within the AJAN-Editor\nThe AJAN-Editor, open-source available on GitHub3, is a web-based graphical\ninterface for modeling, executing, and debugging agents in the AJAN frame-\nwork. It addresses the challenge of authoring RDF-based agent models and\nSPARQL Behavior Trees (SBTs) by providing an interactive environment that\nlowers the entry barrier for users. Developed withEmber.js4,Node.js5and\nCytoscape.js6, the editor connects to the AJAN-Service and RDF triplestores\n(e.g.,RDF"}
{"file": "2510.06911v1.pdf", "section": "2 Hypermedia Agent Modeling", "chunk": "interactive environment that\nlowers the entry barrier for users. Developed withEmber.js4,Node.js5and\nCytoscape.js6, the editor connects to the AJAN-Service and RDF triplestores\n(e.g.,RDF4JorGraphDB), enabling the creation and management of agent tem-\nplates including endpoints, events, goals, and SBTs. The editor features visual\nand tabular tools for constructing event-driven agent behavior via SPARQL\nqueries, and supports live monitoring, execution tracing, and model updates; it\nalso facilitates collaborative development and component reuse.\n3AJAN-Editor: https://anonymous.4open.science/r/AJAN-editor-0E30/\n4EmberJS:https://emberjs.com/\n5NodeJS:https://nodejs.org/en\n6Cytoscape:https://js.cytoscape.org/\nLLM-Assisted Agent Modeling with AJAN 3\n2.1 The AJAN Framework\nThe AJAN service forms the runtime core of the AJAN framework, enabling exe-\ncution of Semantic Web-enabled agents and multi-agent systems through REST-\nful Linked Data (LD) interfaces. Agent models and behaviors are stored in RDF\nand manipulated via SPARQL. Agents follow the BDI paradigm, with RDF-\nbased knowledge, SPARQLASK-defined goals, and behavior trees expressed as\nSPARQL-BTs. These SBTs use SPARQLASK,SELECT,UPDATE, andCONSTRUCT\nfor decision-making and communication. The modular SBT structure allows for\nreuse and adaptation of agent logic. For more details, we refer the reader to [1].\n2.2 AJAN-Editor: Overview\nTheAgents tabof the AJAN-Editor enables the user to configure and instanti-\nate agents from RDF-defined agent templates, each specifying behaviors, events,\ngoals, and endpoints. Users can assign URIs as agent endpoints, link SBTs to\nevents and goals, and manage templates, while the respective interface sup-\nports validation, import/export, and modular deployment. TheBehaviors tab\n(cf. Fig. 1) offers a graphical editor for modeling SBTs using drag-and-drop of\nSBT nodes like composites, decorators, and leaves. This includes support for\nzooming, node linking, and SPARQL validation. A dynamic properties panel\nfacilitates configuration of node-specific parameters and SPARQL"}
{"file": "2510.06911v1.pdf", "section": "2.2 AJAN-Editor: Overview", "chunk": "-and-drop of\nSBT nodes like composites, decorators, and leaves. This includes support for\nzooming, node linking, and SPARQL validation. A dynamic properties panel\nfacilitates configuration of node-specific parameters and SPARQL queries. The\nActions tabsupports the definition of AJAN agent actions, including semantic\npre- and post-conditions and REST bindings for synchronous or asynchronous\nLinked Data (LD) interaction. TheQueries tabintegrates a SPARQL editor\n(Ace-based7) with resizable panes and tabbed tools for management, code in-\nsertion, and help. It enables creation, testing, and reuse of SPARQL queries\nwithintheagentdevelopmentprocess,andvisualizesqueryresultsinatabularor\ngraphical form (cf. Fig. 2). The queries can be applied to any W3C-standardized\nSPARQL endpoint8, like the AJAN agent knowledge bases (KBs).\nFig.2: Queries tab to model queries and access RDF triple stores\nTheDefinitions taballowstheusertomanagereusableRDFvocabularies,SPARQL\nsnippets,andtemplates.IntheTemplates section(cf.Fig.3),theusermaydefine\n7Ace Editor:https://ace.c9.io/\n8SPARQL 1.1 Protocol:https://www.w3.org/TR/sparql11-protocol/\n4 H. Hechehouche et al.\nparameterized SPARQL queries with custom input forms including text fields,\ndropdowns, and SPARQL-driven menus. A preview interface shows the resulting\nUI, enabling verification and reuse across agent models. These templates with\ntheir UIs can be used for SBT nodes and hide complex SPARQL queries behind\na user-friendly view in the respective node property in theBehaviors tab.\nFig.3: Templates view to create parametrized SPARQL templates and UIs\n2.3 Monitoring, Testing and Learning\nThe AJAN-Editor provides an interface for initializing and executing agents\nbased on predefined templates (cf. Fig. 4). Accessible via theAgents tab, it\noffers an overview of all instantiated agents. The instance view displays runtime\ndetails,includingtheassignedtemplateandcurrentlyexecutingbehaviors.These\nbehaviors can be monitored in real time through the live view (cf. Fig. 5), and\nthe agent\u2019s KB can be queried and visualized"}
{"file": "2510.06911v1.pdf", "section": "2.3 Monitoring, Testing and Learning", "chunk": "displays runtime\ndetails,includingtheassignedtemplateandcurrentlyexecutingbehaviors.These\nbehaviors can be monitored in real time through the live view (cf. Fig. 5), and\nthe agent\u2019s KB can be queried and visualized in theQueries tab.\nFig.4: Agent instances view to monitor initiated AJAN agents\nLLM-Assisted Agent Modeling with AJAN 5\nA log view shows the execution status of individual SBT nodes. The interface\nalso supports interaction via agent REST endpoints. When aBREAKPOINTnode\nis added in a SBT and reached during execution, the editor enters the debugging\nmode that allows the stepwise execution of the SBT, display of the node status,\nand running of associated SPARQL queries against the agent KB.\nFig.5: Live view of executed SBTs, indicating node states using colored borders\nFor testing agents and verifying Linked Data-based communication, theActions\ntab provides tools to monitor inbound agent messages and define respective syn-\nchronous or asynchronous LD response messages. The same tab includes a demo\nenvironment for the user, that shows a basic example of agent modeling with\nAJAN (cf. Fig. 6) in a classic Blocks World scenario, where an agent manip-\nulates four colored RDF-described blocks via a LD endpoint. Using predefined\nactions likePickUP,PutDown,Stack, andUnStack, the agent rearranges the\nblocks to achieve specified goals. These actions are visualized via an animated\nrobotic arm, and all message exchanges are shown to illustrate behavior exe-\ncution. Combined with the live view, debugging tools, the AJAN Wiki9, and a\npre-configured Blocks World agent behavior (cf. Fig. 1), this environment offers\na complete learning toolkit for new users.\nFig.6: World scenario demo environment to learn the modeling of agents\n9AJAN-Service Wiki:https://github.com/aantakli/AJAN-service/wiki\n6 H. Hechehouche et al.\n3 LLM-Assisted Modeling of Agents\nThe LLM-based user interface (available on GitHub10) for the Semantic Web-\nenabled AJAN framework is modular and scalable, supporting seamless human-\nagent interaction. Designed for extensibility, it integrates with AJAN\u2019s RDF-\nbased agent model and enables workflows such as behavior generation, SPARQL\nquerying, and semantic search of the AJAN Wikis.\n3."}
{"file": "2510.06911v1.pdf", "section": "3 LLM-Assisted Modeling of Agents", "chunk": "seamless human-\nagent interaction. Designed for extensibility, it integrates with AJAN\u2019s RDF-\nbased agent model and enables workflows such as behavior generation, SPARQL\nquerying, and semantic search of the AJAN Wikis.\n3.1 LLM-Based User Interface: Overview\nThe system architecture (cf. Fig. 7) comprises two main component classes:\ncommon and specialized. Common components are shared across workflows and\nform the foundation of input processing. These components include theParser\nfor syntactic and semantic analysis, theLinkerfor mapping entities to RDF\nresources, and theDisambiguatorfor resolving ambiguities. In this context,\nthe usedElasticsearchengine11supports fuzzy matching and retrieval from\nthe agent KB. The user-facing layer includes chat and orchestration modules.\nTheASR(Automatic Speech Recognition) transcribes spoken input, while TTS\n(Text-to-Speech) returns audio responses. The centralOrchestratorroutes in-\nput to the appropriate workflow and coordinates response generation. In the\nSPARQLqueryworkflow,theQuery Generator,Autocorrector,andAnswer Gen-\neratorwork together to translate intent into executable queries. AWord Dictio-\nnarysupports personalized, consistent vocabulary usage. The SBT workflow al-\nlows users to define agent behaviors via natural language, using theBTF Builder\nto construct a high-level tree, theSBT Generatorto populate it, and theSBT\nNode Factoryto instantiate elements. The semantic search workflow includes an\nEmbedding Generator,Vector Store, andAnswer Generatorfor retrieving and\nsynthesizing content from AJAN documentation.\nFig.7: Overview of the LLM-based AJAN modeling interface\n10AJAN-Editor NLP-Extension:https://anonymous.4open.science/r/HYENA-69FF\n11https://www.elastic.co\nLLM-Assisted Agent Modeling with AJAN 7\n3.2 Chat Interface and Orchestration\nUserinteractionisstructuredalongtwodimensions:interaction modalities,which\ndefine how users communicate with the system (text or voice), andinteraction\nmodes, which describe the operational context (offline or online). The Natural\nLanguage Processing (NLP) interface for AJAN supports both text and voice\ninput. In text mode, users enter instructions directly into the interface, which are\npassed to the Orchestrator for semantic processing and workflow selection. For\nvoice"}
{"file": "2510.06911v1.pdf", "section": "3.2 Chat Interface and Orchestration", "chunk": "Language Processing (NLP) interface for AJAN supports both text and voice\ninput. In text mode, users enter instructions directly into the interface, which are\npassed to the Orchestrator for semantic processing and workflow selection. For\nvoice input, the NLP interface employs an Automatic Speech Recognition (ASR)\npipeline: user speech is recorded via microphone and digitized using the open-\nsourcePyAudiolibrary12. The audio is then transcribed by OpenAI\u2019sWhisper\nmodel13, which offers high accuracy across diverse acoustic conditions. The re-\nsulting text is forwarded to the NLP engine, enabling seamless multimodal in-\nteraction. The NLP extension operates in two modes:offlineandonline.\nFig.8: Interface to generate SPARQL-BTs using a LLM-driven chat interface\n\u2013 Offline Mode:In offline mode, users interact within the AJAN-Editor (cf.\nFig.8,therespectifegeneratedSBTcanbeseeninFig.17)viaachatwindow\nin theBehaviorstab. Messages are transmitted to the NLPinterface through\na WebSocket connection. The system responds with a generated SPARQL\nBehavior Tree, which is visualized in the editor and can be modified, saved,\nor executed. This mode is suited for agent development and debugging.\n\u2013 Online Mode:Online mode enables direct interaction with agents outside\nthe AJAN-Editor. Users communicate through a console interface, receiving\n12https://people.csail.mit.edu/hubert/pyaudio\n13https://github.com/openai/whisper\n8 H. Hechehouche et al.\nresponses in text or synthesized speech viaPyTTS14. Commands result in\nSBTs sent to and executed by instantiated agents via REST, while queries\ntrigger SPARQL generation and execution against the agent\u2019s RDF knowl-\nedge base. This mode emphasizes conversational simplicity while abstracting\ninternal execution details.\nOrchestration LogicTheOrchestratorserves as the central controller, inter-\npreting the user input, selecting the appropriate processing workflow, and man-\naging system responses. Upon receiving user input\u2014either transcribed speech\nor typed text\u2014it performs workflow classification to determine the suitable\npipeline. Workflow classification is delegated to the GPT-3.5-turbo-16k-0613\nmodel15. A prompt defining the context and three possible workflows\u2014SBT\ngeneration, SP"}
{"file": "2510.06911v1.pdf", "section": "3.2 Chat Interface and Orchestration", "chunk": "workflow classification to determine the suitable\npipeline. Workflow classification is delegated to the GPT-3.5-turbo-16k-0613\nmodel15. A prompt defining the context and three possible workflows\u2014SBT\ngeneration, SPARQL querying, and semantic search\u2014is combined with the user\ninput. The model returns a structured JSON object indicating the most likely\nworkflow (cf. Fig. 9a). Based on this output, the Orchestrator triggers one of the\nfollowing:\n\u2013SPARQL Query Workflowfor simulation-specific queries (see Sect. 3.3)\n\u2013SBT Generation Workflowfor behavior-related commands (see Sect. 3.4)\n\u2013Semantic Search Workflowfor documentation lookup (see Sect. 3.7)\nIt then awaits the workflow result and returns a formatted natural language re-\nsponseaccordingtothecurrentinteractionmode.Ambiguousinputsareresolved\nvia theDisambiguatormodule. For instance, if a verb like \u201clift\u201d is unrecognized\nin the agent\u2019s ontology, the system prompts for clarification (cf. Fig. 9b). In case\nof errors\u2014such as network issues or failed SBT generation\u2014the Orchestrator\nprovides user-friendly feedback. A structured conversation history is maintained\nto ensure context-aware interaction.\n(a) Derived workflow by the Orchestrator\n(b) Disambiguation process\nFig.9: Example data of the Orchestrator and Disambiguator\nSystem ResponsesSystem responses vary by input type. Commands return\nfeedback on SBT generation and execution, while questions are answered using\nthe agent\u2019s knowledge base or documentation. Ambiguous inputs prompt clar-\nifications, and errors result in diagnostic messages. Output is provided as text\n14https://github.com/Kyubyong/pytts\n15https://platform.openai.com/docs/models/gpt-3-5\nLLM-Assisted Agent Modeling with AJAN 9\nand optionally via synthesized voice. ThePyTTSlibrary supports multiple voice\noptions, enabling adaptation to different user preferences and accessibility needs.\n3.3 LLM-based SPARQL Query Generation\nAnswering user questions about the agent or its environment involves transform-\ning natural language input into executable SPARQL queries evaluated against\ntheagent\u2019sRDF-basedknowledgebase.Thisprocessisbothlinguisticallyandse-\nmantically complex, requiring several dedicated components and sub-processes.\nUnderstanding User QueriesAccurate"}
{"file": "2510.06911v1.pdf", "section": "3.3 LLM-based SPARQL Query Generation", "chunk": "-\ning natural language input into executable SPARQL queries evaluated against\ntheagent\u2019sRDF-basedknowledgebase.Thisprocessisbothlinguisticallyandse-\nmantically complex, requiring several dedicated components and sub-processes.\nUnderstanding User QueriesAccurate query generation begins with inter-\npreting user intent, achieved through three main steps: syntactic parsing, se-\nmanticlinking,anddisambiguation.WhileParsing,thesystemfirsttokenizesthe\nuserinputandconstructsadependencyparsetreeusingtheSpaCylibrary16.This\nidentifies parts of speech and syntactic roles (e.g., subjects, modifiers), providing\na structural representation of the question. The parsed structure helps identify\nrelevant entities and predicates. DuringLinking, identified entities and relations\nare mapped to RDF resources by comparing them to the ontology used by the\nagent (contains terminological knowledge and statements about the agent\u2019s envi-\nronment), using specificallyrdfs:labelvalues given in the RDF-data. A fuzzy\nmatching algorithm (Levenshtein distance) ranks similarity scores, while Elas-\ntic Search provides efficient indexed search. The linking module, adapted from\ntheEARLframework [5], returns candidate URIs prioritized by confidence. Fig-\nure 10 illustrates the output for an example input. If multiple candidates match\na user term or confidence is low (threshold set to 0.5), the Disambiguator mod-\nule prompts the user to select the correct URI. These choices are stored in a\nsynonym dictionary for future reuse, improving personalization and recall.\nFig.10: Example output of the linking process\nTranslating Questions into SPARQL QueriesAfter resolving the enti-\nties and relations, the system constructs a SPARQL query that corresponds to\nthe user\u2019s question. To minimize errors, Bloom filters are applied to restrict\npredicate-object combinations to valid patterns observed in the RDF data. This\nfilteringstepreducesthelikelihoodofgeneratingsyntacticallycorrectbutseman-\ntically invalid queries. The filtered ontology, along with a task description and\nuser input, is passed to the GPT-3.5-turbo-16k-0613 model17using the prompt\n16https://www.spacy.io\n17https://platform.openai.com/docs/models\n10 H. Hechehouche et al.\ndepictedinFigure11.Themodelthenproducesata"}
{"file": "2510.06911v1.pdf", "section": "3.3 LLM-based SPARQL Query Generation", "chunk": "16k-0613 model17using the prompt\n16https://www.spacy.io\n17https://platform.openai.com/docs/models\n10 H. Hechehouche et al.\ndepictedinFigure11.ThemodelthenproducesatailoredSPARQLquery,either\nasaSELECTorASKquery,dependingonthedetectedintent.Figure16billustrates\nin the AJAN-editor a generated query corresponding to the user question\u201cIs the\npurple block clear?\u201d. This generative approach proves effective for well-formed\nqueries but may struggle with lengthy or ambiguous input.\nFig.11: Prompt to generate SPARQL ASK queries using filtered RDF data\nSPARQL Execution and Error HandlingThe generated query is executed\nagainst the agent\u2019s KB via the RDF4J REST API18. The results are then parsed\nand forwarded to theAnswer Generator. If execution fails or returns no data,\ntheQuery Autocorrectoris activated, as shown in Figure 12. This module\nsends the faulty query, its error message, and surrounding context to the GPT-\n4-32k-0314 model19. GPT-4 produces a corrected version of the query, which\nis typically successful in the first iteration. The corrected result is returned to\ntheAnswer Generatorfor further processing. Finally, theAnswer Generator\nconverts the query output into a coherent, natural language response, delivered\nto the user through the chat interface. If the query fails entirely or yields no\nresult, the user gets informed.\n18RDF4J API:https://rdf4j.org/documentation/reference/rest-api/\n19https://platform.openai.com/docs/models\nLLM-Assisted Agent Modeling with AJAN 11\nFig.12: SPARQL execution and error handling\n3.4 SPARQL Behavior Tree Generation Workflow\nThe generation of SPARQL Behavior Trees in AJAN hinges on reusable building\nblocks\u2014tree nodes\u2014provided by theSBT Node Factory. This module maintains\nblueprints for five primary node types:GoalProducer,Condition,Sequence,\nPriority, andRoot. These nodes were specifically selected because they allow\nhigher-level conditional behaviors to be created, which orchestrate more com-\nplexdomain-integratedparameterizedbehaviorsviatheintegrationofadditional\npredefined sub-behaviors viaGoalProducers. Accordingly, a level of complexity\nis introduced that remains manageable even for non-experts. Each is defined\nvia"}
{"file": "2510.06911v1.pdf", "section": "3.4 SPARQL Behavior Tree Generation Workflow", "chunk": "com-\nplexdomain-integratedparameterizedbehaviorsviatheintegrationofadditional\npredefined sub-behaviors viaGoalProducers. Accordingly, a level of complexity\nis introduced that remains manageable even for non-experts. Each is defined\nvia dedicated Python classes which generate the respective RDF representation\nneeded for SBTs and designed for composability, allowing for flexible, extensi-\nble behavior models. These modular nodes enable the synthesis of executable\nagent logic directly from high-level user instructions. Figure 13 illustrates them\nas visualized within the AJAN-Editor.\nFig.13: SPARQL-BT nodes that can be generated by the NLP interface\nUnderstanding User InstructionsSBT creation begins with parsing user\ninput into a structured representation. This starts with generating a Behavior\nTree Frame (BTF)\u2014a scaffold devoid of attributes\u2014which guides the SBT\u2019s\ncomposition. Using few-shot prompting, thegpt-3.5-turbo-16k-0613model\nreceives the user instruction along with 20 instruction\u2013BTF examples and out-\nputs a JSON-encoded frame. Figure 14 shows a generated BTF from the Blocks\nWorld domain, with which the blue block is placed on the purple block, other-\nwise on the orange one, if the purple block is not clear.\nSyntactic and Semantic AnalysisTo enrich the BTF with logic, the sys-\ntem parses the instruction usingSpaCyto extract part-of-speech tags and de-\npendencies. Verbs, objects, and modifiers are identified for transformation into\nactionable semantics. Next,gpt-3.5-turbo-16kextracts goals and target enti-\ntiesfromthetext,formingthebasisforSPARQLqueriesandnodeconfiguration.\n12 H. Hechehouche et al.\nFig.14: Generated BTF from the user\u2019s instruction\n3.5 Action and Entity Linking\nAfter identifying actions from the user\u00b4s input, they are linked to RDF-based\nAJAN goals via a three-tiered mapping strategy:\nAction LinkingFirst, actions are matched directly against goal labels in the\nRDF model. Figure 15a shows an example set of goals from the Blocks World\nagentasvisualizedintheAJAN-Editor.Ifnodirectmatchexists,thesystemcon-\nsults a user-specific synonym dictionary updated through previous disambigua-\ntions. Figure 15b shows"}
{"file": "2510.06911v1.pdf", "section": "3.5 Action and Entity Linking", "chunk": "goals from the Blocks World\nagentasvisualizedintheAJAN-Editor.Ifnodirectmatchexists,thesystemcon-\nsults a user-specific synonym dictionary updated through previous disambigua-\ntions. Figure 15b shows a visual representation of the user dictionary module.\nIf still unresolved, the system computes semantic similarity using theT5trans-\nformer model20. Vectors are generated for the unknown and known actions, and\ncosine similarity is applied. If the similarity exceeds 0.5, the action is linked;\notherwise, user disambiguation is triggered.\n(a) Blocks World AJAN goals\n(b) User dictionary module\nFig.15: AJAN goals and the respective dictionary\nEntity LinkingEntities referenced in instructions (e.g., \u201cblue block\u201d) must\nalso be uniquely mapped to RDF resources. Like action linking, this process\nuses theDisambiguatorto resolve ambiguities. While this remains a challeng-\ning task\u2014particularly in cases of synonymy or context-dependence\u2014the layered\nstrategy of direct mapping, synonym memory, and semantic similarity provides\na practical solution within the AJAN ecosystem.\n3.6 Translation of User Instructions into Behavior Trees\nAfter constructing the BTF for a given user instruction, the system translates it\ninto a complete SBT by traversing the BTF, instantiating nodes from theSBT\nNode Factory, and assigning relevant properties and SPARQL queries to each\n20T5:https://huggingface.co/docs/transformers/model_doc/t5\nLLM-Assisted Agent Modeling with AJAN 13\nnode. For inner nodes that describe the internal execution logic, such asROOT,\nSEQUENCE,andPRIORITYnodes,thefactoryprovidespredefinedRDFrepresenta-\ntions that only need to be linked to child nodes in the form of an RDF list during\ntranslation. The nodes implemented that require additional parameters such as\neventsorquerydefinitions,aretheGoalProducernodeandtheConditionnode:\n(a) Construct template for the generation of GoalPro-\nducer nodes\n(b) Condition property with generated\nASK query\nFig.16: Template for GoalProducers and generated Condition properties\nEachGoalProducernode corresponds to an atomic agent action or a sub-\nbehavior tree. The target goal URI is inserted into the node based on its RDF\nlabel (see Fig. 15a). A SPARQLCONSTRUCTquery template"}
{"file": "2510.06911v1.pdf", "section": "3.6 Translation of User Instructions into Behavior Trees", "chunk": "EachGoalProducernode corresponds to an atomic agent action or a sub-\nbehavior tree. The target goal URI is inserted into the node based on its RDF\nlabel (see Fig. 15a). A SPARQLCONSTRUCTquery template\u2014linked to the ac-\ntion\u2014is populated with previously identified entities, serving as dynamic ar-\nguments. This query encodes the precondition context required for successful\nexecution, derived from the associatedASKquery. Figure 16a shows an example\ntemplate for the actionstack, with placeholders like<BLOCK_X>and<BLOCK_Y>.\nConditionnodes are generated by directly translating conditional phrases into\nSPARQLASKqueries without using templates. Each query reflects the condition\nextracted during semantic parsing. Figure 16b shows a condition node as ren-\ndered in the AJAN-Editor. Details can be found in the AJAN Wiki21.\nOnce assembled, the SBT is either saved to the RDF repository for offline in-\nspection in the AJAN Editor or pushed directly to the triplestore for execution\nin online mode. This supports both iterative development and live control. Fig-\nure 17 displays the complete SBT for the Blocks World example instruction.\n3.7 Semantic Search over AJAN Documentation\nTo respond to documentation-related queries, the system uses a semantic search\nworkflow that encodes textual resources (e.g., the AJAN Wiki) into dense vector\n21https://github.com/aantakli/AJAN-service/wiki/SPARQL-BT-Leaf-Nodes\n14 H. Hechehouche et al.\nFig.17: Generated SPARQL-BT for the Blocks World demo\nrepresentations.Thesevectorsareindexedforsimilarity-basedretrieval,enabling\naccurate extraction of relevant content during user interaction. Text is converted\nintofixed-sizesemanticvectorsusingOpenAI\u2019stext-embedding-ada-002model22.\nEach input string is encoded into a high-dimensional vector reflecting its se-\nmantic meaning, allowing similar texts to be mapped to nearby points in vec-\ntor space. AJAN documentation hosted on GitHub is automatically fetched\nand parsed usingBeautifulSoup23. It is segmented into paragraph- or section-\nlevel chunks, which are embedded and stored inPinecone24, a managed vector\ndatabase. Each vector is stored with metadata\u2014document URI, section title,\nand offset\u2014to enable provenance-aware retrieval. User queries are"}
{"file": "2510.06911v1.pdf", "section": "3.7 Semantic Search over AJAN Documentation", "chunk": "paragraph- or section-\nlevel chunks, which are embedded and stored inPinecone24, a managed vector\ndatabase. Each vector is stored with metadata\u2014document URI, section title,\nand offset\u2014to enable provenance-aware retrieval. User queries are embedded us-\ning the same model to ensure compatibility. Cosine similarity is then applied\nto identify the top-Kmost relevant document vectors (typicallyK= 5). The\nretrieved segments, along with the query, are compiled into a prompt for the\ngpt-3.5-turbo-0613model25, which generates an answer grounded in the re-\ntrieved context. This response is passed to theOrchestratorfor delivery.\n4 Application Example\n(a) Shelf assembly steps\n(b) 3D MOSIM environment\nFig.18: Manual assembly application example in the Industry 4.0 context\nAs shown in [1], the AJAN framework has been already applied ain various do-\nmains. The goal of the AJAN-Editor is to supports users within these domains\nin implementing agent-based applications, particularly targeting developers and\nMAS experts in the Semantic Web. In the past, AJAN was very often used for\n22https://platform.openai.com/docs/guides/embeddings\n23https://pypi.org/project/beautifulsoup4/\n24https://www.pinecone.io\n25https://platform.openai.com/docs/models\nLLM-Assisted Agent Modeling with AJAN 15\nIndustry 4.0 scenarios to simulate human-in-the-loop processes in 3D environ-\nments, supporting ergonomic evaluations and feasibility studies. The following\napplication example continues this line of use cases, focusing on human process\nplanners who design and validate manual assembly steps.\nIn the application example described below, the AJAN-Editor, primarily with\nits LLM-based user interface, is used to model manual work steps performed by\na worker and simulate them directly in 3D using the MOSIM framework [13].\nThe example used is the assembly of a shelf26. In general, the assembly process\nis as follows: (i) Position the leg; (ii) position the shelf; (iii) insert the screw, (iv)\nand tighten the screw. Since the shelf has four legs and four shelves, these four\nsteps are repeated several times. For the MOSIM scene, the AJAN agent has\naccess to the general ontology or the"}
{"file": "2510.06911v1.pdf", "section": "4 Application Example", "chunk": "insert the screw, (iv)\nand tighten the screw. Since the shelf has four legs and four shelves, these four\nsteps are repeated several times. For the MOSIM scene, the AJAN agent has\naccess to the general ontology or the T-Box and A-Box information in RDF and\ncan therefore perform the assembly in the MOSIM environment using predefined\nSPARQL-BTs executing MOSIM animation calls. InVideo I27, this process can\nbe seen accompanied by the AJAN-Editor Live view.Video II28also shows\nthe use of the AJAN-NLP interface. The various modes of the interface are\ndemonstrated here. Inoffline mode, a SPARQL-BT for the Blocks World demo\nis generated textually, stored and later on executed in a MOSIM simulation. In\noffline mode, on the other hand, the user is speaking directly to an instantiated\nagent to obtain information about the scene or execute commands immediately.\nThe last part of the video demonstrates the AJAN ChatBot for the AJAN Wiki.\n5 Related Work\nThe integration of natural language processing (NLP) for Behavior Tree (BT)\ngeneration as in AJAN may enhance the accessibility and scalability of agent\nmodeling. There are few relevant approaches in this direction. For example,\nFriedrich et al. [6] used templates to extract behavior components but lacked se-\nmantic and domain flexibility. Shu et al. [11] achieved domain-specific precision\nvia rule-based methods in the medical field, without dynamic generation. Sud-\ndrey et al. [14] introduced a modular, grammar-based generator with dialogue-\ndriven disambiguation and a Learning Mode, yet suffered from handcrafted rule\nlimitations and absent error handling. Jin and Zhou [7] parsed input into action\nsequences but did not construct full BTs. Cao and Lee [3] employed LLMs to\npopulate a fixed BT structure, showing resilience to linguistic variation but lack-\ning editing, modularity, and disambiguation. Unlike their fine-tuning approach,\nwe found few-shot prompting more effective for preserving reasoning abilities.\nOn the other hand, relevant work on natural language to SPARQL translation\ngained some attraction as well. Liang et al. [9] proposed a modular, DBpedia-\n26Shelf construction manual:https://manuals.plus/ikea/\nhyllis-shelving-unit-in"}
{"file": "2510.06911v1.pdf", "section": "5 Related Work", "chunk": "SPARQL translation\ngained some attraction as well. Liang et al. [9] proposed a modular, DBpedia-\n26Shelf construction manual:https://manuals.plus/ikea/\nhyllis-shelving-unit-in-outdoor-manual\n27AJAN-Editor NLP:https://anonymous.4open.science/r/AJAN-material-1E45/\nAJAN-MOSIM.mp4\n28AJAN-Editor NLP:https://anonymous.4open.science/r/AJAN-material-1E45/\nAJAN-NLP.mov\n16 H. Hechehouche et al.\nspecific pipeline using Tree-LSTMs for query ranking. Sima et al. [12] built\nschema-driven queries from lookup tables, limited to simple questions and sci-\nentific knowledge graphs (KG). Borroto et al. [2] applied dual-LSTM models\nfor entity extraction and template generation, constrained by outdated embed-\ndings. Chen et al. [4] developed BAMnet, a bi-attentive system with strong\ninterpretability but high resource needs. Rony et al. [10] introduced SGPT, in-\ntegrating KG knowledge into GPT-2, and Kovriguina et al. [8] proposed SPAR-\nQLGEN as a further development of SGPT, both improving semantic alignment\nbutreducingexplainabilityandcross-KGadaptability.Ourapproachemphasizes\nmodularity, KG independence, and transparent, context-aware query generation\nwithout relying on rigid or monolithic architectures.\nIn general, our method overcomes prior shortcomings through modular, editable\nBT synthesis with integrated SPARQL generation and live agent deployment.\n6 Conclusion\nThis paper introduced the AJAN-Editor with its integrated LLM-based user\ninterface for modeling and executing Semantic Web-enabled agents, and demon-\nstrated its use through an example 3D simulation scenario. The AJAN-Editor\nprovides a comprehensive environment for managing agent templates, modeling,\nvisualizing and monitoring SPARQL Behavior Trees and querying agent knowl-\nedge. The LLM interface enables users to generate executable agent behaviors\nand SPARQL queries from natural language, supporting both textual and vocal\nmodalitiesinonlineandofflinemodes.Unlikeearlierrule-based,template-driven,\norlimitedLLM-basedapproaches,oursystemenablesmoremodularandeditable\nBehavior"}
{"file": "2510.06911v1.pdf", "section": "6 Conclusion", "chunk": "queries from natural language, supporting both textual and vocal\nmodalitiesinonlineandofflinemodes.Unlikeearlierrule-based,template-driven,\norlimitedLLM-basedapproaches,oursystemenablesmoremodularandeditable\nBehavior Tree construction. It also improves SPARQL generation by overcom-\ning key limitations of previous LLM based approaches, especially by enabling\nknowledge graph independence. Furthermore, this is the first work to use LLMs\nto generate SPARQL-extendet Behavior Trees described in RDF, which can also\nbe processed directly by an agent in its dynamic environment while lowering the\nbarrier for developing semantic web agents, even for non-experts.\nAcknowledgments.ThisworkhasbeenfundedbytheGermanMinistryforResearch,\nTechnology and Space (BMFTR) in the project MOMENTUM (01IW22001), and the\nEuropean Commision in the project InnovAIte (09I02-03-V01-00029).\nReferences\n1. Antakli, A., Kazimov, A., Spieldenner, D., Rojas, G.E.J., Zinnikus, I., Klusch, M.:\nAjan: An engineering framework for semantic web-enabled agents and multi- agent\nsystems.In:InternationalConferenceonPracticalApplicationsofAgentsandMulti-\nAgent Systems. pp. 15\u201327. Springer (2023)\n2. Borroto, M.A., Ricca, F., Cuteri, B.: A system for translating natural language\nquestions into sparql queries with neural networks: Preliminary results. In: Pro-\nceedings of the 29th Italian Symposium on Advanced Database Systems, SEBD.\nvol. 2994, pp. 226\u2013234 (2021)\nLLM-Assisted Agent Modeling with AJAN 17\n3. Cao, Y., Lee, C.: Robot behavior-tree-based task generation with large language\nmodels. arXiv preprint arXiv:2302.12927 (2023)\n4. Chen, Y., Wu, L., Zaki, M.J.: Bidirectional attentive memory networks for question\nanswering over knowledge bases. arXiv preprint arXiv:1903.02188 (2019)\n5. Dubey, M., Banerjee, D., Chaudhuri, D., Lehmann, J.: Earl: joint entity and re-"}
{"file": "2510.06911v1.pdf", "section": "6 Conclusion", "chunk": "arXiv preprint arXiv:1903.02188 (2019)\n5. Dubey, M., Banerjee, D., Chaudhuri, D., Lehmann, J.: Earl: joint entity and re-\nlation linking for question answering over knowledge graphs. In: International Se-\nmantic Web Conference. pp. 108\u2013126. Springer (2018)\n6. riedrich, F., Mendling, J., Puhlmann, F.: Process model generation from natu- ral\nlanguage text. In: International conference on advanced information systems engi-\nneering. pp. 482\u2013496. Springer (2011)\n7. Jin, K., Zhuo, H.H.: Integrating ai planning with natural language processing: A\ncombination of explicit and tacit knowledge. ACM Transactions on Intelligent Sys-\ntems and Technology (2022)\n8. Kovriguina, L., Teucher, R., Radyush, D., Mouromtsev, D., Keshan, N., Neumaier,\nS., Gentile, A., Vahdati, S.: Sparqlgen: One-shot prompt-based approach for sparql\nquery generation. In: SEMANTiCS (Posters and Demos) (2023)\n9. Liang, S., Stockinger, K., De Farias, T.M., Anisimova, M., Gil, M.: Querying knowl-\nedge graphs in natural language. Journal of big data 8(1), 3 (2021)\n10. Rony, M.R.A.H., Kumar, U., Teucher, R., Kovriguina, L., Lehmann, J.: Sgpt: A\ngenerative approach for sparql query generation from natural language questions.\nIEEE access 10, 70712\u201370723 (2022)\n11. Shu, S., Preum, S., Pitchford, H.M., Williams, R.D., Stankovic, J., Alemzadeh, H.:\nA behavior tree cognitive assistant system for emergency medical services. In: 2019\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). pp.\n6188\u20136195. IEEE (2019)\n12. Sima,A.C.,MendesdeFarias,T.,Anisimova,M.,Dessimoz,C.,Robinson"}
{"file": "2510.06911v1.pdf", "section": "6 Conclusion", "chunk": "on Intelligent Robots and Systems (IROS). pp.\n6188\u20136195. IEEE (2019)\n12. Sima,A.C.,MendesdeFarias,T.,Anisimova,M.,Dessimoz,C.,Robinson-Rechavi,\nM., Zbinden, E., Stockinger, K.: Bio-soda: enabling natural language question an-\nswering over knowledge graphs without training data. In: Proceedings of the 33rd\nInternational Conference on Scientific and Statistical Database Management. pp.\n61\u201372 (2021)\n13. Sprenger, J., Antakli, A., Fischer, K. (2025, May). The MOSIM Framework: Sim-\nulating Smart Workers in the Industrial Metaverse. In 2025 IEEE 8th International\nConference on Industrial Cyber-Physical Systems (ICPS) (pp. 1-8). IEEE.\n14. Suddrey, G., Talbot, B., Maire, F.: Learning and executing re-usable behaviour\ntrees from natural language instruction. IEEE Robotics and Automation Letters\n7(4), 10643\u201310650 (2022)"}
{"file": "2510.07043v1.pdf", "section": "Introduction", "chunk": "Preprint\nCOMPASS: A MULTI-TURNBENCHMARK FORTOOL-\nMEDIATEDPLANNING& PREFERENCEOPTIMIZATION\nTian Qin\u2217\nHarvard UniversityFelix Bai\nAppleTing-Yao Hu\nAppleRaviteja Vemulapalli\nApple\nHema Swetha Koppula\nAppleZhiyang Xu\nVirginia Tech, AppleBowen Jin\nUIUC, AppleMert Cemri\nUC Berkeley, Apple\nJiarui Lu\nAppleZirui Wang\nAppleMeng Cao\nApple\nABSTRACT\nReal-world LLM agents must master strategic tool orchestration and user preference\noptimization through multi-turn interactions to assist users with complex planning\ntasks. We introduceCOMPASS(ConstrainedOptimization throughMulti-turn\nPlanningandStrategicSolutions), a benchmark that evaluates agents on realistic\ntravel planning scenarios. We cast travel planning as aconstrained preference opti-\nmizationproblem, where agents must satisfy hard constraints while simultaneously\noptimizing soft user preferences. To support this, we build a realistic travel database\ncovering transportation, accommodation and ticketing for 20 U.S. National Parks\nand a comprehensive tool ecosystem that mirrors commercial booking platforms.\nEvaluating state-of-the-art models, we uncover two critical gaps: (i) anaccept-\nable\u2013optimal gap, where agents reliably meet constraints but fail to optimize\npreferences, and(ii)aplan-coordination gap, where performance collapses on\nmulti-services (flight, hotel) coordination tasks, especially for open-source models.\nBy grounding reasoning and planning in a practical, user-facing domain,COM-\nPASSoffers a benchmark that directly measures an agent\u2019s ability to optimize user\npreferences in realistic tasks, bridging theoretical advances with real-world impact.\n1 INTRODUCTION\nAs Large Language Models (LLMs) move toward real-world deployment, their role is increasingly\nto assist users with everyday tasks that blend strict requirements with soft preferences. A natural\nway to formalize such interactions is through the lens ofconstrained preference optimization: agents\nmust satisfy hard constraints (e.g., travel dates, budget limits, accessibility requirements) while\nsimultaneously optimizing soft user preferences (e.g., cheapest options or wish-lists). Within this\nframework, tool-mediated problem solving is the mechanism that enables agents to reason strategically\nand invoke external resources. By iteratively refining their plans, agents can build coherent solutions\nthat"}
{"file": "2510.07043v1.pdf", "section": "1 INTRODUCTION", "chunk": "preferences (e.g., cheapest options or wish-lists). Within this\nframework, tool-mediated problem solving is the mechanism that enables agents to reason strategically\nand invoke external resources. By iteratively refining their plans, agents can build coherent solutions\nthat align with user preferences. This dual capacity for constraint satisfaction and preference\noptimization is therefore a core requirement for LLM agents to be genuinely useful in practice.1\nCurrent benchmarks, however, fall short of evaluating these core capabilities. Most planning bench-\nmarks reduce tasks to single-turn constraint satisfaction (Xie et al., 2024; Kohli et al., 2024), where\nfinding any feasible solution is sufficient. Yet, preference optimization requiresqualitativelydifferent\nskills: rather than stopping at the first valid solution, agents must compare alternatives and select the\none that best aligns with user preferences. Likewise, existing tool-use benchmarks focus narrowly\non whether agents call the correct tool with valid parameters (Zhong et al., 2025; Yao et al., 2024),\nwithout testing how agents adapt to multi-turn interactions, where users naturally add new require-\nments and provide feedbacks (Wang et al., 2023; Laban et al., 2025). Finally, most benchmarks rely\n\u2217Work done during internship at Apple. Correspondence totqin@g.harvard.edu\n1Data and code is available at:https://github.com/sunnytqin/compass.git\n1arXiv:2510.07043v1  [cs.LG]  8 Oct 2025\nPreprint\nSearch_hotel\n(start_date,\nend_date,\nnum_guest,...)\nGet_hotel_info\n(\nhotel_name\n)\nCalculator\n(math_expression)\nCalendar\n(MM-DD-YYYY)\nSearch_flights\n(start_date,\nend_date,\n...)\nGoogle_maps\n(orig,dest,...)\nOptimization\nObjective\nMinimize\ncost\n(A)\nI\nwant\nto\ngo\nto\nYellowstone\nwith\nmy\npartner\nafter\nAug\n15\nfor\n5\ndays\n.\nWe\ndon't\nwant\nto\nspend\nmore\nthan\n$1,400\n....\nOk,\nafter\ndoing\nresearch,\nhere\nis\nthe\ncheapest\nitinerary\nwith\ndirect\nflight...\nThank\nyou!\nBut\nI\nforgot\nto\nmention\nwe\nwant\ndirect\nflights.\nFind"}
{"file": "2510.07043v1.pdf", "section": "1 INTRODUCTION", "chunk": "Ok,\nafter\ndoing\nresearch,\nhere\nis\nthe\ncheapest\nitinerary\nwith\ndirect\nflight...\nThank\nyou!\nBut\nI\nforgot\nto\nmention\nwe\nwant\ndirect\nflights.\nFind\nus\ncheapest\noption.\nAfter\ndoing\nextensive\nresearch,\nI\nrecommend\nthis\nitinerary...\nA\ncouple\nwants\nto\ngo\nto\nYellowstone\nfor\n5\ndays\nat\nthe\nsecond\nhalf\nof\nAug.\nThey\ndon't\nwant\nto\nspend\nmore\nthan\n$1400,\nand\nthey\nwant\nto\ngo\non\nNorris\nGeyser\nBasin\nTour,\nwhich\nrequires\nobtaining\na\npermit.\nThey\nalso\nrequire\ndirect\nflight\nand\na\n4\nstar\nhotel...\nTravel\nScenario\nUser\nPersona\nHard\nConstraints\n(B)\nSecond\nhalf\nof\nAug\nDates\nLocation\nHotel\nBudget\nFlight\n5\ndays\nYellowstone\nNational\nPark\ntotal\n<\n$1,400\nDirect\nflight\n2\npeople\nroom\n4\nstar\nabove\n2\nadults\nLow\nUpfrontness\n(reveal\ntask\ninformation)\nHigh\nAttention-to-detail\n(catch\nagent's\nmistakes)\nMedium\nTrusting\n(ask\nto\nshow\nreasoning)\n(C)\n(D)\n(\nE\n)\nPermit\nNorris\nGeyser\nBasin\nTour\nDate\nFlight\nPrice\nDate\nHotel\nPrice\nPermit\nAvailability\nFigure 1:COMPASSbenchmark framework.The environment integrates three key components\nfor realistic evaluation of agentic capabilities in travel planning.(A)A modular LLM-based user\nsimulator enables controllable multi-turn interactions, progressive constraint revelation, and diverse\nuser personas.(B)We formalize travel planning asconstrained preference optimization, where\nagents must satisfy hard constraints (feasible solutions) while optimizing soft user objectives (e.g.,\nminimizing cost, maximizing amenities).(C)Agents interact with realistic travel databases(E)using\na comprehensive tool ecosystem(D), requiring iterative planning and refinement across conversation\nturns to construct optimal itineraries.\non simplified mock databases. However, database realism directly shapes task difficulty\u2014authentic\nevaluation requires rich, complex environments to meaningfully assess"}
{"file": "2510.07043v1.pdf", "section": "1 INTRODUCTION", "chunk": "inerary) consistency. While closed-source models show gradual decline,\nopen-source models struggle dramatically, highlighting the difficulty of coordination in realistic travel\nscenarios. Together,COMPASSserves as a rigorous benchmark for diagnosing the core challenges\nof constrained preference optimization and guiding the development of user-aligned AI agents.\n2\nPreprint\nClosed-source reasonClosed-sourceOpen-source\nAcceptable Rate vs. Optimal Rate Gap\nFigure 2:COMPASSbenchmark main results. Acceptable ratemeasures feasibility (satisfying\nall hard constraints).Optimal ratemeasures preference optimization (achieving utility within the\ntop 10% of feasible solutions). All models show a \u223c20% gap between high acceptable rates and low\noptimal rates, revealing that agents settle for feasible solutions rather than optimizing preferences.\nEncouragingly, open-source models like Qwen3-32B achieve non-trivial performance, demonstrating\nemerging agentic capabilities.\n2 RELATEDWORK\nTool-Use and Planning BenchmarksRecent benchmarks have evaluated language model agents\nacross tool-use and planning capabilities, though typically in isolation. Tool-use benchmarks like\nTau-Bench (Yao et al., 2024; Barres et al., 2025) provide comprehensive API libraries but focus\nprimarily on instruction-following in a online customer service setting. Patil et al. (2025) includes\na wider tool library but user multi-turn is based on predetermined conversation trajectories, while\nothers (Wang et al., 2023) evaluate coding scenarios with oracle-like users. Planning benchmarks\nhave emerged to assess constraint satisfaction and optimization: Zheng et al. (2024) evaluates natural\nlanguage planning tasks while eliminating tool-use complexity, and Kohli et al. (2024) presents flight\nbooking scenarios in multiple-choice format without interactive tool integration. More comprehensive\nplanning evaluation includes multi-day travel planning with hard constraints (Xie et al., 2024), though\nwithout the multi-turn preference revelation that characterizes real user interactions. Our work is\nthe first to combine tool-calling, planning, and preference optimization in a multi-turn setting\u2014a\ncombination essential for agents deployed in real-world scenarios where users progressively reveal\nconstraints while agents must dynamically adapt their search and optimization strategies. Concurrent\nwork Qian et al. (2025) also explores user preference optimization in a planning setting but with a\nstronger focus on preference elicitation.\nMulti-Turn Inter"}
{"file": "2510.07043v1.pdf", "section": "2 RELATEDWORK", "chunk": "constraints while agents must dynamically adapt their search and optimization strategies. Concurrent\nwork Qian et al. (2025) also explores user preference optimization in a planning setting but with a\nstronger focus on preference elicitation.\nMulti-Turn Interactions and Task-Oriented DialogueAssessing agent capabilities in multi-turn\nsettings is crucial for real-world deployment, as users naturally reveal requirements incrementally\nrather than specifying complete constraints upfront (Laban et al., 2025). Traditional task-oriented\ndialogue systems (Budzianowski et al., 2018; Rastogi et al., 2019) provide foundational insights:\nmulti-turn interactions occur because users reveal new requirements, request information, and adapt\ngoals when constraints cannot be satisfied. However, these systems typically lack the tool integration\nand utility optimization challenges present in modern agent evaluation. Recent work on multi-turn\nagent evaluation includes simple tasks like city guessing (Abdulhai et al., 2023) and preference\nlearning through turn-based rewards (Wan et al., 2025), but lacks the complexity of realistic planning\nscenarios. Drawing inspiration from task-oriented dialogue research, our user simulator design\nachieves diverse interaction dynamics through progressive constraint revelation, mimicking how real\nusers gradually specify preferences while maintaining the authentic conversational patterns observed\nin human-to-human planning dialogues. We provide an extensive related work in App. B.\n3\nPreprint\nLLM\nUser\nDynamic\nPrompt\nTemplate\nYou\nare\na\nuser\nlooking\nfor\ntravel\nrecommendations...\nBasic\nTask\nscenario:\nA\ncouple\nplanning\na\nround-trip\nitinerary...\nUser\nPersona\n:\nYou\nalways\nask\nthe\nagent\nto\njustify\nits\nreasoning\nand\nprovide\nclear\nexplanations...\nInformation\nrevealed\nso\nfar:\n{dates,\norigin,\nbudget}\nInformation\nto\nreveal\nthis\nturn:\n{permit\nrequirement}\nProviding\nagent\nfeedback:\n{the\nagent?s\nlast\nrecommendation\nexceeds\nyour\nbudget}\nRepeat\nand\nquestion:\n{N/A}\nGenerate\na\nnatural,\nconversational\nmessage\nbased\non\nthe\ninstructions\nabove.\nMinimize\ncost\n(flight\n+\nhotel)\nHard\nConstraints\nSingle\nMetric\nOptimization\nSecond\nhalf\nof\nAug"}
{"file": "2510.07043v1.pdf", "section": "2 RELATEDWORK", "chunk": "a\nnatural,\nconversational\nmessage\nbased\non\nthe\ninstructions\nabove.\nMinimize\ncost\n(flight\n+\nhotel)\nHard\nConstraints\nSingle\nMetric\nOptimization\nSecond\nhalf\nof\nAug\nDates\nLocation\nHotel\nBudget\nFlight\n5\ndays\nYellowstone\nNational\nPark\nTotal\n<\n$1,400\nDepart\nfrom\nJFK\n2\npeople\nroom\n2\nadults\nPermit\nNorris\nGeyser\nBasin\nTour\nFeature\nCount\nMaximization\nHotel\n4\nstar+\nFly\nbusiness\n(A)\n(B)\nDirect\nflight\nFree\nbreakfast\nDepart\nlater\nthan\nnoon\n...\nType\nI\nType\nII\nFigure 3:Examples of task types and dynamic user simulator prompt. (A)Two task types\nare defined based on the soft preference-optimization objective. Each task type includes hard\nconstraints but differs in optimization objective (Sec. 3.2).(B)The dynamic LLM user simulator\nprompt (Sec. 3.4) controls multi-turn conversation dynamics. The system prompt consists of static\ninstructions (orange), fixed for each conversation, and dynamic fields (purple), which are updated at\nevery turn based on the conversation state.\n3COMPASSBENCHMARK\n3.1 TRAVELPLANNING ASCONSTRAINEDPREFERENCEOPTIMIZATION\nA central challenge for real-world agents is to not only satisfy user requirements but also optimize\nfor user satisfaction. To capture this duality, we formulate travel planning as a constrained prefer-\nence optimization problem, wherehard constraintsdefine the feasible set of solutions andsoft\npreferencesrank those solutions by user preference. Hard constraints represent non-negotiable\nrequirements\u2014budget limits (\u201ctotal cost under $3000\u201d), occupancy needs (\u201c6 people, max 2 per\nroom\u201d), or logistical requirements (\u201cpermit required for Extended Cave Tour\u201d). Within this feasible\nset, user preference objectives capture user priorities such as minimizing cost, maximizing amenities.\nThis separation between feasibility and desirability provides a principled way to evaluate whether\nagents can go beyond finding one valid solution to finding the best solution based on user requests.\n3.2 TASKDESIGN\nTask TypesReal users have different ways of defining the \u201cbest\u201d travel itinerary. To balance\nrealism with clarity of evaluation, we design two complementary soft preference types that capture"}
{"file": "2510.07043v1.pdf", "section": "3.1 TRAVELPLANNING ASCONSTRAINEDPREFERENCEOPTIMIZATION", "chunk": "best solution based on user requests.\n3.2 TASKDESIGN\nTask TypesReal users have different ways of defining the \u201cbest\u201d travel itinerary. To balance\nrealism with clarity of evaluation, we design two complementary soft preference types that capture\ndistinct optimization behaviors.\n\u2022Single Metric Optimization Task.The user specifies one numerical objective, and the agent\noptimizes along that single axis. For example: \u201cfind me the cheapest trip\u201d (minimize cost) or\n\u201cbook the highest-rated hotel available\u201d (maximize review score). Here, success means identifying\nthe extreme value on a clearly defined metric.\n\u2022Feature Count Maximization Task.The user provides a wish list of desirable attributes, such as\n\u201chotel with spa, pool, and in-room kitchen; flight with WiFi and direct routing.\u201d These are not strict\nrequirements\u2014an itinerary that satisfies three out of five features is preferable to one that satisfies\nonly one. The agent must therefore find the option that maximizes the total number of satisfied fea-\ntures, reflecting the realistic setting where users treat many preferences as flexible \u201cnice-to-haves.\u201d\nIn both task types, each taskalwaysincludes a set of hard constraints. Figure 3(A)shows examples\nof each task type. The optimization objectives are explicitly designed to be quantitatively measurable,\nensuring that each task is well-specified from a benchmarking perspective. At the same time, when\ncombined with hard constraints, these optimization objectives yield rich and realistic user behaviors.\nFor instance, in a single-metric optimization task, a user may aim for the cheapest option (optimization\nobjective) while still requiring a hotel rated at least three stars (hard constraint). In a feature-count\nmaximization task, a user may prefer business-class flights and luxury hotels (optimization objective)\nbut restrict total spending to under $2,000 (hard constraint). This combination balances benchmark\nclarity and comparability with realism and diversity in user preferences.\n4\nPreprint\nTable 1:COMPASSbenchmark characteristics.Left:Task distribution (Sec. 3.2) across three\ncomplexity levels, and split between the two optimization types, and data scale covering geographic\ncoverage, inventory size, and benchmark components.Right:Complete tool suite (Sec. 3.3) organized\nby category, listing 18 APIs for navigation, search, validation, and utility functions.\nCategory Item Count\nTask LevelsLevel I (Hotel only) 142\nLevel II"}
{"file": "2510.07043v1.pdf", "section": "3.2 TASKDESIGN", "chunk": ".Right:Complete tool suite (Sec. 3.3) organized\nby category, listing 18 APIs for navigation, search, validation, and utility functions.\nCategory Item Count\nTask LevelsLevel I (Hotel only) 142\nLevel II (Hotel, Flight) 69\nLevel III (Hotel, Flight, Permit) 70\nTotal Tasks 281\nTask TypesSingle Metric Optimization 154\nFeature Count Maximization 127\nDatabase ScaleNational Parks (Destinations) 20\nAirports 21\nHotel Room Bookable Offers 100,000+\nFlight Bookable Offers 67,000+\nTrail Permits 50Tool Category Tool Name\nHotelsearch hotel\ngethotel details\ngetroom details\nsearch location name\nFlightsearch airports\nsearch flights\ngetflight details\ngetairport location\nPermitlist permits\nsearch permit availability\nUtilitynotebook\ncalendar\ncalculator\ncalculate driving distance\nvalidate hotel( flight permit) id\nTask LevelsFor each task type, we also vary the difficulty of tasks. We design three levels of\nincreasing complexity, enabling systematic evaluation of agents\u2019 planning and reasoning capabilities.\n\u2022Level I (Hotel Only):Agents must navigate room configurations and pricing structures under\nconstraints such as budget, occupancy, and amenities.\n\u2022Level II (Hotel + Flight):Agents plan complete trips with both accommodations and trans-\nportation. This requires temporal reasoning to align departure dates with hotel availability while\ndistributing the budget across both services.\n\u2022Level III (Hotel + Flight + Permits):Agents incorporate reservations for guided tours or hiking\nexperiences with limited availability. Permits introduce additional temporal constraints: permit\navailability can dictate travel dates, which in turn determine feasible hotel and flight options.\nThis progression serves three evaluation purposes: (1)Practical relevance\u2014 comprehensive planning\nacross services (hotel, flights) reflects real deployment scenarios, where an ideal agent should produce\nfull itineraries that optimize user experience. (2)Sequential reasoning\u2014 higher levels require\nchaining constraints across domains, such as permit availability driving hotel selection, which then\nconstrains flight booking. (3)Tool orchestration complexity\u2014 agents must coordinate multiple APIs\nand handle compound constraints (e.g., \u201chotel within 3 hours\u2019 drive of airport\u201d).\nWe design tasks using 36 diverse user templates (e.g., budget backpacker, family with children, wed-\nding planner), each specifying a travel scenario with associated hard"}
{"file": "2510.07043v1.pdf", "section": "3.2 TASKDESIGN", "chunk": "(e.g., \u201chotel within 3 hours\u2019 drive of airport\u201d).\nWe design tasks using 36 diverse user templates (e.g., budget backpacker, family with children, wed-\nding planner), each specifying a travel scenario with associated hard constraints and soft preferences.\nFrom these templates, we generate 240 total tasks across the three complexity levels by sampling\nparameters such as travel dates, destinations, and budget ranges (See Tab. 1 for task breakdown). We\nfurther vary task difficulty along multiple axes: number of constraints, temporal complexity (flexible\nvs. fixed dates), spatial complexity (single vs. multiple destinations), and required tool interaction\ndepth. App. D.1 provides a detailed breakdown of all tasks and concrete examples.\n3.3 DATABASECONSTRUCTION& TOOLSDESIGN\nThe complexity of the underlying data directly shapes the difficulty and realism of an optimization\nbenchmark. Oversimplified databases fail to reflect real-world challenges, while overly complex\nones reduce practical relevance. To balance these factors,COMPASSuses real travel data obtained\nthrough RapidAPI,2constructing clean SQL databases from actual booking information. Our dataset\ncovers 20 U.S. National Parks, each with 20 hotels offering diverse room types, prices, amenities, and\npolicies. We also include flight data from four major airlines with varied routing options and booking\nclasses. Permit data is mocked, but modeled after real high-demand permits (e.g., the Yosemite Half\nDome hike3), ensuring scenarios remain authentic.\nCOMPASSalso provides a comprehensive tool suite that both mirrors commercial booking platforms\nand tests agents\u2019 reasoning. Core search functions (search hotel,search flights) are complemented by\n2https://rapidapi.com/DataCrawler/api/booking-com15\n3https://www.nps.gov/yose/planyourvisit/hdpermits.htm\n5\nPreprint\nutility tools (calculator, notebook) that support multi-step problem solving. Our search tools accept\nrich filters (e.g., price ranges, amenity requirements, routing preferences), providing agents the\noptionality to strategically conduct search. A full list of tools is shown in Tab. 1(right)and detailed\nin App. D.2.\n3.4 USERSIMULATORDESIGN\nWe use GPT-5 as the backbone of our user simulator. Unlike prior benchmarks (Yao et al., 2024; Lu\net al., 2024) that rely"}
{"file": "2510.07043v1.pdf", "section": "3.3 DATABASECONSTRUCTION& TOOLSDESIGN", "chunk": ".2.\n3.4 USERSIMULATORDESIGN\nWe use GPT-5 as the backbone of our user simulator. Unlike prior benchmarks (Yao et al., 2024; Lu\net al., 2024) that rely on a single static prompt for the entire dialogue, our simulator employsdynamic\nprompting\u2014adjusting instructions both across different users and within each conversation turn.\nCompared to existing works (Yao et al., 2024; Lu et al., 2024) that use static user prompt designs,\nour simulator enables fine-grained control over user behavior. Fig. 3(B)shows a demonstration\nof the dynamic prompting template and App. D.4 lists the verbatim prompt template. We control\ntwowithin-conversation dynamics:(1) Progressive constraint revelation, where users disclose\ntask information incrementally rather than upfront, and(2) Trust and communication patterns,\nwhere users vary in whether they accept recommendations immediately or demand explicit reasoning.\nWe also introduce two axes ofpersona diversity:(3) Constraint-checking reliability, where users\ndiffer in how consistently they detect and correct agent mistakes, and(4) Conversation style, where\npersonas range from concise and pragmatic to repetitive, emotional, or insistent.\nAt each dialogue turn, the dynamic sections of the user prompt are updated to reflect the current\nconversation state\u2014what task information has been revealed, what remains to be revealed, the user\u2019s\nfeedback on the agent\u2019s last response, and whether the user should trigger questioning behaviors\n(e.g., asking the agent to double-check a recommendation). Static sections, by contrast, encode user\npersona and task-level context. Together, this dynamic prompting framework produces diverse yet\ncontrolled conversations, allowing systematic evaluation of how agents adapt to different users.\n3.5 EVALUATIONMETRICDESIGN\nGround Truth GenerationTo evaluate agents reliably, we construct ground truth solutions for ev-\nery task through exhaustive search. We begin by enumerating all candidate hotels, flights (Levels II &\nIII), and permits (Level III), applying single-domain filters (e.g., direct flights, 4-star hotels). We then\ngenerate all possible cross-domain combinations and eliminate any that violate hard constraints such\nas budget limits or airport\u2013hotel driving times. The remaining set defines the complete feasible space.\nFor both task types (Sec 3.2), we compute an exactutility score. In single-metric optimization tasks,\nthe score"}
{"file": "2510.07043v1.pdf", "section": "3.5 EVALUATIONMETRICDESIGN", "chunk": "hard constraints such\nas budget limits or airport\u2013hotel driving times. The remaining set defines the complete feasible space.\nFor both task types (Sec 3.2), we compute an exactutility score. In single-metric optimization tasks,\nthe score is the numerical value of the target metric (e.g., total price, average review score). In feature-\ncount maximization tasks, the score is the number of desired features satisfied. This exhaustive\napproach guarantees correctness, since every feasible solution is enumerated and scored. We further\nvalidate ground truth quality by discarding tasks with too few feasible solutions or with too many tied\noptima (\u226520% of the feasible set), ensuring each task poses a meaningful optimization challenge.\nEvaluation MetricsFrom this solution library, we evaluate agents along two dimensions. For\nconstraint satisfaction, we check whether at least one agent-recommended itinerary belongs to the\nfeasible set, reporting the acceptable rate. Forpreference optimization, we assess how close the\nagent\u2019s best recommendation comes to the top-performing feasible solutions, reporting the optimality\nrate. We provide formal definition below:\nAcceptable Rate: Fraction of tasks agent\u2019s recommendation satisfies user\u2019s hard constraints.\nOptimality Rate: Fraction of tasks where agent\u2019s recommendation falls within the top i-th percentile.\n4 EXPERIMENT\n4.1 MAINRESULTS\nWe evaluateCOMPASSon a range of frontier closed-source models, including both reasoning-enabled\nand standard versions of GPT OpenAI et al. (2023); OpenAI (2025), Claude Anthropic (2025), and\nGemini(Comanici et al., 2025), as well as the frontier open-source model Qwen3 Yang et al. (2025).\nAll models support native tool calling, and we pass the tool schema directly to the model interface.\n6\nPreprint\nTable 2:Human evaluation of LLM user simulator quality.Left:Scoring rubrics used by\nannotators forclarityandcontextual appropriateness.Right:Human evaluation results on 198 user\nresponses. Rubric scores are reported as mean \u00b1standard deviation, as well as median, and quantile\ncut-offs. Error rates are reported as percentages of responses exhibiting each error type.\nClarity\n5:Crystal clear communication of requirements and goals\n4:Clear with no confusion about the task\n3:Mostly clear but some minor ambiguity that won\u2019t affect results\n2:Unclear communication that"}
{"file": "2510.07043v1.pdf", "section": "4.1 MAINRESULTS", "chunk": "responses exhibiting each error type.\nClarity\n5:Crystal clear communication of requirements and goals\n4:Clear with no confusion about the task\n3:Mostly clear but some minor ambiguity that won\u2019t affect results\n2:Unclear communication that could lead agent to make mistakes\n1:So unclear the agent will likely fail the task\nContextual Appropriateness\n5:As good as you would write or better\n4:Good without any obvious issues\n3:A bit unnatural but overall harmless to the conversation flow\n2:Noticeable issues that do not follow the conversation logic\n1:Response is completely disconnected from the logicMetric Result\nQuality Scores (1\u20135)\u2191\nClarity3.9 \u00b1 1.2\nMedian 4.0\nPercentiles (25/50/75/90/95) 3 / 4 / 5 / 5 / 5\nContextual appropriateness3.7 \u00b1 1.1\nMedian 4.0\nPercentiles (25/50/75/90/95) 3 / 4 / 5 / 5 / 5\nError Rates\nFactual hallucination 6.1%\nInformation revelation failures 4.0%\nConstraint checking failures 0.5%\nTo standardize outputs, agents are required to return a JSON object listing all \u201cpackage ids\u201d, which\nuniquely identifies a booking configuration, in the required fields (\u201cflights\u201d/\u201chotel\u201d/\u201cpermit\u201d), or\nleave them empty if not applicable. We evaluate each agent on the final itinerary it produces. Fig. 2\nreports aggregate performance. App. E provides inference details for each model, App. A.1 reports\na detailed breakdown table of model performances, and App. A.2 confirms benchmark stability with\nsmall error bars across repeated runs.\nWe find a significantgap between constrained-preference satisfaction and preference optimiza-\ntion. All models achieve high acceptable rates, showing they can interpret constraints and return\nfeasible solutions. However, their 5% and 10% optimality rates are substantially lower, revealing that\nagents often settle for feasible but suboptimal solutions even when objectives are simple and explicitly\nstated. Reasoning models outperform non-reasoning counterparts, and open-source Qwen3-32B\n(reasoning enabled) reaches close to 20% optimality rate\u2014comparable to some closed-source\nsystems such as"}
{"file": "2510.07043v1.pdf", "section": "4.1 MAINRESULTS", "chunk": "explicitly\nstated. Reasoning models outperform non-reasoning counterparts, and open-source Qwen3-32B\n(reasoning enabled) reaches close to 20% optimality rate\u2014comparable to some closed-source\nsystems such as GPT-4o. This suggests that strong agentic capabilities are emerging in open-source\nmodels. Nonetheless, GPT-5 showssuperior performance across both dimensions, outperforming\nall other models.\n4.2 VALIDATINGLLM USERSIMULATORQUALITY\nTo ensure our LLM user simulator produces realistic and reliable interactions, we randomly sample\n45 full conversations spanning diverse personas, task types, and agent interactions. From these, we\nextract198individual user responses for human evaluation by an independent expert annotation\nservice, covering the full range of scenarios in theCOMPASSbenchmark.\nAnnotators assess each response along two dimensions.(1) Rubric-based quality assessment:\nresponses are scored on two axes. Theclarityaxis measures whether user messages are expressed\nin a way that avoids misleading the agent or causing task failure; a score below 3 (on a 1\u20135 scale)\nindicates that the simulator may have provided confusing or incorrect task information. Thecontextual\nappropriatenessaxis evaluates whether responses flow naturally within a travel-planning dialogue.\n(2) Detailed error detection:since behaviors such as constraint revelation and trust patterns are\nexplicitly controlled (Sec. 3.4), annotators tag whether the simulator follows its prompts correctly\nwithout hallucination or deviation. The scoring rubric appears in Tab. 2(left), with full annotation\ninstructions and error definitions in App. C.\nResults (Tab. 2,right) show high quality scores (median clarity: 4; contextual appropriateness: 4 on a\n1\u20135 scale) and very low error rates\u20144% for constraint revelation and 0.5% for feedback accuracy.\nThese results confirm that the simulator reliably follows its script while producing clear, natural\nresponses, validating it as a realistic tool for benchmarking agent capabilities.\n7\nPreprint\n(A)\n(C)(B)\n(D)\nFigure 4:Performance breakdown across benchmark dimensions. (A)Performance degrades\nwith increasing plan coordination complexity (Levels II\u2013III), with open-source models showing\nespecially steep declines (green).(B)Constraint satisfaction rates drop as the number of hard\nconstraints increases, with only the strongest models handling 8"}
{"file": "2510.07043v1.pdf", "section": "4.2 VALIDATINGLLM USERSIMULATORQUALITY", "chunk": "Performance degrades\nwith increasing plan coordination complexity (Levels II\u2013III), with open-source models showing\nespecially steep declines (green).(B)Constraint satisfaction rates drop as the number of hard\nconstraints increases, with only the strongest models handling 8+ constraints reliably.(C)Preference\noptimization weakens as search complexity grows (more searches required to reach the ground-truth\noptimum).(D)Conversation efficiency analysis: how fast agent achieves solutions with the fewest\npost-information revelation turns.\n5 IN-DEPTHANALYSIS\n5.1 WHYDOAGENTSSTRUGGLE INTRAVELPLANNING?\nA central question is why agents sometimes fail to identify feasible solutions or converge to suboptimal\noutcomes. To investigate, we analyze performance acrossCOMPASStasks and identify statistical\npatterns linked to failure, breaking down results by three fundamental task characteristics.\nPlan-Coordination ComplexityWe first analyze performance across the three task levels (Sec. 3.2)\nto test how increasing planning demands affect agents. Higher levels expand the optimization\nscope (from single hotels to multi-service itineraries), enlarge the combinatorial solution space, and\nintroduce temporal dependencies that require sequential reasoning.\nFig. 4(A)shows a sharp decline in constrained-preference satisfaction as task complexity increases\n(similar optimality trends in App. A.3 Fig. 6). While most models handle Level I reliably, performance\ndrops substantially on Level II\u2019s cross-service coordination and collapses on Level III\u2019s temporal\nreasoning. Notably, open-source models perform competitively with frontier closed-source systems\nsuch as GPT-4o and Gemini-2.5-Flash on Level I tasks, but their performance degrades far more\nsteeply on Levels II\u2013III. This divergence underscores a critical weakness: while open-source models\ncan handle single-service(task) optimization, they struggle disproportionately with cross-services\ncoordination and sequential reasoning. This highlights a key limitation: current agents struggle to\npropagate constraints and plan coordination, with the challenge most acute for open-source models.\nConstraint ComplexityConstraint complexity measures how well agents handle tasks with\nincreasing numbers of hard requirements. Each task includes at least five fundamental constraints\n(e.g., location, dates, occupancy, budget), with additional non-negotiable requirements such as\namenities or policies further restricting feasibility. Fig. 4(B)shows performance degrades as\nconstraint count rises. GPT"}
{"file": "2510.07043v1.pdf", "section": "5.1 WHYDOAGENTSSTRUGGLE INTRAVELPLANNING?", "chunk": "fundamental constraints\n(e.g., location, dates, occupancy, budget), with additional non-negotiable requirements such as\namenities or policies further restricting feasibility. Fig. 4(B)shows performance degrades as\nconstraint count rises. GPT-5 and Claude maintain high constrained-preference satisfaction even\nwith 8+ constraints, whereas most other models drop sharply beyond 7. This indicates that despite\n8\nPreprint\n[TOOL\nCALL\n#1]\nsearch_airports\n{'park_name':\n'Yellowstone\nNational\nPark'}\n[TOOL\nCALL\n#2]\nget_weekday\n{'date_str':\n'2025-08-24'}\n[TOOL\nCALL\n#3]\nget_weekday\n{'date_str':\n'2025-08-28'}\n[TOOL\nCALL\n#4]\nsearch_flights\n{'origin':\n'JFK',\n'destination':\n'BZN',\n'departure_date':\n'2025-08-24',\n'return_date':\n'2025-08-28',\n'passengers':\n1,\n'max_price':\n2200\n}\n[TOOL\nCALL\n#5]\nsearch_flights\n{'origin':\n'JFK',\n'destination':\n'JAC',\n'departure_date':\n'2025-08-24',\n'return_date':\n'2025-08-28',\n'passengers':\n1,\n'max_price':\n2200}\n[skipping\nadditional\nsearch_flights\ncalls...]\n[TOOL\nCALL\n#11]\nsearch_hotel\n{'location':\n'West\nYellowstone',\n'start_date':\n'2025-08-24',\n'end_date':\n'2025-08-28',\n'num_guests':\n1,\n'max_price_nightly':\n300\n,\n'min_review_score':\n7,\n'free_cancellation':\nTrue\n}\n[skipping\nadditional\nalternating\nsearch_flights\nand\nsearch_hotel\ncalls...]\nJustification:\n-\nThey\nmeet\nyour\n5?day/4?night\nconstraint\ncleanly:\nTue-Sat\nblocks\ngive\nexactly\nfour\nhotel\nnights\n(Tue-Fri)\nand\navoid\npaying\nfor\na\ntypically\npricier\nSaturday\nnight.\n-\nMid?week\npricing\nadvantage\n:\nFlying\nand\nstaying\nmid?week\n(Tue-"}
{"file": "2510.07043v1.pdf", "section": "5.1 WHYDOAGENTSSTRUGGLE INTRAVELPLANNING?", "chunk": "ue-Fri)\nand\navoid\npaying\nfor\na\ntypically\npricier\nSaturday\nnight.\n-\nMid?week\npricing\nadvantage\n:\nFlying\nand\nstaying\nmid?week\n(Tue-Thu/Fridays)\nis\nusually\ncheaper\nthan\nincluding\nFriday\nnight/Saturday/Sunday,\nespecially\nin\ngateway\ntowns\naround\nYellowstone\nduring\npeak\nseason.\n-\nBroad\ncoverage\nacross\nAugust:\nSampling\nearly,\nmid,\nand\nlate\nAugust\ncatches\ntypical\nfare/hotel\nvolatility\nwithout\nbrute?forcing\nevery\nsingle\ndate.\nThis\nhelps\nsurface\nthe\nreliably\ncheapest\nweek\nwhile\nkeeping\nthe\nsearch\nefficient.\nGPT-5\nTool\nCall\nTrace\n[TOOL\nCALL\n#1]\nsearch_airports\n{'park_name':\n'Yellowstone\nNational\nPark'}\n[TOOL\nCALL\n#2]\nsearch_flights\n{'origin':\n'JFK',\n'destination':\n'COD',\n'departure_date':\n'2025-08-05',\n'return_date':\n'2025-08-10',\n'passengers':\n1,\n'max_price':\n1500\n}\n[TOOL\nCALL\n#3]\nsearch_flights\n{'origin':\n'JFK',\n'destination':\n'COD',\n'departure_date':\n'2025-08-12',\n'return_date':\n'2025-08-17',\n'passengers':\n1,\n'max_price':\n1500}\n[TOOL\nCALL\n#4]\nsearch_location_name\n{'query':\n'Yellowstone\nNational\nPark'}\n[TOOL\nCALL\n#5]\nsearch_hotel\n{'location':\n'Yellowstone\nNational\nPark',\n'start_date':\n'2025-08-05',\n'end_date':\n'\n2025-08-09\n'\n,\n'num_guests':\n1,\n'max_price_nightly':\n400\n}\n[TOOL\nCALL\n#6]\ncalculate\n{'expression':\n'308\n+\n888.21'}\nJustification\n:\nThis\nitinerary\ngives\nyou\nexactly\nwhat\nyou\nrequested:\n5\ndays\ntotal\nincluding\ntravel\ntime,\nwith\n4\nnights\nof\naccommodation\nnear\nYellow"}
{"file": "2510.07043v1.pdf", "section": "5.1 WHYDOAGENTSSTRUGGLE INTRAVELPLANNING?", "chunk": "Justification\n:\nThis\nitinerary\ngives\nyou\nexactly\nwhat\nyou\nrequested:\n5\ndays\ntotal\nincluding\ntravel\ntime,\nwith\n4\nnights\nof\naccommodation\nnear\nYellowstone.\nThe\nYellowstone\nRegional\nAirport\nis\nonly\n1\nhour\ndrive\nfrom\nthe\npark,\nand\nyour\nhotel\nis\nvery\nclose\nto\nthe\npark\nentrance.\nYou'll\nhave\nplenty\nof\nbudget\nleft\nover\nfor\nmeals,\npark\nentrance\nfees,\nand\nactivities!\nClaude\nSonnet\n4\nTool\nCall\nTrace\nHi,\nI'm\nlooking\nto\nplan\na\ntrip\nfrom\nJFK\nto\nYellowstone\nNational\nPark\nfor\none\nperson\n.\nI\nwant\nthe\nabsolute\ncheapest\noption\n(hotel\nand\nflight\ncombination)\nfor\na\n5-day\ntrip,\nincluding\nflying\ntime,\nwhich\nwill\ncover\n4\nnights\nin\na\nhotel.\nI'm\ncompletely\nflexible\non\nthe\ndates\nas\nlong\nas\nit's\nsometime\nin\nAugust\n2025.\nMy\ntotal\nbudget\nfor\nflights\nand\nhotel\ncombined\nis\n$2,200\nor\nless.\nCan\nyou\nhelp\nme\narrange\nthis?\nI\nalso\nwant\nyou\nto\nexplain\nyour\nreasoning\nbehind\nhow\nyou\nsolve\nthis\nproblem.\nPlease\nexplain\nyour\nsearch\nprocess\nand\njustify\nwhy\nyou\nwill\nthe\ntools\nin\nwhat\norder\nand\nhow\nto\npass\nin\nthe\narguments.\nFigure 5:Case study of tool calls and reasoning traces.Top:Prompt given to models for a Level\nII task, with explicit reasoning requested.Bottom:GPT-5 (left) demonstrates strategic planning by\navoiding weekends, systematically exploring date ranges, and using optional parameters (e.g., price\nfilters) to narrow searches. Claude-Sonnet-4 (right) applies optional parameters but searches only\ntwo arbitrary dates without justification. It also makes a temporal coordination error by misaligning\nhotel and flight dates.\nusers explicitly stating requirements and correcting violations,"}
{"file": "2510.07043v1.pdf", "section": "5.1 WHYDOAGENTSSTRUGGLE INTRAVELPLANNING?", "chunk": "narrow searches. Claude-Sonnet-4 (right) applies optional parameters but searches only\ntwo arbitrary dates without justification. It also makes a temporal coordination error by misaligning\nhotel and flight dates.\nusers explicitly stating requirements and correcting violations, many agents struggle to reliably track\nand enforce multiple constraints\u2014highlighting that robust constraint management remains limited\nto the most advanced models.\nOptimization ComplexityBeyond satisfying hard constraints, agents should aim for best option.\nWe capture this throughsearch complexity, defined as the number of distinct searches required\nto guarantee finding the optimum. For example, a request for the cheapest weekend in August\nat Yosemite requires searching all four weekends (search complexity = 4). Fig. 4(C)reports\noptimality rates only on tasks where the agent produced a feasible solution\u2014that is, among valid\nrecommendations, we measure how often they fall within the top 5th percentile of all feasible options.\nWhen search complexity is minimal (1 search, with fully specified dates and location), agents achieve\nhigh optimality. As complexity increases\u2014medium ( <5 searches) to hard ( >5 searches)\u2014optimal\nrates decline across all models. Even GPT-5 drops from over 80% on easy tasks to around 60% on\nhard tasks. This demonstrates that while agents can reliably find feasible solutions, they often fail to\nexplore the solution space deeply enough to identify the truly optimal choice.\n5.2 CONVERSATIONEFFICIENCY\nBeyond final performance (Sec. 4.1), efficiency is critical for user-facing agents, as long, drawn-\nout interactions often cause frustration. We measure efficiency by leveraging our controlled user\nsimulator (Sec. 3.4). For each conversation, we record the turn t\u2217when the user has disclosed all\nhard constraints and preference objectives, and compute the extra turns \u2206tthe agent requires to\nproduce its final recommendation. Extra turns typically arise when agents either fail to provide a\nrecommendation or recommend options that violate constraints\u2014in both cases, the user continues\nprompting the agent to try again until a valid solution is provided or max-turn (10) runs out.\nFig. 4(D)shows that GPT-5 is superior not only in task success but also in conversational economy,\nconsistently requiring the fewest post-revelation turns. In App. A.4, we confirm that the observed dif-\nference is not caused by"}
{"file": "2510.07043v1.pdf", "section": "5.2 CONVERSATIONEFFICIENCY", "chunk": "GPT-5 is superior not only in task success but also in conversational economy,\nconsistently requiring the fewest post-revelation turns. In App. A.4, we confirm that the observed dif-\nference is not caused by variance in user simulator behavior. Importantly, efficiency varies even among\nmodels with similar success rates: for instance, Gemini-2.5-Flash takes substantially more turns\n9\nPreprint\nthan GPT-4o despite comparable final outcomes. This highlights that raw success metrics alone are\ninsufficient\u2014how efficiently an agent reaches its solution is just as important as whether it succeeds.\n5.3 CASESTUDY: SMARTSTRATEGIES FORTRAVELPLANNING\nWhile our ground truth relies on exhaustive search (Sec. 3.5), real travel planning requires reasoning,\nstrategy, and efficient tool use. To examine whether agents display such behaviors, we analyze their\ntool usage and reasoning on a representative Level II task, asking them to explain their thought\nprocess (Fig. 5,top).\nStrategic vs. Naive PlanningGPT-5 exhibits strategic behavior (Fig. 5,bottom left). Rather than\nsearching blindly, it checks the calendar, targets weekday departures (anticipating midweek price\ndrops), and samples across August to ensure coverage. This heuristic aligns with our data\u2014midweek\ntrips are indeed cheaper on average\u2014but whether it is \u201csmart\u201d depends on context, as models may\napply incorrect heuristics elsewhere. A more capable agent would either (i)verifysuch assumptions\nor (ii)articulatethem to the user for confirmation. GPT-5 does the latter, making its reasoning trans-\nparent and allowing user-guided refinement. Nevertheless, it also adds unjustified parameters\u2014e.g.,\nenforcing free cancellation=True \u2014which may exclude valid cheaper options, illustrating\nthe trade-off between strategic search and fidelity to user intent. Claude Sonnet-4 (Fig. 5,bottom\nright) behaves more naively: it searches only two dates without justification and commits a major\ncoordination error by misaligning hotel and flight dates. Its reasoning traces are generic (\u201cgives\nexactly what you requested\u201d), concealing such mistakes.\nThese results show that intelligent travel planning requires more than constraint satisfaction or\nexhaustive search; it depends on heuristics and reasoning grounded in tool use. We"}
{"file": "2510.07043v1.pdf", "section": "5.3 CASESTUDY: SMARTSTRATEGIES FORTRAVELPLANNING", "chunk": "are generic (\u201cgives\nexactly what you requested\u201d), concealing such mistakes.\nThese results show that intelligent travel planning requires more than constraint satisfaction or\nexhaustive search; it depends on heuristics and reasoning grounded in tool use. We expect future\nagents to employ richer strategies, such as maintaining a running \u201cbest\u201d option and progressively\nfiltering around it, balancing efficiency with optimality. Overall, our case studies demonstrate that\nCOMPASS evaluates reasoning-driven planning behaviors beyond mere tool execution.\n6 CONCLUSION ANDDISCUSSION\nWe introducedCOMPASS, a benchmark for evaluating LLM agents on constrained-preference\noptimization in realistic travel planning. The environment integrates three core components\u2014a\ncontrollable user simulator, structured databases of hotels, flights, and permits, and a tool ecosystem\nmirroring commercial booking platforms. Tasks are organized along two axes, optimization type and\nplanning level, to jointly assess constraint satisfaction and preference optimization.\nUsingCOMPASS, we evaluated frontier models and revealed two key limitations: anaccept-\nable\u2013optimal gap, where agents stop at feasible but suboptimal solutions, and aplan-coordination gap,\nwhere performance collapses when aligning multiple services under temporal and budget constraints.\nBenchmark scope and practical relevance.Travel planning is both a realistic use case and a\nrich testbed for evaluating agentic reasoning. Our benchmark currently focuses on flights, ho-\ntels, and permits, but real itineraries often involve additional elements\u2014car rentals, restaurants, or\nmulti-destination trips\u2014where agents must balance commute times and sequence visits efficiently.\nExtendingCOMPASSto web-based environments, where agents navigate unstructured webpages and\ninconsistent interfaces, would further enhance task realism.\nUser interactions and evaluation realism.The user simulator enables controllable multi-turn\ndynamics, but real users exhibit greater variability\u2014changing goals mid-dialogue, providing\nunderspecified inputs, or asking for clarifications. Expanding these behaviors would help evaluate\nagent robustness in more naturalistic settings.\nReasoning and agentic capabilities.While ground-truth solutions rely on exhaustive search,\neffective planning requires reasoning and heuristic strategies. As seen in our case study (Sec. 5.3),\ncapable agents use simplifications such as strategic date selection or parameterized searches to\napproximate optimality. Future work should explore structured agent workflows\u2014planning\u2013execution\npipelines, preference-tracking memory, or multi-agent collaboration\u2014to address limitations in native\ntool"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": "use simplifications such as strategic date selection or parameterized searches to\napproximate optimality. Future work should explore structured agent workflows\u2014planning\u2013execution\npipelines, preference-tracking memory, or multi-agent collaboration\u2014to address limitations in native\ntool use.\n10\nPreprint\nOverall,COMPASSestablishes constrained-preference optimization as a unified framework for\nstudying agent reasoning and exposes concrete challenges that must be solved for LLMs to act as\ntrustworthy, user-aligned assistants in the real world.\nREFERENCES\nMarwa Abdulhai, Isadora White, Charlie Snell, Charles Sun, Joey Hong, Yuexiang Zhai, Kelvin Xu,\nand Sergey Levine. LMRL gym: Benchmarks for multi-turn reinforcement learning with language\nmodels.arXiv [cs.CL], 2023. URLhttp://arxiv.org/abs/2311.18232.\nAnthropic. Claude 4, 2025. URLhttps://claude.ai/. [Large language model].\nVictor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. \u03c42-bench: Evaluating\nconversational agents in a dual-control environment, 2025. URL http://arxiv.org/abs/\n2506.07982.\nPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I \u02dcnigo Casanueva, Stefan Ultes, Osman\nRamadan, and Milica Ga \u02c7si\u00b4c. MultiWOZ \u2013 a large-scale multi-domain wizard-of-oz dataset for\ntask-oriented dialogue modelling.arXiv [cs.CL], 2018. URL http://arxiv.org/abs/\n1810.00278.\nGheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit\nDhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin\nGaffney, Asaf Aharoni, Nathan Lintz, Tiago Cardal Pais, Henrik Jacobsson, Idan Szpektor, Nan-\nJiang Jiang, and Krishna Haridasan. Gemini 2.5: Pushing the Frontier with Advanced Reason-\ning, Multimodality, Long Context, and Next Generation Agentic Capabilities.ar"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": "Idan Szpektor, Nan-\nJiang Jiang, and Krishna Haridasan. Gemini 2.5: Pushing the Frontier with Advanced Reason-\ning, Multimodality, Long Context, and Next Generation Agentic Capabilities.arXiv preprint\narXiv:2507.06261, July 2025.\nZhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong\nSun, and Yang Liu. StableToolBench: Towards stable large-scale benchmarking on tool learning of\nlarge language models.arXiv [cs.CL], 2024. URL http://arxiv.org/abs/2403.07714 .\nZhicheng Guo, Sijie Cheng, Yuchen Niu, Hao Wang, Sicheng Zhou, Wenbing Huang, and Yang Liu.\nStableToolBench-MirrorAPI: Modeling tool environments as mirrors of 7,000+ real-world APIs.\narXiv [cs.CL], 2025. URLhttp://arxiv.org/abs/2503.20527.\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. A simple\nlanguage model for task-oriented dialogue.arXiv [cs.CL], 2020. URL http://arxiv.org/\nabs/2005.00796.\nHarsh Kohli, Sachin Kumar, and Huan Sun. GroundCocoa: A benchmark for evaluating compositional\n& conditional reasoning in language models.arXiv [cs.CL], 2024. URL http://arxiv.org/\nabs/2404.04237.\nPhilippe Laban, Hiroaki Hayashi, Yingbo Zhou, and Jennifer Neville. LLMs get lost in multi-turn\nconversation.arXiv [cs.CL], 2025. URLhttp://arxiv.org/abs/2505.06120.\nHsien-Chin Lin, Christian Geishauser, Shutong Feng, Nurul Lubis, Carel van Niekerk, Michael Heck,\nand Milica Ga \u02c7si\u00b4c. GenTUS: Simulating user behaviour and language in task-oriented dialogues\nwith generative transformers.arXiv [cs.CL], 2022. URL http://arxiv.org/abs/"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": "Milica Ga \u02c7si\u00b4c. GenTUS: Simulating user behaviour and language in task-oriented dialogues\nwith generative transformers.arXiv [cs.CL], 2022. URL http://arxiv.org/abs/2208.\n10817.\nJiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma,\nShen Ma, Mengyu Li, Guoli Yin, Zirui Wang, and Ruoming Pang. ToolSandbox: A stateful,\nconversational, interactive evaluation benchmark for LLM tool use capabilities.arXiv [cs.CL],\n2024. URLhttp://arxiv.org/abs/2408.04682.\nOpenAI. GPT-5. https://openai.com/index/introducing-gpt-5/ , August 2025.\nLarge language model.\nOpenAI, Achiam, Josh, Agarwal, Sandhini, et al. GPT-4 technical report.arXiv preprint\narXiv:2303.08774, 2023.\n11\nPreprint\nShishir G Patil, Huanzhi Mao, Charlie Cheng-Jie Ji, Fanjia Yan, Vishnu Suresh, Ion Stoica, and\nJoseph E. Gonzalez. The berkeley function calling leaderboard (BFCL): From tool use to agentic\nevaluation of large language models. InForty-second International Conference on Machine\nLearning, 2025.\nAkshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei\nLiu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, Shelby Heinecke, Weiran Yao, Huan Wang,\nSilvio Savarese, and Caiming Xiong. APIGen-MT: Agentic pipeline for multi-turn data generation\nvia simulated agent-human interplay.arXiv [cs.CL], 2025. URL http://arxiv.org/abs/\n2504.03601.\nCheng Qian, Zuxin Liu, Akshara Prabhakar, Zhiwei Liu, Jianguo Zhang, Haolin Chen, Heng Ji,\nWeiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong,"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": "Liu, Akshara Prabhakar, Zhiwei Liu, Jianguo Zhang, Haolin Chen, Heng Ji,\nWeiran Yao, Shelby Heinecke, Silvio Savarese, Caiming Xiong, and Huan Wang. UserBench: An\ninteractive gym environment for user-centric agents, 2025. URL http://arxiv.org/abs/\n2507.22034.\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. Towards\nscalable multi-domain conversational agents: The schema-guided dialogue dataset.arXiv [cs.CL],\n2019. URLhttp://arxiv.org/abs/1909.05855.\nHayley Ross, Ameya Sunil Mahabaleshwarkar, and Yoshi Suhara. When2Call: When (not) to call\ntools.arXiv [cs.CL], 2025. URLhttp://arxiv.org/abs/2504.18851.\nPararth Shah, Dilek Hakkani-T \u00a8ur, Gokhan T \u00a8ur, Abhinav Rastogi, Ankur Bapna, Neha Nayak, and\nLarry Heck. Building a conversational agent overnight with dialogue self-play.arXiv [cs.AI], 2018.\nURLhttp://arxiv.org/abs/1801.04871.\nKarthik Valmeekam, Matthew Marquez, Alberto Olmo, Sarath Sreedharan, and Subbarao Kamb-\nhampati. PlanBench: An extensible benchmark for evaluating large language models on planning\nand reasoning about change.arXiv [cs.CL], 2022. URL http://arxiv.org/abs/2206.\n10498.\nYanming Wan, Jiaxing Wu, Marwa Abdulhai, Lior Shani, and Natasha Jaques. Enhancing\npersonalized multi-turn dialogue with curiosity reward.arXiv [cs.CL], 2025. URL http:\n//arxiv.org/abs/2504.03206.\nXingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. MINT:\nEvaluating LLMs in multi-turn interaction with tools and language feedback.arXiv [cs"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": "Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. MINT:\nEvaluating LLMs in multi-turn interaction with tools and language feedback.arXiv [cs.CL], 2023.\nURLhttp://arxiv.org/abs/2309.10691.\nShujin Wu, May Fung, Cheng Qian, Jeonghwan Kim, Dilek Hakkani-Tur, and Heng Ji. Aligning\nLLMs with individual preferences via interaction.arXiv [cs.CL], 2024. URL http://arxiv.\norg/abs/2410.03642.\nJian Xie, Kai Zhang, Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong Tian, Yanghua Xiao, and\nYu Su. TravelPlanner: A benchmark for real-world planning with language agents.arXiv [cs.CL],\n2024. URLhttp://arxiv.org/abs/2402.01622.\nHongshen Xu, Zichen Zhu, Lei Pan, Zihan Wang, Su Zhu, Da Ma, Ruisheng Cao, Lu Chen, and\nKai Yu. Reducing tool hallucination via reliability alignment.arXiv [cs.CL], 2024. URL\nhttp://arxiv.org/abs/2412.04141.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang\nGao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu,\nHao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin\nYang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang,\nLe Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui\nMen, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang\nRen, Xinyu Wang, Xinyu Zhang, Xuancheng Ren"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": ", Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang\nRen, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger\nZhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan\nQiu. Qwen3 technical report, 2025. URLhttps://arxiv.org/abs/2505.09388.\n12\nPreprint\nShunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. \u03c4-bench: A benchmark for\ntool-agent-user interaction in real-world domains.arXiv [cs.AI], 2024. URL http://arxiv.\norg/abs/2406.12045.\nHuaixiu Steven Zheng, Swaroop Mishra, Hugh Zhang, Xinyun Chen, Minmin Chen, Azade\nNova, Le Hou, Heng-Tze Cheng, Quoc V Le, Ed H Chi, and Denny Zhou. NATURAL\nPLAN: Benchmarking LLMs on natural language planning.arXiv [cs.CL], 2024. URL\nhttp://arxiv.org/abs/2406.04520.\nLucen Zhong, Zhengxiao Du, Xiaohan Zhang, Haiyi Hu, and Jie Tang. ComplexFuncBench:\nExploring multi-step and constrained function calling under long-context scenario.arXiv [cs.CL],\n2025. URLhttp://arxiv.org/abs/2501.10132.\nYifei Zhou, Song Jiang, Yuandong Tian, Jason Weston, Sergey Levine, Sainbayar Sukhbaatar, and\nXian Li. SWEET-RL: Training multi-turn LLM agents on collaborative reasoning tasks, 2025.\nURLhttp://arxiv.org/abs/2503.15478.\n13\nPreprint\nA ADDITIONALRESULTS\nA.1 FULLBENCHMARKRESULT\nIn Table 3, we report the benchmark performance in table form, including a detailed performance\nbreakdown by three task levels.\nTable 3:Model performance breakdown by complexity level onCOMPASSbenchmark. We\nevaluate our benchmark across three complexity levels:Level"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": "3, we report the benchmark performance in table form, including a detailed performance\nbreakdown by three task levels.\nTable 3:Model performance breakdown by complexity level onCOMPASSbenchmark. We\nevaluate our benchmark across three complexity levels:Level I(hotel-only tasks, \u223c142 tasks),Level\nII(flight+hotel tasks, \u223c69 tasks), andLevel III(complex tasks with permits, \u223c70 tasks). Performance\ngenerally degrades with increased complexity, with open-source models showing dramatic capability\ngaps in multi-step planning.\nModel Level Optimal Rate (%)\u2191Acceptable Rate (%)\u2191\nTop-5 Top-10 Top-20ThinkGPT-5I 68.3 77.5 88.0 96.5\nII 52.2 56.5 69.6 89.9\nIII 31.4 40.0 48.6 74.3\nTotal 50.6 58.0 68.7 86.9\nClaude Opus 4I 51.4 63.4 77.5 89.4\nII 26.1 27.5 39.1 68.1\nIII 27.1 35.7 38.6 54.3\nTotal 34.9 42.2 51.7 70.6\nGemini 2.5 ProI 42.3 52.1 62.7 76.8\nII 20.6 29.4 30.9 41.2\nIII 22.9 30.0 34.3 50.0\nTotal 28.6 37.2 42.6 56.0ClosedGPT-4oI 40.1 53.5 69.0 76.8\nII 17.4 23.2 30.4 47.8\nIII 7.1 14.3 14.3 28.6\nTotal 21.6 30.3 37.9 51.1\nGPT-4.1I 31.7 44.4 56.3 66.9\nII 23.2 33.3 42.0 71.0\nIII 22.9 34.3 41"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": "4.1I 31.7 44.4 56.3 66.9\nII 23.2 33.3 42.0 71.0\nIII 22.9 34.3 41.4 57.1\nTotal 25.9 37.3 46.6 65.0\nClaude Sonnet 4I 43.0 54.2 69.7 83.8\nII 34.8 36.2 49.3 66.7\nIII 24.3 31.4 40.0 55.7\nTotal 34.0 40.6 53.0 68.7\nGemini 2.5 FlashI 31.7 42.3 52.8 62.0\nII 21.7 36.2 40.6 59.4\nIII 15.7 20.0 24.3 31.4\nTotal 23.0 32.8 39.2 50.9OpenQwen3 8BI 30.4 39.1 49.3 66.7\nII 0.0 1.4 1.4 5.8\nIII 0.0 0.0 0.0 0.0\nTotal 10.1 13.5 16.9 24.2\nQwen3 14BI 26.8 34.5 47.9 69.7\nII 8.7 8.7 11.6 29.0\nIII 5.7 7.1 10.0 14.3\nTotal 13.7 16.8 23.2 37.7\nQwen3 32BI 33.3 46.8 61.0 78.0\nII 4.3 4.3 7.2 17.4\nIII 1.4 2.9 2.9 2.9\nTotal 13.0 18.0 23.7 32.8\nA.2 BENCHMARKSTABILITY\nTo demonstrate the stability and reliability of ourCOMPASSbenchmark, we evaluate Claude-4-\nSonnet across 5 independent runs on the full benchmark"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": "18.0 23.7 32.8\nA.2 BENCHMARKSTABILITY\nTo demonstrate the stability and reliability of ourCOMPASSbenchmark, we evaluate Claude-4-\nSonnet across 5 independent runs on the full benchmark suite (Table 4). Each run uses identical\nexperimental settings but different random seeds for task sampling and user simulator behavior. The\nconsistency across runs demonstrates that our user simulator produces reproducible interactions\ndespite its stochastic nature, and that the evaluation metrics accurately capture systematic differences\nin agent performance rather than measurement noise.\nA.3 ADDITIONALANALYSISRESULTS\nIn this section, we report additional analysis results discuss in Sec.5. In Fig. 6, we include full\nresults for task level analysis. In Fig. 7, we include additional result (conversation turn efficiency and\nacceptable rate) for agent task efficiency analysis in Sec. 5.2.\n14\nPreprint\nTable 4:Benchmark stability.Benchmark stability across 5 independent runs with Claude-4-Sonnet.\nResults show low variance, demonstrating reliable measurement of agent capabilities.\nMetric Run 1 Run 2 Run 3 Run 4 Run 5 Mean (Std)\nOptimal Top-5 33.97 34.96 33.44 31.92 34.43 33.74 (1.04)\nOptimal Top-10 39.76 41.60 40.56 39.44 41.04 40.48 (0.80)\nOptimal Top-20 47.44 53.36 54.96 52.80 54.40 52.59 (2.69)\nSolve Rate 68.80 68.80 67.20 68.80 66.08 67.94 (1.12)\nLevel I Level II Level III020406080100Top-5% Optimal Rate (%)\nClosed Source Models\nClaude Sonnet 4\nClaude Opus 4\nGemini 2.5 Flash\nClaude Opus 4.1\nGPT-4.1\nGPT-4o\nGPT-5\nLevel I Level II Level III020406080100Top-5% Optimal Rate (%)\nOpen Source Models\nQwen3-8B\nQwen3-14B\nQwen3-32BReference (Closed Source"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": "-5\nLevel I Level II Level III020406080100Top-5% Optimal Rate (%)\nOpen Source Models\nQwen3-8B\nQwen3-14B\nQwen3-32BReference (Closed Source)\nClaude Opus 4.1 (ref)\nGPT-4o (ref)\nGemini 2.5 Flash (ref)\nLevel I Level II Level III020406080100Acceptable Rate (%)\nClosed Source Models\nClaude Sonnet 4\nClaude Opus 4\nGemini 2.5 Flash\nClaude Opus 4.1\nGPT-4.1\nGPT-4o\nGPT-5\nLevel I Level II Level III020406080100Acceptable Rate (%)\nOpen Source Models\nQwen3-8B\nQwen3-14B\nQwen3-32BReference (Closed Source)\nClaude Opus 4.1 (ref)\nGPT-4o (ref)\nGemini 2.5 Flash (ref)\nFigure 6:Task level breakdown performances.Full result for Fig. 4(A). As we increase task levels,\nsolving the task requires more complex temporal reasoning and planning. Agents struggles to solve\nLevel II and III tasks and the performance drop is particularly prominent for open-source models.\nA.4 ADDITIONALCONVERSATIONSTATISTICS\nIn Fig. 8(top), we confirm that the distribution of turns at which users reveal complete task information\nis identical across all agent models. This validates that the efficiency differences observed in Fig. 4\n(D)and Fig. 7 arise from agent behavior rather than user variation. Fig. 8(bottom)shows box plots\nof full conversation lengths, highlighting cross-model differences in dialogue efficiency.\n15\nPreprint\n0.4 0.6 0.8 1.0 1.2 1.4 1.6\nAverage Turns After Full Information Revelation30405060708090Acceptable Rate (%)\nGPT-5\nClaude Opus 4.1\nClaude Opus 4\nGemini 2.5 ProClaude Sonnet 4 GPT-4.1\nGPT-4o Gemini 2.5 Flash\nQwen3 14B\nQwen3 32B\nQwen3 8BThink Models\nClosed-Source\nOpen"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": "net 4 GPT-4.1\nGPT-4o Gemini 2.5 Flash\nQwen3 14B\nQwen3 32B\nQwen3 8BThink Models\nClosed-Source\nOpen-Source\nFigure 7:Conversation efficiency versus Acceptable Rate.Additional results for Fig. 5(D)In\naddition to measuring conversation efficiency against optimal rate, we also measure Acceptable Rate.\nGPT-5\nClaude Opus 4.1Claude Opus 4 Gemini 2.5 ProClaude Sonnet 4GPT-4.1 GPT-4o\nGemini 2.5 FlashQwen3 14B Qwen3 32BQwen3 8B2468Constraint Full Revelation TurnDistribution of Constraint Revelation Timing Across Models\nThink Models\nClosed-Source\nOpen-Source\nGPT-5\nClaude Opus 4.1Claude Opus 4 Gemini 2.5 ProClaude Sonnet 4GPT-4.1 GPT-4o\nGemini 2.5 FlashQwen3 14B Qwen3 32BQwen3 8B246810Conversation Length (turns)\nDistribution of Conversation Lengths Across Models\nFigure 8:Conversation efficiency controls.Top:Distribution of user turn t\u2217when all task infor-\nmation is revealed, showing consistent user behavior across models.Bottom:Box plots of overall\nconversation length, illustrating efficiency differences across agent models. Colors indicate model\ncategories (e.g. open-source, closed-source).\n16\nPreprint\nB RELATEDWORK\nTool-Use BenchmarksRecent work has increasingly recognized the importance of evaluating\nlanguage model agents in multi-turn settings where information is revealed progressively. Yao et al.\n(2024) establishes key requirements for realistic agent evaluation, emphasizing the need for agents\nto interact seamlessly with both humans and APIs over long horizons while adhering to complex\npolicies. However, existing benchmarks (Guo et al., 2024; 2025; Patil et al., 2025; Zhong et al.,\n2025) often feature simplified instruction-following setups where agents interact autonomously with\ncomplete information upfront, lacking realistic human-in-the-loop interaction.Wang et al. (2023)\nprovides multi-turn tool use evaluation but focuses on coding scenarios with overly helpful users,"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": "feature simplified instruction-following setups where agents interact autonomously with\ncomplete information upfront, lacking realistic human-in-the-loop interaction.Wang et al. (2023)\nprovides multi-turn tool use evaluation but focuses on coding scenarios with overly helpful users,\nwhile Patil et al. (2025) introduces multi-step and multi-turn function calling but with predetermined\nconversation trajectories. Guo et al. (2024) offers comprehensive tool libraries through API crawling,\nthough task instructions based directly on tool calls reduce the challenge of realistic tool selection\nunder uncertainty. Xu et al. (2024) identifies tool selection and usage hallucinations as key failure\nmodes, while Ross et al. (2025) explores when agents should defer tool invocation in favor of\nclarification dialogue.\nPlanning BenchmarksRecent planning benchmarks have emerged to evaluate agent capabilities\nin constraint satisfaction and optimization tasks. Xie et al. (2024) introduces multi-day travel\nplanning with hard constraints and budget optimization, requiring agents to coordinate between\nmultiple APIs for transportation, accommodation, and activities. Valmeekam et al. (2022) provides a\ncomprehensive evaluation framework for planning tasks that require reasoning about state changes\nand action consequences. Zheng et al. (2024) focuses on realistic planning in natural language\nwith tasks like trip planning, meeting planning, and calendar scheduling, providing tool outputs as\ncontext to eliminate tool-use complexity while revealing significant performance drops as problem\ncomplexity increases. Kohli et al. (2024) connects compositional and conditional reasoning to flight\nbooking scenarios, presenting detailed user preferences with flight options in multiple-choice format.\nZheng et al. (2024) extends planning evaluation to GUI-based environments where agents must\nnavigate complex interfaces to accomplish user goals. These benchmarks emphasize the importance\nof constraint adherence in realistic planning scenarios. Our work expand on these benchmarks by\nintroduction user preference optimization.\nTask-Oriented Dialogue SystemsTraditional task-oriented dialogue datasets provide foundational\nevaluation frameworks that inspire our user simulator design. Budzianowski et al. (2018) and Rastogi\net al. (2019) offer crowd-sourced human-to-human dialogues with informable and requestable slots\nfor constraint specification. These datasets encourage goal changes when constraints cannot be\nsatisfied, mimicking real user adaptability. Hosseini-Asl et al. (2020) demonstrates that unified GPT\nmodels can handle dialogue state tracking"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": "for constraint specification. These datasets encourage goal changes when constraints cannot be\nsatisfied, mimicking real user adaptability. Hosseini-Asl et al. (2020) demonstrates that unified GPT\nmodels can handle dialogue state tracking, action prediction, and response generation simultaneously.\nHowever, these benchmarks typically lack the tool integration and utility optimization challenges\npresent in modern agent evaluation settings.\nMulti-turn InteractionA critical challenge in multi-turn evaluation lies in creating realistic user\nsimulators that can model underspecified preferences and progressive constraint revelation. Laban\net al. (2025) demonstrates that current models struggle significantly with synthesizing information\nacross turns, particularly when users reveal requirements in non-sequential \u201dshards\u201d rather than\nlogical order. Abdulhai et al. (2023) offers a toolkit for multi-turn RL evaluation on tasks like\ncity guessing and maze navigation that, despite their simplicity, effectively capture the information-\ngathering nature of multi-turn conversations. User simulation approaches range from rule-based\nsystems to sophisticated neural models: Lin et al. (2022) uses BERT-based models for generating\nboth semantic actions and natural language utterances, while Shah et al. (2018) employs machine-\ngenerated dialogue flows rewritten by humans. Recent preference learning work includes Wan et al.\n(2025), which introduces turn-based rewards based on belief updates about user types, and Wu\net al. (2024); Prabhakar et al. (2025); Zhou et al. (2025), which develops diverse user personas and\nmulti-turn preference datasets for SFT and RL training paradigms.\n17\nPreprint\nC HUMANEVALUATION\nC.1 HUMANEVALUATIONINSTRUCTION\nWe provide the complete annotation protocol and instructions used for human evaluation of the LLM\nuser simulator quality in Section 4.2.\nAnnotation Task Overview\nHuman annotators evaluated 198 user responses sampled from 45 full conversations. Each response\nwas assessed for both objective errors and subjective quality metrics by third-party independent expert\nannotation services.\nTask Context for Annotators\nUnderstanding the Travel Planning Task\nHard Constraints (Must-Have Requirements):Non-negotiable requirements forming a checklist\nwhere EVERY item must be satisfied. Examples include:\n\u2022 \u201cMust allow pets\u201d - if not pet-friendly, the hotel is unacceptable\n\u2022 \u201cMust accommodate at least 4 people\u201d - 3 people max would fail this"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": "requirements forming a checklist\nwhere EVERY item must be satisfied. Examples include:\n\u2022 \u201cMust allow pets\u201d - if not pet-friendly, the hotel is unacceptable\n\u2022 \u201cMust accommodate at least 4 people\u201d - 3 people max would fail this requirement\n\u2022 \u201cMust be within $800 total\u201d - anything over $800 fails\nOptimization Objective:Once all must-haves are met, this determines which option to choose:\n\u2022 \u201cI want the cheapest option\u201d - among all acceptable hotels, pick the lowest price\n\u2022 \u201cI want the highest rated\u201d - among all acceptable hotels, pick the best reviews\n\u2022\u201cI want the most amenities from my wish-list\u201d - among all acceptable hotels, pick the one\nwith most desired features\nUser Simulator Configuration\nPersona AttributesEach simulated user follows a defined persona with three key attributes that\nannotators must consider:\n\u2022trust level: \u201csuspicious\u201d or \u201ctrusting\u201d - Whether the user blindly trusts the agent or asks for\nreasoning and double-checking\n\u2022attention level: \u201clow\u201d, \u201cmedium\u201d or \u201chigh\u201d - Whether the user pays attention to agent\u2019s\nresponse and notices obvious mistakes (such as hard constraint violations)\n\u2022communication style: Verbal style defining formality and expression patterns\nUser Script ComponentsAnnotators were provided with the following script information for each\nuser:\n\u2022Hard constraints: Non-negotiable requirements that MUST be satisfied\n\u2022Utility objective: What makes one option \u201cbest\u201d once constraints are met\n\u2022Communication style: How the user expresses themselves based on their personality\nCore User Responsibilities\nAnnotators evaluated whether users fulfilled six core responsibilities at each turn:\n1.State what makes a hotel \u201cbest\u201d for them: User clearly explains their optimization goal\n\u2022 Example: \u201cI want the cheapest option that meets all my requirements\u201d\n\u2022 Example: \u201cI\u2019m looking for the highest-rated place that fits my needs\u201d\n2.Reveal must-have requirements: User shares new non-negotiable requirements\n\u2022 Example: \u201cOh, I also need air conditioning - that\u2019s essential for me\u201d\n18\nPreprint\n\u2022 Example: \u201cI forgot to mention, it absolutely must have free parking\u201d\n3.Verify recommendations meet ALL requirements: User checks if suggested hotels satisfy\nevery must-have\n\u2022 Example: \u201cWait, that costs $850 but I said my budget is maximum $800\u201d\n\u2022 Example: \u201cDoes this hotel allow pets? That\u2019s a must-have for me"}
{"file": "2510.07043v1.pdf", "section": "6 CONCLUSION ANDDISCUSSION", "chunk": "requirements: User checks if suggested hotels satisfy\nevery must-have\n\u2022 Example: \u201cWait, that costs $850 but I said my budget is maximum $800\u201d\n\u2022 Example: \u201cDoes this hotel allow pets? That\u2019s a must-have for me\u201d\n4.Answer agent\u2019s clarifying questions: When agent needs more information, user provides\nit based on their script\n5.Restate requirements if needed: Suspicious users may repeat their needs to ensure agent\nunderstood\n\u2022 Example: \u201cJust to be clear, it MUST be pet-friendly AND under $800 total\u201d\n6.Request formal recommendations: If agent doesn\u2019t properly flag their recommendations,\nuser asks them to formally recommend options\nImportant Note for Annotators:Users should not invent information beyond what\u2019s provided in their\nscript.\nError Detection Categories\nAnnotators tagged each response for six types of objective errors (True if error detected, False\notherwise):\nNo. Error Type Description Example\n1 Factual Hallucination User mentions requirements or prefer-\nences not in their script, changing the\ntaskUser adds \u201cI need a gym\u201d when\nscript doesn\u2019t mention this\n2 Revelation Instruction Fol-\nlowing FailureUser was told to reveal a new must-have\nrequirement but didn\u2019tInstruction says \u201creveal need for\nparking\u201d but user doesn\u2019t mention\nit\n3 Revelation Accuracy Fail-\nureUser mentioned a requirement but un-\nclearly, making it seem optional instead\nof must-haveSays \u201cparking would be nice\u201d in-\nstead of \u201cI need parking\u201d (must-\nhave)\n4 Recommendation Check\nFailureUser incorrectly claims a hotel vio-\nlates their requirements when it actually\nmeets themSays \u201cthis is over budget\u201d when\nhotel is actually within budget\n5 Repetition Failure User was told to restate requirements\nbut didn\u2019tInstruction says \u201crepeat your bud-\nget constraint\u201d but user doesn\u2019t\n6 Critical Comments Any other significant errors affecting\nthe taskNote any issues not covered above\nQuality Assessment Rubrics\nClarity (1-5 Scale)\nEvaluation Question:Has the user clearly communicated:\n\u2022 What their must-have requirements are (and that they\u2019re non-negotiable)?\n\u2022 What makes one hotel \u201cbetter\u201d than another for them?\n\u2022 Any problems with the agent\u2019s recommendations?\nEvaluation Goals:\n\u2022User can have different communication styles (casual, formal, etc.) but the core message\nmust be clear\n\u2022 The agent should understand"}
{"file": "2510.07043v1.pdf", "section": "6 Critical Comments Any other significant errors affecting", "chunk": "hotel \u201cbetter\u201d than another for them?\n\u2022 Any problems with the agent\u2019s recommendations?\nEvaluation Goals:\n\u2022User can have different communication styles (casual, formal, etc.) but the core message\nmust be clear\n\u2022 The agent should understand both the requirements AND the optimization goal\n19\nPreprint\nScore Quality Level Description Example\n5 Excellent Crystal clear communication of require-\nments and goals\u201cI need a pet-friendly hotel under $500\ntotal. Among options meeting these\nrequirements, I want the cheapest.\u201d\n4 Good Clear communication with no confu-\nsion about the taskRequirements and goals are clear, mi-\nnor wording issues don\u2019t affect under-\nstanding\n3 Acceptable Mostly clear but some minor ambiguity\nthat won\u2019t affect resultsSays \u201cI prefer under $500\u201d when they\nmean it\u2019s required, but context makes it\nclear\n2 Poor Unclear communication that could lead\nagent to wrong recommendationsMixes up must-haves with nice-to-\nhaves, unclear about optimization goal\n1 Critical So unclear the agent will likely fail the\ntask due to miscommunicationContradictory requirements, no clear\ngoal, or completely confusing instruc-\ntions\nContextual Appropriateness (1-5 Scale)\nEvaluation Question:Does the response logically and naturally follow the conversation so far?\nEvaluation Goals:\n\u2022 User maintains natural conversation flow\n\u2022 User demonstrates awareness of entire conversation history\nScore Quality Level Description\n5 Excellent As good as you would write or better\n4 Good Good without any obvious issues\n3 Acceptable A bit unnatural but overall harmless to the conversation flow\n2 Poor Noticeable issues that do not follow the conversation logic at all\n1 Critical Response is completely disconnected from the conversation logic\n(e.g., User starts acting as an agent)\nAnnotation Workflow\nProcess for Each TurnFor each user response (excluding turn 0), annotators:\n1. Compare the given user response to the instructions given to the user\n2. Answer 6 true/false questions to identify any errors that occurred\n\u2022 AnswerTrueif error or failure is identified\n\u2022 AnswerFalseif no error or if question is not applicable\n3. Answer 2 rubric grading questions to score the quality of the user\u2019s response\n20\nPreprint\nD ADDITIONALBENCHMARKDETAILS\nD.1 TASKSTATISTICS ANDDISTRIBUTION\nOur benchmark consists of 241 tasks spanning diverse travel planning scenarios across 20 U.S.\nNational Parks destination . This section"}
{"file": "2510.07043v1.pdf", "section": "1 Critical Response is completely disconnected from the conversation logic", "chunk": "response\n20\nPreprint\nD ADDITIONALBENCHMARKDETAILS\nD.1 TASKSTATISTICS ANDDISTRIBUTION\nOur benchmark consists of 241 tasks spanning diverse travel planning scenarios across 20 U.S.\nNational Parks destination . This section provides detailed breakdowns of the task characteristics.\nD.1.1 CONSTRAINTDISTRIBUTION\nTasks in our benchmark contain both initial (revealed at the first user query) and progressive constraints\n(revealed during the multi-turn conversation). Fig. 9 shows the distribution of total constraints per\ntask. The majority of tasks have 7-8 constraints. This distribution ensures a balanced mix of task\ncomplexity levels, from simpler scenarios with basic requirements to more complex multi-constraint\nplanning challenges.\n5 6 7 8 9 10\nNumber of Total Constraints01020304050607080Number of Tasks324877\n74\n40\n10Distribution of Total Constraints per Task\nFigure 9: Distribution of total constraints (initial + progressive) across benchmark tasks.\nD.1.2 TASKTYPEI: SINGULARMETRICOPTIMIZATION\nFor tasks using continuous metric optimization, Fig. 10 shows the specific objectives. Price mini-\nmization (price for Level II and total cost for level II and III) dominates continuous tasks, reflecting\nthe common real-world constraint of budget optimization. Additionally, we also have review score\nmaximization, distance minimization and review count maximization continuous tasks.\n0 10 20 30 40 50 60 70 80\nNumber of Tasksminimize total_costminimize pricemaximize hotel_review_scoreminimize distance_to_park_kmmaximize review_scoremaximize review_count\n804010888Distribution of Continuous Metric Utility Objectives\nFigure 10: Distribution of optimization objectives for continuous metric tasks.\nD.1.3 TASKTYPEII: FEATURECOUNTMAXIMIZATION\nFor feature count maximization tasks, Fig. 11 shows the distribution of target attributes to maximize.\nThe most common scenario involves 8 target attributes, followed by 10 attributes. This distribution\ncreates varying levels of optimization complexity, with tasks requiring agents to balance multiple\ncompeting preferences simultaneously.\n21\nPreprint\n5 6 7 8 9 10 11\nNumber of Target Attributes to Maximize010203040Number of Tasks\n872346\n924\n10Distribution of Target Attribute Counts\nin Maximal Attribute Satisfaction Tasks"}
{"file": "2510.07043v1.pdf", "section": "1 Critical Response is completely disconnected from the conversation logic", "chunk": "Preprint\n5 6 7 8 9 10 11\nNumber of Target Attributes to Maximize010203040Number of Tasks\n872346\n924\n10Distribution of Target Attribute Counts\nin Maximal Attribute Satisfaction Tasks\nFigure 11: Distribution of target attribute counts for feature count maximization tasks.\nD.2 AVAILABLETOOLSCHEMA\nAgents have access to four categories of tools for comprehensive travel planning and utility max-\nimization. The tool schema is defined using OpenAI function calling format with JSON schema\nspecifications.\nD.2.1 AIRPORT ANDFLIGHTTOOLS\nsearch airports\nDescription:Find airports serving a specific national park with driving times\nParameters: park name- National park name (e.g., \u2019Yosemite\u2019, \u2019Grand Canyon\u2019)\nReturns:List of airports with airport code, airport name, city, drive hours, airport type\nsearch flights\nDescription:Search for round-trip flights between airports on specific dates\nParameters: origin (3-letter airport code), destination (3-letter airport code),\ndeparture date (YYYY-MM-DD), return date (YYYY-MM-DD), passengers\n(opt.), maxprice (opt.), airline preference (opt.), booking class (opt.),\ndirect flights only(opt.)\nReturns:Flight packages with pricing, schedules, airline info, policies, package id\ngetflight details\nDescription:Get detailed information about a specific flight package\nParameters: package id- Flight package ID (format: flight templateid depday retday)\nReturns:Complete flight details including route, schedule, dates, airline, policies, pricing\ngetairport coordinates\nDescription:Get latitude/longitude coordinates for an airport\nParameters: airport code- 3-letter airport code (e.g., \u2019LAX\u2019, \u2019COD\u2019)\nReturns:Airport coordinates (airport code, name, latitude, longitude)\nD.2.2 PERMITTOOLS\nlist permits\nDescription:List all available park permits and guided tours for a national park\nParameters: park- National park name (e.g., \u2019Yosemite National Park\u2019)\nReturns:Available permit types (day hikes, guided tours, backpacking) with template name\nsearch permit availability\nDescription:Search park permit availability for specific permit template\nParameters: park (park name), template name (exact from list permits), date range\n(opt.),duration days(opt. for backpacking)\nReturns:Available dates with package ids for booking permits\n22\nPreprint\nD.2.3 ACC"}
{"file": "2510.07043v1.pdf", "section": "1 Critical Response is completely disconnected from the conversation logic", "chunk": "Parameters: park (park name), template name (exact from list permits), date range\n(opt.),duration days(opt. for backpacking)\nReturns:Available dates with package ids for booking permits\n22\nPreprint\nD.2.3 ACCOMMODATIONTOOLS\nAccommodationSearch\nDescription:Search for available accommodations by location and date range\nParameters: location ,start date ,enddate ,numguests ,minprice nightly\n(opt.), maxprice nightly (opt.), minreview score (opt.), free cancellation\n(opt.), noprepayment needed (opt.), hasparking (opt.), breakfast included (opt.),\nispetfriendly(opt.)\nReturns:Hotels with group booking packages, including package id, pricing, policies\ngethotel details\nDescription:Get detailed hotel information. Supports fuzzy matching\nParameters: hotel identifier- Hotel ID or name (supports partial names)\nReturns:Hotel details including amenities, location, star rating, review score\ngetroom details\nDescription:Get detailed room information including bed configuration\nParameters: package id- Package ID (format: pkg configid startday endday rooms)\nReturns:Room details including room type, beds, amenities, policies\nsearch location name\nDescription:Search for location names using fuzzy matching\nParameters: query- Location search query (can be partial/informal)\nReturns:Matching locations with confidence score and location type\nD.2.4 UTILITYTOOLS\ngetweekday\nDescription:Returns the weekday name for a given date\nParameters: date str- Date in YYYY-MM-DD format\nReturns:Weekday name (e.g., \u2019Monday\u2019)\ncalculate\nDescription:Evaluate a basic arithmetic expression\nParameters: expression- Arithmetic expression (e.g., \u2019100 + 200 * 3\u2019)\nReturns:Numerical result\ncalculate distance\nDescription:Estimate driving time between two locations using coordinates\nParameters: lat1,lon1(starting point),lat2,lon2(destination)\nReturns:Estimated driving time (e.g., \u20192h 15m\u2019) with distance\nNotebook\nDescription:Agent\u2019s scratch pad for notes and memory\nParameters: action (\u2019write\u2019, \u2019read\u2019, \u2019delete\u2019, \u2019list all\u2019),input data (for \u2019write\u2019), index\n(for \u2019read\u2019/\u2019delete\u2019)\nReturns:Confirmation and content based on action\nD.2.5 VALIDATIONTOOLS\nrecommend hotel\nDescription:MANDATORY validation for hotel package IDs before recommendation\nParameters:"}
{"file": "2510.07043v1.pdf", "section": "1 Critical Response is completely disconnected from the conversation logic", "chunk": "\u2019write\u2019), index\n(for \u2019read\u2019/\u2019delete\u2019)\nReturns:Confirmation and content based on action\nD.2.5 VALIDATIONTOOLS\nrecommend hotel\nDescription:MANDATORY validation for hotel package IDs before recommendation\nParameters: package ids(list of 1-3 IDs),reasoning(explanation)\nReturns:Validation results confirming package ID validity\nrecommend flight\nDescription:MANDATORY validation for flight package IDs before recommendation\nParameters: flight ids(list of 1-3 IDs),reasoning(explanation)\nReturns:Validation results confirming package ID validity\n23\nPreprint\nrecommend permit\nDescription:MANDATORY validation for permit package IDs before recommendation\nParameters: permit ids(list of 1-3 IDs),reasoning(explanation)\nReturns:Validation results confirming package ID validity\nD.3 SAMPLETASKS\nThis section presents two representative tasks from our benchmark to illustrate the complexity\nand diversity of scenarios agents must handle. Each task demonstrates the progressive constraint\nrevelation mechanism and different utility optimization paradigms.\nTask 1: Budget Hiking Trip (Single Metric Optimization) - Level I\nInitial Query:\u201cHi, I\u2019m planning a solo hiking trip to Death Valley National Park and need\naccommodations for 1 person. I\u2019m looking for a 7-day (6-night) stay sometime in August 2025,\nbut I have no preference on the exact dates. My budget is no more than $1400 for the entire\nstay. Can you help me find the cheapest options available that meet these requirements?\u201d\nTask Details:\n\u2022Utility Type:Single metric optimization (minimize price)\n\u2022Duration:7 days (6 nights) in August 2025\n\u2022Location:Death Valley National Park\n\u2022Budget:$1,400 maximum\nInitial Constraints:\n\u2022 National park: Death Valley National Park\n\u2022 Guests: 1 person\n\u2022 Date flexibility: Any 7-day period in August 2025\n\u2022 Total budget:\u2264$1,400\nProgressive Constraints (revealed during conversation):\n\u2022 Distance to park:\u226440 km\n\u2022 Air conditioning: Required\nGround Truth Solution:\n\u2022Hotel:The Ranch At Death Valley\n\u2022Dates:August 24-30, 2025 (6 nights)\n\u2022Total Cost:$1,062.60 ($177.10/night)\n\u2022Utility Score:1.0 (optimal price minimization)\n\u2022Feasible Options:51 packages within constraints"}
{"file": "2510.07043v1.pdf", "section": "1 Critical Response is completely disconnected from the conversation logic", "chunk": "30, 2025 (6 nights)\n\u2022Total Cost:$1,062.60 ($177.10/night)\n\u2022Utility Score:1.0 (optimal price minimization)\n\u2022Feasible Options:51 packages within constraints\nTask 2: Business Luxury Retreat (Feature Count Maximization) - Level I\nInitial Query:\u201cHello, I\u2019m planning a 6-day (5-night) business retreat for one person at\nGrand Teton National Park from August 11th to August 16th, 2025. The company is covering\nup to $5500 for the trip, so we need to stay within that budget. Could you help me find\naccommodations that meet these requirements?\u201d\nTask Details:\n\u2022Utility Type:Feature count maximization\n\u2022Duration:6 days (5 nights), fixed dates\n24\nPreprint\n\u2022Location:Grand Teton National Park\n\u2022Budget:$5,500 maximum\nInitial Constraints:\n\u2022 National park: Grand Teton National Park\n\u2022 Guests: 1 person\n\u2022 Fixed dates: August 11-16, 2025\n\u2022 Total budget:\u2264$5,500\nTarget Attributes to Maximize:\n\u2022 Gym facilities\n\u2022 Spa services\n\u2022 Restaurant on-site\n\u2022 Airport shuttle\n\u2022 Star rating\u22654.0\n\u2022 Review score\u22658.5\n\u2022 Distance to park\u226420 km\n\u2022 In-room fridge\nGround Truth Solution:\n\u2022Hotel:The Lodge at Jackson Hole\n\u2022Dates:August 11-16, 2025 (5 nights)\n\u2022Total Cost:$3,256.15 ($651.23/night)\n\u2022Utility Score:0.75 (6 out of 8 target attributes satisfied)\n\u2022Feasible Options:103 packages within constraints\nTask 3: Airport Drive Time Optimizer (Single Metric Optimization) - Level II\nInitial Query:\u201cHi, I\u2019m looking to plan a trip from JFK to Yosemite National Park for one\nperson. I\u2019m completely flexible on dates as long as the trip happens in August 2025. I need a\n4-day trip (3 nights hotel) including travel time. It\u2019s essential that the hotel is within 1.5 hours\nfrom the airport, and I want to keep the total budget for flights and hotel combined under $1,500.\nCan you help me find the cheapest options meeting these requirements?\u201d\nTask Details:\n\u2022Utility Type:"}
{"file": "2510.07043v1.pdf", "section": "1 Critical Response is completely disconnected from the conversation logic", "chunk": "within 1.5 hours\nfrom the airport, and I want to keep the total budget for flights and hotel combined under $1,500.\nCan you help me find the cheapest options meeting these requirements?\u201d\nTask Details:\n\u2022Utility Type:Single metric optimization (minimize total cost)\n\u2022Duration:4 days (3 nights) in August 2025\n\u2022Location:Yosemite National Park\n\u2022Budget:$1,500 maximum for combined flights and hotel\nInitial Constraints:\n\u2022 Origin city: JFK\n\u2022 National park: Yosemite National Park\n\u2022 Guests: 1 person\n\u2022 Date flexibility: Any 4-day period in August 2025\n\u2022 Total budget:\u2264$1,500 (flights + hotel)\n25\nPreprint\nProgressive Constraints (revealed during conversation):\n\u2022 Hotel to airport drive time:\u22641.5 hours\n\u2022 Hotel star rating:\u22652.5 stars\n\u2022 Flight must have wifi\nGround Truth Solution:\n\u2022Flight:United Airlines JFK\u2192FAT (Fresno)\n\u2022Outbound:August 26, 2025 at 7:00 AM\n\u2022Return:August 29, 2025 at 2:00 PM\n\u2022Flight Cost:$310 (Basic Economy, direct flight with wifi)\n\u2022Hotel:Yosemite View Lodge\n\u2022Dates:August 26-29, 2025 (3 nights)\n\u2022Hotel Cost:$850.50\n\u2022Total Package Cost:$1,160.50\n\u2022Utility Score:1.0 (optimal cost minimization)\n\u2022Feasible Options:1,800 packages within constraints\nTask 4: Senior Comfort Tour Maximizer (Feature Count Maximization) - Level III\nInitial Query:\u201cHello, I\u2019m looking to organize a 4-day trip (3 nights hotel) including travel\ntime for two people from LAX to Arches National Park. We want to make sure we include\na guided tour at Fiery Furnace. Our schedule is flexible on exact dates, but we want to avoid\nweekends in August 2025. Our total budget for flights and hotel combined is $6,000. Could\nyou help us arrange this complete travel package?\u201d\nTask Details:\n\u2022Utility Type:Feature count maximization\n\u2022Duration:4 days (3 nights), weekdays only\n\u2022Location:Arches National Park\n\u2022Budget:$6,000 maximum for flights and hotel\n\u2022Special"}
{"file": "2510.07043v1.pdf", "section": "1 Critical Response is completely disconnected from the conversation logic", "chunk": "travel package?\u201d\nTask Details:\n\u2022Utility Type:Feature count maximization\n\u2022Duration:4 days (3 nights), weekdays only\n\u2022Location:Arches National Park\n\u2022Budget:$6,000 maximum for flights and hotel\n\u2022Special Requirement:Guided tour at Fiery Furnace\nInitial Constraints:\n\u2022 Origin city: LAX\n\u2022 National park: Arches National Park\n\u2022 Guests: 2 people\n\u2022 Date preference: Weekdays only in August 2025\n\u2022 Total budget:\u2264$6,000 (flights + hotel)\n\u2022 Required guided tour: Fiery Furnace\nProgressive Constraints (revealed during conversation):\n\u2022 Outbound departure time:>9:00 AM\n\u2022 Hotel review score:\u22658.0\nTarget Attributes to Maximize:\n\u2022 Hotel has restaurant on-site\n26\nPreprint\n\u2022 Hotel has air conditioning\n\u2022 Hotel star rating\u22653.0\n\u2022 Distance to park\u226410 km\n\u2022 Direct flight preferred\n\u2022 Seat selection included\n\u2022 In-flight wifi available\nGround Truth Solution:\n\u2022Flight:United Airlines LAX\u2192CNY (Moab)\n\u2022Outbound:August 4, 2025 at 4:00 PM\n\u2022Return:August 7, 2025 at 8:00 AM\n\u2022Flight Cost:$560 (Economy, direct flight with wifi)\n\u2022Hotel:Field Station Moab (4-star)\n\u2022Dates:August 4-7, 2025 (3 nights)\n\u2022Hotel Cost:$870.30\n\u2022Permit:Fiery Furnace Guided Tour on August 6, 2025\n\u2022Total Package Cost:$1,445.30\n\u2022Utility Score:0.71 (5 out of 7 attributes satisfied)\n\u2022Feasible Options:19,589 packages within constraints\nD.4 LLM USERSIMULATOR\nBelow we list the dynamic prompt template we used for the LLM user simulator (Sec. 3.4). The blue\nfields are fields are populated at each conversation turn based on user persona and the conversation\nstate.\nCOREINSTRUCTIONS\nRole Definition:\nYou are theuser(not the assistant) responding to a travel assistant\u2019s recommendation.\nYour goal is to find one complete itinerary (flights + hotel + optional permits) that satisfies\nyour hard constraints (non-negotiable) while expressing soft preferences as utility objectives.\nConversation Context:\nYour conversation style, attention"}
{"file": "2510.07043v1.pdf", "section": "1 Critical Response is completely disconnected from the conversation logic", "chunk": "travel assistant\u2019s recommendation.\nYour goal is to find one complete itinerary (flights + hotel + optional permits) that satisfies\nyour hard constraints (non-negotiable) while expressing soft preferences as utility objectives.\nConversation Context:\nYour conversation style, attention level, and persona are specified below and remain consistent\nthroughout.\nYou MUST consistently embody your communication style\u2014your personality should be clearly\nevident in every response.\nDynamic fields update each turn with constraint check results, utility instructions, and behavioral\ntriggers.\nDYNAMICPROMPTFIELDS\n\u2022Conversation Style:{persona description}\n\u2022Attention Level:{attention level}\nHigh\u2014carefully reviews all details;\nMedium\u2014notices key points but may miss minor ones;\nLow\u2014focuses on big picture, often misses specific violations.\n\u2022Hard Constraints (non-negotiable):\n{hard constraints}\n\u2022Utility Objective (soft preference):\n27\nPreprint\n{utility objective}\nNote: DO NOT check or comment on recommendations based on utility (soft prefer-\nences).\n\u2022Hard Constraint Check Results:\n{constraint check natural response}\n\u2022Utility Instruction:\n{utility instruction}\n\u2022Adding Hard Constraint Instruction:\n{constraint instruction}\n\u2022Questioning & Verification Instruction:\n{questioning instruction}\nRESPONSECOMPONENTS\nYour response has five components:\n1. Respond to agent questions if asked\n2. Incorporate hard constraint check results\n3. State utility objective if you haven\u2019t done so in the conversation\n4. Mention additional new hard constraints if instructed\n5. Repeat utility goals/constraints if instructed\nImportant Guidelines:\n\u2022You only mention constraint violations listed in HARD CONSTRAINT CHECK\nRESULTS. DO NOT independently check recommendations against your constraints.\n\u2022Answer agent questions using ONLY information explicitly stated in your constraints\nand utility objective. Ignore questions that cannot be answered from these sources.\nDO NOT hallucinate information.\n\u2022 Follow all additional hard constraint injection instructions exactly.\n\u2022Be authentic to your persona and natural in conversation. Do not repeat information\nalready mentioned unless instructed to be verbose and repetitive.\n\u2022Do not try to be helpful or suggest what the agent should do\u2014expect the agent to do\nthe work.\n\u2022Your goal is to walk away with one itinerary recommendation. Do not ask the agent to\nlist multiple itineraries.\n\u2022Do not ask the agent to lock-in or book anything. Express satisfaction (e.g., \u201dthis looks\npretty good\u201d) or dissatisfaction (when agent violates constraints or doesn\u2019t provide a\nrecommendation), but do not"}
{"file": "2510.07043v1.pdf", "section": "1 Critical Response is completely disconnected from the conversation logic", "chunk": "E.1 AGENTSYSTEMPROMPT\nCOREINSTRUCTIONS\nRole Definition:\nYou are a helpful and proactive travel planning assistant. The current date is June 1st, 2025.\nYour goal is to help the user find the best travel itinerary (flights + hotel + optional permits)\nby satisfying their constraints and maximizing their implicit preferences.\nCritical Requirement:\nYou MUST respond ONLY in valid JSON format using the exact schema below.\nDo NOT include any text outside the JSON structure.\nAGENTWORKFLOW\n1.Engage in conversation:Natural, friendly interaction to understand user needs for\ncomplete travel packages\n2.Use relevant tools:Find available flights, hotels, and permits based on user criteria\n3.Validate recommendations:MUST use recommend itinerary tool to validate\ncomplete package before response\n\u2022 Provide flight package IDs, hotel package IDs, and optional permit IDs\n\u2022 Explain reasoning for the complete itinerary selection\n4.Optional note-taking:Use Notebook tool as scratch pad for complex itinerary\nplanning\nRESPONSEFORMATSPECIFICATION\nWhen making an itinerary recommendation (after validation):\n{\n\"message\": \"I found a great travel package for your Yosemite trip!\nFlight Package:\n- United Airlines: JFK - SFO\n- Outbound: Aug 15 at 10:30 AM\n- Return: Aug 19 at 6:00 PM\n- Cost: $450\nHotel Package:\n- Yosemite View Lodge (3-star)\n- 4 nights: Aug 15-19\n- Cost: $1,200\nTotal Package: $1,650\",\n\"formal_recommendation\": {\n\"flight_package_ids\": [\"flight_pkg_UA123_JFK_SFO_0815\"],\n\"hotel_package_ids\": [\"hotel_pkg_YVL_0815_0819_2\"],\n\"permit_ids\": [],\n\"reasoning\": \"This itinerary offers morning departure as\nrequested,\nstays within budget, and provides convenient access\nto Yosemite National Park.\"\n}\n}\nWhen NOT making a recommendation:\n{\n30\nPreprint\n\"message\": \"You are welcome! Hope you have a wonderful trip!\",\n\"formal_recommendation\": \"None\"\n}\nIMPORTANTREQUIREMENTS\n\u2022Always include both \"message\" and\"formal recommendation\" fields in\nJSON response\n\u2022 User will only see the\"message\"field\u2014include all necessary travel details there\n\u2022Response must be ONLY the JSON object\u2014no text before or after the JSON structure\n\u2022 The entire response must be parseable"}
{"file": "2510.07043v1.pdf", "section": "1 Critical Response is completely disconnected from the conversation logic", "chunk": "fields in\nJSON response\n\u2022 User will only see the\"message\"field\u2014include all necessary travel details there\n\u2022Response must be ONLY the JSON object\u2014no text before or after the JSON structure\n\u2022 The entire response must be parseable as valid JSON\n\u2022For itinerary-level recommendations, always recommend ONE complete itinerary (not\nmultiple options)\nThis prompt design ensures that agents provide consistent, evaluable responses for complete travel\nitineraries while maintaining natural conversational flow. The mandatory JSON format with formal\nrecommendations enables precise evaluation against ground truth solutions for flights, hotels, and\npermits as integrated packages.\nE.2 AGENTLLM CONFIGURATION ANDSAMPLING\nWe employ diverse model configurations to capture a broad spectrum of agent capabilities:\nAgent Model Settings:Different models require specialized configurations. GPT-5 agents use the\ndefault thinking setting (medium level thinking) to leverage their enhanced reasoning capabilities.\nQwen3 models employ sampling temperature 0.7 as recommended by their documentation, with\nthinking mode enabled to improve multi-step reasoning performance. All other agent models use\ntemperature 0 (greedy decoding) to ensure deterministic and reproducible results across evaluation\nruns.\nE.3 AGENTRESPONSEFORMATVALIDATION ANDANSWEREXTRACTION\nPackage IDs serve as unique identifiers that enable deterministic mapping between agent recommen-\ndations and ground truth solutions. Each package ID encodes the complete booking configuration\n(hotel, room type, dates, occupancy, flight number, booking class) ensuring that evaluation is based\non exact matches rather than fuzzy similarity metrics.\nTo ensure precise evaluation alignment with ground truth solutions, agents must respond in a struc-\ntured JSON format containing both a natural language message visible to users and a formal recom-\nmendation section with specific package IDs. This dual-format approach maintains conversational\nnaturalness while enabling exact matching against benchmark solutions.\nWe also provide agent format validation tool, which serves as a mandatory validation checkpoint,\npreventing agents from hallucinating non-existent package IDs. This tool verifies that all recom-\nmended packages exist in the accommodation database before agents include them in their final\nrecommendations, substantially reducing evaluation noise from invalid responses. For hotel recom-\nmendation (Level I), if agent recommends multiple options, we pick the best one for evaluation. For\nitinerary-level recommendation (Level II and III), due to complexity, we ask the agent to always\nrecommend for one valid itinerary."}
{"file": "2510.07043v1.pdf", "section": "1 Critical Response is completely disconnected from the conversation logic", "chunk": "mendation (Level I), if agent recommends multiple options, we pick the best one for evaluation. For\nitinerary-level recommendation (Level II and III), due to complexity, we ask the agent to always\nrecommend for one valid itinerary.\n31"}
{"file": "2510.07423v1.pdf", "section": "Introduction", "chunk": "ProSEA: Problem Solving via Exploration Agents\nWilliam Nguyen\nAitomatic, Inc.\nwilliam@aitomatic.comVinh Luong\nAitomatic, Inc.\nvinh@aitomatic.comChristopher Nguyen\nAitomatic, Inc.\nctn@aitomatic.com\nAbstract\nLarge language models (LLMs) have empow-\nered AI agents to tackle increasingly complex\ntasks. Yet, most existing agents remain lim-\nited to static planning and brittle interactions,\nfalling short of true collaboration or adaptive\nreasoning. We introduceProSEA, a modu-\nlar, general-purpose multi-agent framework de-\nsigned for iterative problem solving throughex-\nploration and plan evolution. ProSEA features\na hierarchical architecture in which a Manager\nAgent orchestrates domain-specialized Expert\nAgents, decomposes tasks, and adaptively re-\nplans based on structured feedback from failed\nattempts. Unlike prior systems, ProSEA agents\nreport not only success or failure but also de-\ntailed reasons for failure and newly discov-\nered constraints-enabling dynamic plan refine-\nment informed by exploratory traces. The\nframework operates autonomously but supports\nseamless integration with human collaborators\nwhen needed. Experiments on the challeng-\ning FinanceBench benchmark demonstrate that\nProSEA, even without human feedback, outper-\nforms state-of-the-art baselines and achieves ro-\nbust performance across reasoning-heavy tasks.\nThese results underscore ProSEA\u2019s potential\nas a foundation for more transparent, adaptive,\nand human-aligned AI agents.\n1 Introduction\nRecent advances in large language models (LLMs)\nhave enabled AI agents to achieve impressive re-\nsults on complex tasks (Schick et al., 2023; Park\net al., 2023; Wu et al., 2023; Li et al., 2023; Chen\net al., 2023). However, current AI agents still face\nsignificant limitations in handling complex, multi-\nfaceted problems that require deep reasoning and\nadaptive problem-solving strategies. One major\nchallenge is the inability of single-agent systems to\neffectively decompose and explore large solution\nspaces. When faced with complex tasks, mono-\nlithic agents often struggle to balance breadth anddepth of exploration, leading to suboptimal solu-\ntions or complete failures (Bansal et al., 2024). Ad-\nditionally, these agents lack robust mechanisms for\nlearning from failed"}
{"file": "2510.07423v1.pdf", "section": "1 Introduction", "chunk": "agents often struggle to balance breadth anddepth of exploration, leading to suboptimal solu-\ntions or complete failures (Bansal et al., 2024). Ad-\nditionally, these agents lack robust mechanisms for\nlearning from failed attempts and adapting their\nstrategies accordingly.\nAnother fundamental limitation is the absence of\nmetacognitive capabilities in current LLM-based\nagents. They have a well-documented tendency to\ngenerate confident but incorrect responses without\nrecognizing their own knowledge limitations (Liu\net al., 2024). This lack of self-awareness prevents\nagents from effectively identifying when to pursue\nalternative approaches or seek additional resources.\nIn complex problem-solving scenarios, the inabil-\nity to assess one\u2019s own reasoning quality and adapt\nstrategies accordingly represents a critical bottle-\nneck (F\u00fcgener et al., 2021). These shortcomings\nhighlight the need for more sophisticated agent\narchitectures that can reason deeply, adapt dynami-\ncally, and leverage multiple perspectives to solve\nchallenging problems. In this work, we propose to\ndevelop ProSEA, a hierarchical multi-agent system\ndesigned for deep reasoning and adaptive problem-\nsolving. ProSEA\u2019s core innovation lies in its ability\nto explore solution spaces through both breadth and\ndepth simultaneously. At a high level, ProSEA\u2019s\narchitecture will be inspired by hierarchical team\nstructures found in effective organizations. It will\nconsist of amanager agentthat oversees and coor-\ndinates multiplespecialized expert agents, enabling\nsophisticated task decomposition and parallel ex-\nploration of solution strategies. The manager agent\nwill decompose complex problems into manage-\nable subtasks, assign them to appropriate expert\nagents, and synthesize their findings into coherent\nsolutions.\nCrucially, ProSEA will incorporate a novel\nfeedback-driven approach where expert agents pro-\nvide rich, structured feedback about their reasoning\nprocesses, including detailed insights about fail-\nures, learnings, and attempted alternatives. This\n1arXiv:2510.07423v1  [cs.AI]  8 Oct 2025\nfeedback mechanism will enable the manager to\nunderstand the problem landscape more deeply\nand adaptively refine its strategy. The design will\nallow ProSEA to tackle problems through two-\ndimensional exploration: the manager explores\nsolution spaces in breadth through task decompo-\nsition and dynamic replanning, while experts"}
{"file": "2510.07423v1.pdf", "section": "1 Introduction", "chunk": "landscape more deeply\nand adaptively refine its strategy. The design will\nallow ProSEA to tackle problems through two-\ndimensional exploration: the manager explores\nsolution spaces in breadth through task decompo-\nsition and dynamic replanning, while experts ex-\nplore in depth through iterative reasoning within\ntheir specialized domains. Because each agent in\nProSEA will be an instance of an LLM, the system\nwill leverage the diverse strengths of large models\nwithout requiring any additional training or fine-\ntuning. ProSEA will be entirely model-agnostic,\nmeaning it can be powered by any suitable LLM\nout-of-the-box and can optionally integrate external\ntools or domain knowledge as needed. Importantly,\nthe architecture will naturally support human feed-\nback within the problem-solving loop without re-\nquiring any structural modifications \u2013 humans can\nprovide input at any stage of the reasoning pro-\ncess, which the agents will incorporate just as they\nwould feedback from other agents. Our proposed\ncontributions are threefold:\n\u2022A model-agnostic hierarchical multi-agent\nframework:We will introduce ProSEA, a\nflexible agent system that requires no task-\nspecific fine-tuning and works with any LLM.\nUnlike many prior systems that demand\nspecialized training or rigid architectures,\nProSEA will leverage prompt engineering to\ncreate a powerful yet practical framework.\nThis approach will enable immediate deploy-\nment with state-of-the-art language models\nwhile maintaining the flexibility to incorpo-\nrate domain-specific tools or knowledge when\nbeneficial. The same architecture will seam-\nlessly accommodate human feedback during\nproblem-solving, treating human input as an-\nother source of expert knowledge without re-\nquiring any architectural changes.\n\u2022Feedback-driven adaptive reasoning:We\nwill develop a novel feedback mechanism\nwhere expert agents provide rich, structured\nfeedback beyond simple success/failure sig-\nnals. When experts encounter obstacles, they\nwill report why they failed, what was learned\nduring the attempt, and what alternative ap-\nproaches were explored. This detailed feed-\nback will enable the manager agent to build a\ncomprehensive understanding of the problem\nlandscape and adapt its planning accordingly.By treating failures as valuable information\nrather than dead ends, ProSEA will navigate\ncomplex solution spaces more effectively than\nagents that rely on binary outcomes.\n\u2022Two-dimensional exploration architecture:\nWe will present a hierarchical architecture\nthat naturally enables simultaneousbreadth\nanddepthexploration. The manager agent\nwill explore in breadth by decom"}
{"file": "2510.07423v1.pdf", "section": "1 Introduction", "chunk": "complex solution spaces more effectively than\nagents that rely on binary outcomes.\n\u2022Two-dimensional exploration architecture:\nWe will present a hierarchical architecture\nthat naturally enables simultaneousbreadth\nanddepthexploration. The manager agent\nwill explore in breadth by decomposing tasks\nand dynamically replanning based on feed-\nback, while expert agents will explore in depth\nthrough iterative reasoning within their spe-\ncialized domains. This division will create\na robust and modular system where global\nstrategy and local expertise complement each\nother, enabling ProSEA to tackle problems\nthat are beyond the reach of single-agent sys-\ntems.\nIn summary, ProSEA represents a significant ad-\nvance in multi-agent systems for complex problem-\nsolving. By combining hierarchical task decom-\nposition with rich feedback mechanisms and two-\ndimensional exploration, ProSEA will demonstrate\nsuperior performance on challenging tasks that re-\nquire deep reasoning and adaptive strategies. The\nfollowing sections will detail the proposed ProSEA\narchitecture, implementation plan, and evaluation\nmethodology, showing how this approach will out-\nperform existing solutions on complex reasoning\nbenchmarks. These contributions aim to advance\nthe field of AI agents toward more capable and\nadaptive problem-solving systems.\n2 Related Works\n2.1 Limitations of Current AI Agents\nDespite their remarkable capabilities, LLM-based\nagents face well-known issues that limit their re-\nliability and trustworthiness in real-world applica-\ntions. A core challenge is hallucination-the genera-\ntion of plausible but incorrect information-which\nundermines user trust (Huang et al., 2023). Com-\npounding this issue, LLMs often exhibit overcon-\nfidence and fail to signal uncertainty, even when\nproducing erroneous answers (Griot et al., 2025;\nChhikara, 2025). This miscalibration is particu-\nlarly problematic in domains where correctness is\ncritical.\nAnother barrier is the opacity of reasoning. Most\nLLM-based assistants do not reveal their decision\nprocess, leaving users unable to evaluate whether\n2\nconclusions are well-founded (Liao and Vaughan,\n2023). This lack of transparency impairs user\ntrust and the ability to calibrate responses appropri-\nately (Senoner et al., 2024). Furthermore, current\nagents lack metacognitive insight-they do not know\nwhat they know or when they may be wrong (Griot\net al.,"}
{"file": "2510.07423v1.pdf", "section": "2.1 Limitations of Current AI Agents", "chunk": "ability to calibrate responses appropri-\nately (Senoner et al., 2024). Furthermore, current\nagents lack metacognitive insight-they do not know\nwhat they know or when they may be wrong (Griot\net al., 2025). Research in high-stakes domains,\nsuch as healthcare, has emphasized the need for\nself-assessment mechanisms to help AI systems\ndefer or ask for assistance when uncertain.\nSeveral mitigation strategies have emerged. For\nexample, prompting methods such as chain-of-\nthought or ReAct (Yao et al., 2022) enable more in-\nterpretable and grounded reasoning by encouraging\nintermediate steps and self-verification. However,\nthese strategies still operate within a single-agent,\none-shot paradigm and are limited in their ability\nto support dynamic replanning or collaboration.\n2.2 Human-AI Collaboration and Interaction\nParadigms\nA growing body of research has shifted from view-\ning AI as a passive tool toward designing agents\nthat function as active collaborators (Hemmer et al.,\n2024). In traditional models, humans instruct the\nAI directly, and the AI executes without initiative-\ncommon in search engines or assistants. More\ninteractive systems act as suggestion engines or\npassive copilots, where humans retain full control\nand responsibility.\nEmerging collaborative paradigms aim for\nshared responsibility. In these systems, agents\nproactively contribute ideas, identify inconsisten-\ncies, and engage in dialogue-mirroring human\nteammate behaviors (Hemmer et al., 2024). This\nmixed-initiative setup allows for complementary\nperformance, where both human and AI contribute\nto the solution. Studies in fields such as medicine,\ndesign, and law confirm that such assistive partner-\nships can improve outcomes if agents are transpar-\nent and adaptive.\nA key design trade-off lies in balancing auton-\nomy and human control. Fully autonomous agents\ncan increase efficiency but risk cascading errors,\nwhile human-in-the-loop designs improve safety\nat the cost of speed. ProSEA adopts a hybrid ap-\nproach: it operates autonomously by default but is\ndesigned to seamlessly integrate human feedback\nduring exploration when needed. This positions\nProSEA to support real-world collaborative use\ncases.2.3 Iterative Planning and Multi-Agent\nArchitectures\nRecent agent systems have begun to explore\nmulti-agent designs and iterative planning to en-\nhance"}
{"file": "2510.07423v1.pdf", "section": "2.2 Human-AI Collaboration and Interaction", "chunk": "when needed. This positions\nProSEA to support real-world collaborative use\ncases.2.3 Iterative Planning and Multi-Agent\nArchitectures\nRecent agent systems have begun to explore\nmulti-agent designs and iterative planning to en-\nhance scalability and robustness. Architectures\nlike MetaGPT (Hong et al., 2023) and Chain-of-\nAgents (Zhang et al., 2024) use a manager-agent\nparadigm, where a central controller delegates sub-\ntasks to specialized agents. These systems improve\nmodularity and reasoning depth but often rely on\npredefined scripts or fragile coordination proto-\ncols. Other efforts like Reflexion (Shinn et al.,\n2023) and AutoGPT (Yang et al., 2023) focus on\nmemory-driven self-correction. Agents reflect on\nfailures and retry with modified prompts, but these\nfeedback loops are coarse-grained and do not in-\nclude structured failure reporting. Similarly, V oy-\nager (Wang et al., 2023) in Minecraft uses skill\nlibraries and episodic memory for long-term explo-\nration, but lacks domain generality and reasoning\ntransparency.\nProSEA builds on these ideas but introduces\nthree key advances: (1) a structured feedback loop\nin which expert agents reportwhyfailures occur\n(e.g., unmet constraints, invalid assumptions); (2)\nadaptive plan evolution at the manager level based\non this rich feedback; and (3) a domain-agnostic\narchitecture that supports optional integration of\nexternal tools and human guidance, without requir-\ning retraining. This enables ProSEA to support\ndynamic, explainable reasoning across diverse do-\nmains while remaining deployable in real-world\napplications.\n3 Interactive and Iterative Problem\nSolving Agents\nWe present ProSEA, a novel multi-agent frame-\nwork designed to address complex problem-solving\ntasks through iterative exploration and adaptive\nrefinement. Our approach fundamentally differs\nfrom traditional linear problem-solving pipelines\nby incorporating a dynamic feedback mechanism\nthat enables agents to identify infeasible solution\npaths and collaboratively navigate around obstacles\nthrough systematic re-planning.\n3.1 Overall Architecture\nThe ProSEA framework employs a hierarchical\nmulti-agent architecture comprising four principal\ncomponents: a Manager Agent, a Problem Ana-\nlyzer, a Planner, and multiple domain-specific Ex-\n3\nManagerAgentProblemAnalyzerPlannerExpert Agents(Multi-steps)External"}
{"file": "2510.07423v1.pdf", "section": "3.1 Overall Architecture", "chunk": "framework employs a hierarchical\nmulti-agent architecture comprising four principal\ncomponents: a Manager Agent, a Problem Ana-\nlyzer, a Planner, and multiple domain-specific Ex-\n3\nManagerAgentProblemAnalyzerPlannerExpert Agents(Multi-steps)External Tools\nHuman in the loopQuestionFinal AnswerRe-planningInternal reasoning\nStep\u2019s feedbackDomain KnowledgeInternal moduleAgentExternal moduleFigure 1:ProSEA\u2019s pipeline:The Manager Agent receives the initial question and forwards it to the Problem\nAnalyzer for preliminary analysis. The analyzed problem is then passed to the Planner, which generates a multi-step\nsolution plan. Each step is assigned to a domain-specific Expert Agent responsible for execution through reasoning,\ntool usage, and user interaction. Results from each step are sent back to the Manager Agent, which evaluates the\nneed for plan adjustment. Upon successful completion of all steps, the Manager Agent synthesizes the final answer\nbased on the step outputs.\npert Agents. These components are organized in\na pipeline structure with bidirectional communi-\ncation channels that facilitate both forward execu-\ntion and feedback-driven adaptation. The Manager\nAgent serves as the central orchestrator, receiving\nuser queries and coordinating the flow of informa-\ntion throughout the system. Upon receiving a ques-\ntion, the Manager Agent forwards it to the Prob-\nlem Analyzer, which performs preliminary anal-\nysis to extract key constraints, requirements, and\nimplicit assumptions embedded within the query.\nThis analysis transforms natural language questions\ninto structured problem representations suitable for\nsystematic planning.\nThe analyzed problem representation is subse-\nquently passed to the Planner module, which gen-\nerates comprehensive multi-step solution strategies.\nThe Planner decomposes complex problems into\ndiscrete, executable steps and assigns each step\nto appropriate Expert Agents based on domain re-\nquirements. This decomposition process considers\nboth the sequential dependencies between steps and\nthe specialized capabilities required for each sub-\ntask. Importantly, our planning mechanism main-\ntains sufficient flexibility to accommodate dynamic\nmodifications based on execution feedback, a criti-\ncal feature that distinguishes ProSEA from conven-\ntional static planning approaches.\n3.2 Expert Agents and the Exploration\nExpert Agents constitute the primary exploration\nmechanism within our framework and embody the\ncore innovation of ProSEA\u2019s approach. Each Ex-\npert Agent possesses specialized domain knowl-\nedge and maintains access to external tools and\nknowledge bases relevant to its area of expertise.These agents are capable of performing complex"}
{"file": "2510.07423v1.pdf", "section": "3.2 Expert Agents and the Exploration", "chunk": "our framework and embody the\ncore innovation of ProSEA\u2019s approach. Each Ex-\npert Agent possesses specialized domain knowl-\nedge and maintains access to external tools and\nknowledge bases relevant to its area of expertise.These agents are capable of performing complex,\nmulti-step reasoning processes and can operate in\nboth autonomous and collaborative modes. In col-\nlaborative mode, Expert Agents can seamlessly in-\ntegrate human expertise by requesting assistance\nwhen encountering particularly challenging deci-\nsions or when validation is required.\nThe exploration process implemented by Expert\nAgents operates through a goal-directed internal\nreasoning loop. Each step assigned to an Expert\nAgent consists of two critical components: a spe-\ncific task to be performed and a well-defined goal\nthat must be achieved. The Manager Agent es-\ntablishes these goals based on the overall prob-\nlem requirements, providing clear success criteria\nfor each exploration step. Expert Agents then en-\ngage in iterative exploration, attempting various\napproaches through reasoning and tool usage until\nthey either achieve the specified goal or determine\nthat it cannot be reached through the current path.\nDuring this exploration process, Expert Agents\ncontinuously evaluate their progress toward the\ndefined goal while simultaneously discovering\nnew information about the problem space. This\ndual nature of exploration-both goal-seeking and\nknowledge-discovering-represents a key strength\nof our approach. When an Expert Agent success-\nfully achieves its goal, it reports not only the so-\nlution but also any new insights, patterns, or con-\nstraints discovered during exploration. Similarly,\nwhen a goal proves unachievable, the agent pro-\nvides detailed feedback about why the goal cannot\nbe reached, what alternative approaches were at-\ntempted, and what new understanding was gained\nabout the problem structure. This rich feedback,\nencompassing both goal achievement status and\n4\nDifficulty Level #Qs LlamaIndex RAG LangChain ReAct OpenAI Assistant OpenSSA DANA-NK-NP Autonomous ProSEA\n0-RETRIEVE 56 71% 85% 49% 95%98%\n1-COMPARE 23 83% 90% 46% 90%100%\n2-CALC-CHANGE 9 78% 69% 36% 93%100%\n3-CALC-COMPLEX 43 31% 88% 40%100%95%\n4-CAL"}
{"file": "2510.07423v1.pdf", "section": "3.2 Expert Agents and the Exploration", "chunk": "%100%\n2-CALC-CHANGE 9 78% 69% 36% 93%100%\n3-CALC-COMPLEX 43 31% 88% 40%100%95%\n4-CALC-AND-JUDGE 10 14% 60% 14%94%70%\n5-EXPLAIN-FACTORS 2 70% 70% 50%100% 100%\n6-OTHER-ADV ANCED 7 43% 37% 46%89%43%\nAccuracy150 56.7% 81.6% 42.7%95.3%93.2%\nTable 1:Performance Comparison Across Difficulty Levels.Autonomous ProSEA (without human collaboration\nor domain knowledge) achieved state-of-the-art performance in tasks 0-RETRIEVE, 1-COMPARE, 2-CALC-\nCHANGE, and 5-EXPLAIN-FACTORS, and performed comparably to DANA in 3-CALC-COMPLEX. However,\nperformance on 6-OTHER-ADV ANCED was lower, as this set is relatively small and requires domain knowledge\nbeyond what is available in LLMs or provided documents.\nnewly acquired knowledge, enables the Manager\nAgent to make informed decisions about subse-\nquent exploration steps and to refine the overall\nsolution strategy based on accumulated discover-\nies.\n3.3 Iterative Exploration Through Adaptive\nPlanning\nThe iterative nature of ProSEA\u2019s exploration man-\nifests through a continuous cycle of hypothesis\ngeneration, testing, and refinement. When Expert\nAgents discover that certain solution paths are in-\nfeasible, this information becomes part of the ex-\nploration state that guides future search directions.\nThe Manager Agent processes exploration feed-\nback to maintain a comprehensive understanding\nof the solution space, including both promising\ndirections and identified dead-ends. This accumu-\nlated knowledge triggers adaptive planning, where\nthe Planner generates new hypotheses that account\nfor discoveries made during previous exploration\niterations.\nThis exploration process, which inherently in-\ncludes re-planning as a natural component, con-\ntinues until either a complete solution is found or\nthe system determines that the problem space has\nbeen sufficiently explored without finding a fea-\nsible solution. The exploration-based approach\nprovides several critical advantages. First, it treats\nproblem-solving as a search process where initial"}
{"file": "2510.07423v1.pdf", "section": "3.3 Iterative Exploration Through Adaptive", "chunk": "is found or\nthe system determines that the problem space has\nbeen sufficiently explored without finding a fea-\nsible solution. The exploration-based approach\nprovides several critical advantages. First, it treats\nproblem-solving as a search process where initial\nstrategies are refined based on empirical discover-\nies rather than predetermined paths. Second, it cre-\nates a transparent exploration trace that documents\nthe reasoning process, including both successful\npaths and informative failures. Third, by detectinginfeasible directions early in the exploration, the\nsystem efficiently prunes the search space and fo-\ncuses computational resources on more promising\nareas.\n3.4 Human-in-the-Loop Exploration\nProSEA incorporates human collaboration as an\nintegral part of the exploration process. The frame-\nwork supports bidirectional human-agent interac-\ntion where Expert Agents can proactively seek hu-\nman assistance during exploration, and humans\ncan provide guidance or suggest alternative ex-\nploration directions. This collaborative capability\nis particularly valuable in domains where human\nintuition can identify promising paths or quickly\nrecognize dead-ends that might require extensive\ncomputation to discover automatically. The human-\nin-the-loop mechanism is seamlessly woven into\nthe exploration process, allowing Expert Agents\nto treat human input as valuable exploration guid-\nance alongside computational tools and knowledge\nbases.\n4 Experiments\nTo evaluate the effectiveness of ProSEA, we con-\nducted comprehensive experiments using the Fi-\nnanceBench dataset (Islam et al., 2023) and com-\npared our approach against several state-of-the-\nart baseline systems including LlamaIndex RAG\nagents, LangChain ReAct agents (Yao et al., 2022),\nOpenAI Assistants, and DANA (Luong et al.,\n2024).\nFinanceBench comprises 150 carefully curated\nquestions requiring comprehension of complex\nfinancial documents including 10-Ks, 10-Qs, 8-\n5\nKs, and earnings reports. Questions range from\nstraightforward information extraction to complex\nmulti-step calculations across financial statements,\nmaking it ideal for evaluating ProSEA\u2019s exploration\nmechanism when initial solution attempts fail. Fol-\nlowing DANA\u2019s evaluation protocol (Luong et al.,\n2024), we employ strict accuracy metrics where re-\nsponses must match gold standard answers within\nacceptable tolerances for numerical values and se-\nmantic equivalence for text. We account for minor\nvariations in units and rounding while"}
{"file": "2510.07423v1.pdf", "section": "4 Experiments", "chunk": "et al.,\n2024), we employ strict accuracy metrics where re-\nsponses must match gold standard answers within\nacceptable tolerances for numerical values and se-\nmantic equivalence for text. We account for minor\nvariations in units and rounding while maintain-\ning stringent requirements for logical and factual\ncorrectness.\nTo ensure fair comparison with existing methods,\nwe evaluate ProSEA in fully autonomous mode,\ndisabling its human collaboration capabilities. This\nallows direct comparison with baseline systems that\noperate without human intervention. Our results\ndemonstrate that autonomous ProSEA significantly\noutperforms traditional approaches across all diffi-\nculty levels. Table X shows that ProSEA achieves\nsubstantial improvements over LlamaIndex RAG\nagents, LangChain ReAct agents, and OpenAI As-\nsistants by a large margin. Notably, ProSEA es-\ntablishes new state-of-the-art performance on sev-\neral challenging question categories: 0-RETRIEVE\n(information retrieval), 1-COMPARE (compara-\ntive analysis), 2-CAL-CHANGE (change calcula-\ntions), and 5-EXPLAIN-FACTORS (factor expla-\nnation). These categories particularly benefit from\nProSEA\u2019s iterative exploration mechanism, which\ncan adaptively refine its approach when initial re-\ntrieval or reasoning attempts prove insufficient.\nWhen compared with DANA, ProSEA achieves\ncomparable performance on 3-CALC-COMPLEX\n(complex calculations) while showing lower per-\nformance on 4-CALC-AND-JUDGE (calculation\nwith judgment) and 6-OTHER-ADV ANCED (other\nadvanced tasks). However, this comparison reveals\na crucial distinction: DANA requires extensive\ndomain knowledge engineering and manual plan\nguidance from human experts, whereas ProSEA\noperates entirely autonomously. This fundamental\ndifference demonstrates ProSEA\u2019s superior scala-\nbility and practical applicability, as it eliminates\nthe need for labor-intensive human guidance while\nstill achieving competitive performance.\nThe strong autonomous performance of ProSEA\nvalidates our exploration-based approach, showing\nthat systematic exploration with adaptive refine-\nment can effectively substitute for manual human\nplanning. Furthermore, while our experiments fo-cus on autonomous operation for fair comparison,\nProSEA\u2019s architecture naturally supports human-in-\nthe-loop collaboration. When human feedback is\nincorporated, we expect performance to improve\nsignificantly,"}
{"file": "2510.07423v1.pdf", "section": "4 Experiments", "chunk": "planning. Furthermore, while our experiments fo-cus on autonomous operation for fair comparison,\nProSEA\u2019s architecture naturally supports human-in-\nthe-loop collaboration. When human feedback is\nincorporated, we expect performance to improve\nsignificantly, making ProSEA particularly well-\nsuited for real-world deployment where human ex-\npertise can complement automated reasoning.\n5 Conclusion\nWe presented ProSEA, a multi-agent framework\nthat achieves effective problem-solving through\niterative exploration without requiring human guid-\nance or domain-specific training. Our experiments\non FinanceBench demonstrate that ProSEA sig-\nnificantly outperforms traditional RAG and Re-\nAct systems, achieving performance compara-\nble to DANA-which requires substantial human\nintervention-while operating fully autonomously.\nProSEA\u2019s key innovation lies in treating problem-\nsolving as an exploration process where Expert\nAgents adaptively refine their approaches based on\ndiscovered constraints and failures. This enables\nthe system to navigate around obstacles and find\nviable solutions without predetermined paths. The\nstrong autonomous performance validates that sys-\ntematic exploration with adaptive refinement can\neffectively substitute for manual human planning.\nWhile our evaluation focused on autonomous op-\neration, ProSEA\u2019s architecture naturally supports\nhuman-in-the-loop collaboration, suggesting even\ngreater potential for real-world deployment. Future\nwork could explore enhanced human interaction\nmodes and applications to other complex domains.\nProSEA represents an important step toward AI\nagents that can serve as true collaborative part-\nners, combining deep reasoning capabilities with\nthe flexibility to work effectively alongside human\nexperts.\nAcknowledgments\nReferences\nGagan Bansal, Jennifer Wortman Vaughan, Saleema\nAmershi, Eric Horvitz, Adam Fourney, Hussein\nMozannar, Victor Dibia, and Daniel S. Weld. 2024.\nChallenges in human-agent communication.ArXiv,\nabs/2412.10380.\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang,\nChenfei Yuan, Cheng Qian, Chi-Min Chan, Yujia Qin,\nYa-Ting Lu, Ruobing Xie, Zhiyuan Liu, Maosong\nSun, and Jie Zhou. 2023. Agentverse: Facilitating\nmulti-agent collaboration and exploring emergent\nbehaviors in agents.ArXiv, abs/2308.10848.\n6\nPrateek Chhik"}
{"file": "2510.07423v1.pdf", "section": "5 Conclusion", "chunk": "Sun, and Jie Zhou. 2023. Agentverse: Facilitating\nmulti-agent collaboration and exploring emergent\nbehaviors in agents.ArXiv, abs/2308.10848.\n6\nPrateek Chhikara. 2025. Mind the confidence gap:\nOverconfidence, calibration, and distractor effects\nin large language models.ArXiv, abs/2502.11028.\nAndreas F\u00fcgener, J\u00f6rn Grahl, Alok Gupta, and Wolf-\ngang Ketter. 2021. Cognitive challenges in human-\nartificial intelligence collaboration: Investigating the\npath toward productive delegation.Inf. Syst. Res.,\n33:678\u2013696.\nMaxime Griot, Coralie Hemptinne, Jean Vanderdonckt,\nand Demet Yuksel. 2025. Large language models\nlack essential metacognition for reliable medical rea-\nsoning.Nature Communications, 16.\nPatrick Hemmer, Max Schemmer, Niklas Kuhl, Michael\nV ossing, and Gerhard Satzger. 2024. Complementar-\nity in human-ai collaboration: Concept, sources, and\nevidence.ArXiv, abs/2404.00029.\nSirui Hong, Xiawu Zheng, Jonathan P. Chen, Yuheng\nCheng, Ceyao Zhang, Zili Wang, Steven Ka Shing\nYau, Zi Hen Lin, Liyang Zhou, Chenyu Ran,\nLingfeng Xiao, and Chenglin Wu. 2023. Metagpt:\nMeta programming for multi-agent collaborative\nframework.ArXiv, abs/2308.00352.\nLei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong,\nZhangyin Feng, Haotian Wang, Qianglong Chen,\nWeihua Peng, Xiaocheng Feng, Bing Qin, and Ting\nLiu. 2023. A survey on hallucination in large lan-\nguage models: Principles, taxonomy, challenges, and\nopen questions.ACM Transactions on Information\nSystems, 43:1 \u2013 55.\nPranab Islam, Anand Kannappan, Douwe Kiela, Re-\nbecca Qian, Nino Scherrer, and Bertie Vidgen. 2023.\nFinancebench: A new benchmark for financial ques-\ntion answering.ArXiv, abs"}
{"file": "2510.07423v1.pdf", "section": "5 Conclusion", "chunk": ", Anand Kannappan, Douwe Kiela, Re-\nbecca Qian, Nino Scherrer, and Bertie Vidgen. 2023.\nFinancebench: A new benchmark for financial ques-\ntion answering.ArXiv, abs/2311.11944.\nG. Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin,\nand Bernard Ghanem. 2023. Camel: Communicative\nagents for \"mind\" exploration of large scale language\nmodel society.ArXiv, abs/2303.17760.\nQingzi Vera Liao and Jennifer Wortman Vaughan. 2023.\nAi transparency in the age of llms: A human-centered\nresearch roadmap.ArXiv, abs/2306.01941.\nZhendong Liu, Changhong Xia, Wei He, and Chong-\nJun Wang. 2024. Trustworthiness and self-awareness\nin large language models: An exploration through\nthe think-solve-verify framework. InInternational\nConference on Language Resources and Evaluation.\nVinh Luong, Sang Dinh, Shruti Raghavan, Thanh\nNguyen, Zooey Nguyen, Quynh Le, Hung V o, Ken-\ntaro Maegaito, Loc Nguyen, Thao Nguyen, Anh Hai\nHa, and Christopher Nguyen. 2024. Dana: Domain-\naware neurosymbolic agents for consistency and ac-\ncuracy.ArXiv, abs/2410.02823.\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai,\nMeredith Ringel Morris, Percy Liang, and Michael S.\nBernstein. 2023. Generative agents: Interactive sim-\nulacra of human behavior.Proceedings of the 36thAnnual ACM Symposium on User Interface Software\nand Technology.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\nArXiv, abs/2302.04761.\nJulian Senoner, Simon Schallmoser, Bernhard\nKratzwald, Stefan Feuerriegel, and Torbj\u00f8rn Netland.\n2024. Explainable ai improves task performance in"}
{"file": "2510.07423v1.pdf", "section": "5 Conclusion", "chunk": "/2302.04761.\nJulian Senoner, Simon Schallmoser, Bernhard\nKratzwald, Stefan Feuerriegel, and Torbj\u00f8rn Netland.\n2024. Explainable ai improves task performance in\nhuman\u2013ai collaboration.Scientific Reports, 14.\nNoah Shinn, Federico Cassano, Beck Labash, Ashwin\nGopinath, Karthik Narasimhan, and Shunyu Yao.\n2023. Reflexion: language agents with verbal re-\ninforcement learning. InNeural Information Pro-\ncessing Systems.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi (Jim) Fan,\nand Anima Anandkumar. 2023. V oyager: An open-\nended embodied agent with large language models.\nTrans. Mach. Learn. Res., 2024.\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,\nBeibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,\nShaokun Zhang, Jiale Liu, Ahmed Hassan Awadal-\nlah, Ryen W. White, Doug Burger, and Chi Wang.\n2023. Autogen: Enabling next-gen llm applications\nvia multi-agent conversation.\nHui Yang, Sifu Yue, and Yunzhong He. 2023. Auto-\ngpt for online decision making: Benchmarks and\nadditional opinions.ArXiv, abs/2306.02224.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels.ArXiv, abs/2210.03629.\nYusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister,\nRui Zhang, and Sercan \u00d6. Arik. 2024. Chain of\nagents: Large language models collaborating on long-\ncontext tasks.ArXiv, abs/2406.02818.\n7"}
{"file": "2510.07423v1.pdf", "section": "5 Conclusion", "chunk": "abs/2406.02818.\n7"}
{"file": "2510.07593v1.pdf", "section": "Introduction", "chunk": "AGENTASK: Multi-Agent Systems Need to Ask\nBohan Lin\u22c6, Kuo Yang\u22c6, Yingchuan Lai\u2663, Yudong Zhang\u22c6,, Chen Zhang\u22c6,\nGuibin Zhang\u2660,Xinlei Yu\u2660,Miao Yu\u22c6,Xu Wang\u22c6, andYang Wang\u22c6,\n\u22c6University of Science and Technology of China\n\u2663Xi\u2019an Jiaotong University\u2660National University of Singapore\nContact:linbohan@stu.xjtu.edu.cn,zyd2020@mail.ustc.edu.cn\nCorresponding authors\nAbstract\nMulti-agent systems built on large language\nmodels (LLMs) promise enhanced problem-\nsolving capabilities through collaborative di-\nvision of labor. However, they frequently un-\nderperform single-agent baselines due toedge-\nlevelerror cascades: minor inaccuracies at one\nmessage handoff propagate across the entire\nchain. We proposeAgentAsk, a lightweight\nand plug-and-play clarification module that\ntreats every inter-agent message as a potential\nfailure point and insertsminimally necessary\nquestions to arrest error propagation. Agen-\ntAsk follows a three-stage pipeline: (i) distill-\ning edge-level judgments from curated failure\ntraces into a compact policy, (ii) supervising the\npolicy to determinewhen/what/whom/howto\nask, and (iii) optimizing online withE-GRPO,\na reinforcement learning objective that balances\naccuracy, latency, and cost. The module is\narchitecture-agnostic and easy to integrate into\nexisting orchestration. Across math, reason-\ning, and coding benchmarks, AgentAsk consis-\ntently improves accuracy and robustness over\npublic multi-agent implementations while keep-\ning overhead minimal, with latency and extra\ncost all less than 5%, approaching the perfor-\nmance of a strong evaluator. Beyond empiri-\ncal improvements, we contribute a principled\ntaxonomy of edge-level errors and a practical\nrecipe for link-local intervention, offering a\nscalable pathway toward more reliable LLM-\nbased multi-agent systems. The code is avail-\nable athttps://github.com/BoHan-LIN04/\nAgentAsk.\n1 Introduction\nLarge language model (LLM)\u2013based agent systems\nhave attracted increasing attention for their ability\nto combine multi-step reasoning with dynamic tool"}
{"file": "2510.07593v1.pdf", "section": "Introduction", "chunk": "avail-\nable athttps://github.com/BoHan-LIN04/\nAgentAsk.\n1 Introduction\nLarge language model (LLM)\u2013based agent systems\nhave attracted increasing attention for their ability\nto combine multi-step reasoning with dynamic tool\nuse (Wei et al., 2022a; Schick et al., 2023). By\ncoordinating multiple LLM-driven agents, these\nsystems aim to leverage collective intelligence to\nsolve complex, real-world tasks that necessitate de-\nFigure 1:Multi-Agent System Reasoning: Success vs.\nCascade Failure.The left shows the normal operation\nof MAS, while the right shows that the upstream agents\ntransmit errors during the interaction that trigger a cas-\ncade effect and cause the entire system to fail.\ncomposition, external knowledge access, and itera-\ntive refinement(Park et al., 2023). This paradigm is\nreferred to as a multi-agent system (MAS). Appli-\ncations now span diverse domains such as software\nengineering (Hong et al., 2024; Qian et al., 2024)\nand AI for Science (Ghafarollahi and Buehler,\n2025; M. Bran et al., 2024), and there is growing\ninterest in developing general-purpose multi-agent\nframeworks for cross-domain problem solving (Wu\net al., 2024a; Liu et al., 2024; Yu et al., 2025a).\nThese advances underscore both the promise of\nmulti-agent collaboration and the difficulty of mak-\ning MAS reliable in practice.\nDespite the promise of collaborative decomposi-\ntion, multi-agent systems often fail to reliably sur-\npass strong single-agent baselines across diverse\ntasks. Previous research and benchmarks report\nunstable gains and brittle behavior under realis-\ntic orchestration (Liu et al., 2024; Cemri et al.,\n2025). Execution logs reveal a recurring pattern\nin which small inconsistencies or missing details\nintroduced at one step travel along the chain and\naccumulate into system-level failures. Building on\nthis pattern, chainwise amplification is intensified\nby specification gaps and inter-agent misalignment,\nand in some settings, collaboration reduces accu-\nracy (Zhang et al., 2025d; Wynn et al., 2025; WangarXiv:2510.07593v1  [cs.AI"}
{"file": "2510.07593v1.pdf", "section": "1 Introduction", "chunk": "alignment,\nand in some settings, collaboration reduces accu-\nracy (Zhang et al., 2025d; Wynn et al., 2025; WangarXiv:2510.07593v1  [cs.AI]  8 Oct 2025\net al., 2025). Prior studies quantify the effect, with\nsuccess on difficult programming tasks falling to\nabout 25% (Cemri et al., 2025). Similar vulnerabil-\nities appear in single-agent traces, including misin-\nterpretations, logical gaps, and limited reflection,\nwhich indicates that subtle errors arise early and\npersist through execution. This sets up the problem\nwe study: limiting error growth at the handoff be-\ntween agents so that small inconsistencies do not\naccumulate into system-level failures.\nA growing body of recent research attempts\nto improve the reliability of MAS. One direction\nemphasizes structured roles and workflow gover-\nnance, enforcing explicit hierarchies and proto-\ncols to reduce miscommunication (Hong et al.,\n2024; Qian et al., 2024; Chen et al., 2024b; Yu\net al., 2025b). Another line refines orchestration\nand prompting strategies, leveraging prompt engi-\nneering and search to better allocate tasks (Zhuge\net al., 2024; Zhang et al., 2025b,a; Yue et al., 2025).\nConcurrently, self-checking and feedback mecha-\nnisms encourage agents to reflect on or repair their\nown outputs (Liang et al., 2024; Shinn et al., 2023;\nMadaan et al., 2023). These approaches, spanning\narchitecture, algorithm, and introspection, provide\nuseful gains but remain limited. Many proposed\nsolutions are domain-specific, costly to scale, or\nincrease complexity without offering general prin-\nciples. Moreover, existing taxonomies of failure\nare largely descriptive and rarely yield prescriptive\nmechanisms for prevention or correction. Conse-\nquently, current systems remain fragile when con-\nfronted with complex or open-ended tasks.\nWe argue that ensuring reliability necessitates a\nparadigm shift towardedge-levelintervention: each\ninter-agent message should be treated as a poten-\ntial failure point where minimal clarification can\nhalt cascading errors, illustrated in Figure 1."}
{"file": "2510.07593v1.pdf", "section": "1 Introduction", "chunk": "that ensuring reliability necessitates a\nparadigm shift towardedge-levelintervention: each\ninter-agent message should be treated as a poten-\ntial failure point where minimal clarification can\nhalt cascading errors, illustrated in Figure 1. To\nthis end, we proposeAgentAsk, a plug-and-play\nclarification module for LLM-based multi-agent\nsystems. AgentAsk introduces a taxonomy of four\ncore edge-level error types and implements a three-\nstage pipeline: (i) distilling knowledge from fail-\nure traces with a powerful evaluator, (ii) transfer-\nring this distilled capability to a lightweight clar-\nifier trained to generate targeted questions, and\n(iii) refining the clarifier through reinforcement\nlearning for adaptive, budget-aware behavior. Cru-\ncially, AgentAsk is designed to be lightweight,\narchitecture-agnostic, and easily integrated into ex-\nisting frameworks, enabling systems to intercept\nmisunderstandings before they spread.Our work makes the following contributions:\n\u2776Edge-level perspective and taxonomy:We\nformalize a four-type taxonomy of inter-agent\nerrors and motivate clarification as a first-class\nmechanism for multi-agent collaboration.\n\u2777Practical clarification module:AgentAsk\nis a lightweight and plug-and-play module\ntrained withE-GRPOreinforcement learn-\ning method, which agents can seamlessly inte-\ngrate to identify and resolve interaction errors.\n\u2778Comprehensive empirical evaluation:Exten-\nsive experiments across diverse benchmarks\ndemonstrate that AgentAsk consistently im-\nproves both accuracy and robustness over pub-\nlic multi-agent baselines with excessive la-\ntency and extra cost of only less than 5%.\n2 Related Work\nLLM-Based Multi-Agent Systems.MAS co-\nordinates multiple role-specialized agents, often\nequipped with tools, to solve complex tasks. Early\nsystems demonstrated that organizing multiple\nLLMs into role-specialized, conversational teams\nwith tool use can outperform single models un-\nder controlled settings, through frameworks that\nscaffold roles, protocols, and conversation pro-\ngramming such as AutoGen (Wu et al., 2024a),\nAgentVerse (Chen et al., 2024b), CAMEL (Li\net al., 2023), MetaGPT (Hong et al., 2024), and\nChatDev (Qian et al., 2024). Beyond hand-\ncrafted blueprints,"}
{"file": "2510.07593v1.pdf", "section": "2 Related Work", "chunk": "2024b), CAMEL (Li\net al., 2023), MetaGPT (Hong et al., 2024), and\nChatDev (Qian et al., 2024). Beyond hand-\ncrafted blueprints, recent work treats orchestra-\ntion as a learnable object: GPTSwarm models\nagents and communication links as an optimizable\ngraph (Zhuge et al., 2024), AFlow searches over\ncode-represented workflows via Monte Carlo tree\nsearch (Zhang et al., 2025b), MaAS samples query-\ndependent sub-architectures from an agentic super-\nnet (Zhang et al., 2025a), and MasRouter learns\na cascaded controller for collaboration mode, role\nallocation, and LLM selection (Yue et al., 2025).\nThese directions build on and complement decision-\ntime tool use, such as ReAct (Yao et al., 2023) and\nToolformer (Schick et al., 2023). Our work is or-\nthogonal to global orchestration search: we hold\nan existing pipeline fixed and inject a lightweight\nedge-level clarifier that adds minimal, well-placed\nquestions at message handoffs to curb error propa-\ngation with low latency and cost.\nFailure Analyses in MAS.Large-scale audits\nshow that multi-agent gains over strong single-\nagent baselines are not guaranteed and that many\nfailures arise at message handoffs where missing\ndetails, ambiguous references, or distorted interme-\ndiate results are left unclarified (Cemri et al., 2025;\nZhu et al., 2025; Zhang et al., 2025d). Method-\nologically related attempts to improve reliability\ninclude multi-agent debate and discussion (Chan\net al., 2024; Liang et al., 2024), self-feedback loops\nsuch as Reflexion (Shinn et al., 2023) and Self-\nRefine (Madaan et al., 2023), and verification or\nself-correction procedures (Wu et al., 2024b; Lee\net al., 2025), yet recent analyses document con-\nditions where debate or naive self-correction can\nfail or even degrade accuracy (Wynn et al., 2025).\nA complementary line advocates asking clarify-\ning questions under uncertainty in single-agent\nhuman"}
{"file": "2510.07593v1.pdf", "section": "2 Related Work", "chunk": "5), yet recent analyses document con-\nditions where debate or naive self-correction can\nfail or even degrade accuracy (Wynn et al., 2025).\nA complementary line advocates asking clarify-\ning questions under uncertainty in single-agent\nhuman\u2013computer interaction(Wang et al., 2024;\nZhang et al., 2025c; Mukherjee et al., 2025; Lee\net al., 2023). We follow this clarification-centric\nview but operationalize it for multi-agent settings\nwith an edge-aware policy that decides when, what,\nwhom, and how to ask exactly at the handoff where\nuncertainty or distortion originates.\n3 Error Taxonomy\nMany studies catalog errors in multi-agent systems\nand report large-scale statistics, yet most empha-\nsize breadth over depth. Prior classifications cover\nmany error types but give limited attention to isolat-\ning the most critical ones or to prescribing targeted\nremedies. In particular, although prior classifica-\ntions cover a wide range of error types, little atten-\ntion has been paid to identifying the most critical er-\nrors and developing targeted remedies. In this work,\nwe empirically reveal chain-style error propagation\nas a fundamental root cause of failures in multi-\nagent systems, where a single error of an agent can\ncascade into system-wide collapse. We audit 824\nexecution logs and annotate each agent-to-agent\nmessage as the unit of analysis, building on the\nexisting research (Wang et al., 2024). Cross anno-\ntation and adjudication yield a concise taxonomy\nwith high agreement that concentrates intervention\nexactly where failures begin.\nAnalogous to human collaboration, our empiri-\ncal results (shown in Figure 2) indicate that most er-\nrors in MAS stem from violations of a fundamental\nprinciple:agents should complete tasksaccurately\nwithintheir capabilitiesand deliverclear and com-\nplete informationhandovers to downstream agents\nwithout omission.Building on this observation,\nfour types of errors have been condensed.\nFigure 2:From taxonomy to design.(a) Empirical dis-\ntribution of edge-level error types ( N=824). (b) Three\ndesign inspirations distilled from our taxonomy: local-\nize fixes at the edge; maptype \u2192intent & addressee;\nenforce minimality-by-design. (c) Our designed Agen-\ntAsk for edge-level intervention (see Section 4"}
{"file": "2510.07593v1.pdf", "section": "3 Error Taxonomy", "chunk": "inspirations distilled from our taxonomy: local-\nize fixes at the edge; maptype \u2192intent & addressee;\nenforce minimality-by-design. (c) Our designed Agen-\ntAsk for edge-level intervention (see Section 4).\n3.1 Data Gap\nIn a good collaboration, a handoff never misses a\ncrucial detail. In human teams, a brief that forgets\na constraint sends downstream work off course,\nand the cost multiplies with each relay. In multi-\nagent settings, the same dynamic appears when a\nboundary case, a unit, or a condition is left out.\nSmall omissions turn into systemic bias as later\nsteps harden a wrong default. A short check at\nthe handoff that lists required fields and confirms\nranges stops the slide before it spreads.\n3.2 Referential Drift\nTeams falter when words stop pointing to the\nsame thing. People know the feeling when two\nteammates use different names for the same ob-\nject, and the dialogue quietly splits into parallel\nthreads. Agents replicate this when pronouns,\nvariable names, or record keys are passed with-\nout grounding, so later steps bind them inconsis-\ntently. The result is confident but incompatible\nreasoning. A crisp confirmation of who is who\nand which value is which at the handoff restores a\nsingle shared state.\n3.3 Signal Corruption\nA distorted intermediate result enters the chain and\ngets propagated as truth. In human work, a spread-\nsheet with a subtle formula error can contaminate\nevery chart that follows. Agents exhibit the same\nfragility when a wrong fact, a malformed structure,\nor a unit mistake is presented with fluency and then\nreused without scrutiny. Downstream modules am-\nplify the error instead of correcting it. A targeted\nrequest to verify the source value or structure at the\nhandoff contains the damage early.\n3.4 Capability Gap\nHealthy organizations respect the limits of each\nrole. Teams recognize this when a specialist is\nasked to deliver outside their training, and impro-\nvisation replaces method. In multi-agent pipelines,\nthis appears when a role tuned for explanation is\nasked for algebra or when a planner is pushed\ninto domain judgment. Progress stalls because the\nwrong skill sits at the critical edge. An early redi-\nrect or a brief request for guidance at the handoff\naligns the task with a"}
{"file": "2510.07593v1.pdf", "section": "3.4 Capability Gap", "chunk": "ed for algebra or when a planner is pushed\ninto domain judgment. Progress stalls because the\nwrong skill sits at the critical edge. An early redi-\nrect or a brief request for guidance at the handoff\naligns the task with a capable peer and keeps the\nchain coherent.\n3.5 Inspiration\nOur analysis shows that all four error types reduce\nto failures at the edge, the information handoff be-\ntween agents in a multi-agent system. This shifts\nthe design focus to the handoff itself rather than\nthe global workflow.In our annotated corpus\nN=824, these errors account for most failures,\nwith Data Gap 29.1%, Referential Drift 27.3%, Sig-\nnal Corruption 36.8%, and Capability Gap 6.8%.\nIn effective human collaboration (Woolley et al.,\n2010; Cobbe et al., 2021), handoffs are governed\nby three habits: \u2776check the handoff for missing\ncontent, ambiguous reference, corrupted intermedi-\nate results, or role mismatch; \u2777if a problem exists,\nfix it at the source so it does not propagate down-\nstream; \u2778intervene only as needed, keeping the\nexchange brief and within latency and cost limits.\nThese habits motivate an edge-centered design\nthat monitors each transfer and issues minimal clar-\nifications exactly at the point of exchange. The\ndetails can be seen in Appendix C.\n4 Design of AgentAsk\nGrounded in the taxonomy, failures cluster at the\nedge where one agent hands information to the next.\nWe therefore design around three moves and intro-\nduceAgentAsk, a framework for real-time moni-\ntoring and correction in multi-agent systems. First,\nto find errors, we place an external clarifier at each\nhandoff that inspects the outgoing message and its\nlocal context and flags risk early. Second, to correct\nerrors, we use \u201cAsk\u201d: the AgentAsk sends a brief,\ntargeted question to the right party so the handoff\nis repaired before the chain advances. Third, we\ntransferred the asking capabilities of the powerful\nmodel to AgentAsk, but to keep correction efficient,\nfine-tuning alone is not enough, so we add rein-\nforcement learning that adapts the ask decision, the\naddressee, and the phrasing to context and budget.Concretely, we use a two-stage training paradigm,\nKnowledge"}
{"file": "2510.07593v1.pdf", "section": "4 Design of AgentAsk", "chunk": ",\nfine-tuning alone is not enough, so we add rein-\nforcement learning that adapts the ask decision, the\naddressee, and the phrasing to context and budget.Concretely, we use a two-stage training paradigm,\nKnowledge Transfer via Supervised Fine-tuningfol-\nlowed byAutonomous Learning via Reinforcement\nOptimization, which givesAgentAskan inherited\nability to detect faulty outputs and a learned ability\nto apply minimal, cost-aware clarifications. The\nempirical experiments are provided in Section 5.\nThe design of the proposed method is illustrated in\nFigure 3. Complete training pseudocode appears\nin Appendix A as Algorithm 1.\n4.1 Modeling of Multi-Agent System\nWe view a multi-agent system as a directed inter-\naction graph G= (V, E) that is unrolled per input\ninstance into an edge sequence {et}T\nt=1, where each\nedge et= (u t\u2192vt)carries a message produced\nby agent utand consumed by agent vt.AgentAsk\nis an edge-local controller that sits between utand\nvtand decides whether to intervene with a mini-\nmal clarification before the message is delivered;\nthe decision must respect latency and budget while\nimproving task success.\nWe represent the edge context by a compact state\nthat contains only what is available at the handoff:\nxt= (xin\nt, ut, vt, mt, ht).(1)\nHerexin\ntdenotes the user query or current subgoal,\nmtis the candidate message from ut, and hten-\ncodes short-horizon interaction traces relevant to\nthis link . The controller produces a structured ac-\ntionat= (z t,\u02dcvt, qt), where zt\u2208 {0,1} is an ask\ngate, \u02dcvt\u2208 {u t, vt}is the addressee when asking,\nandqtis a short clarification string following a\nfixed schema.\nThe decision policy is factored to separate the bi-\nnary gate, the addressee, and the question generator\nwhile falling back to no-op whenz t= 0:\n\u03c0\u03b8(at|xt) =\u03c0 \u03b8(zt|xt)h\n\u03c0\u03b8(\u02dcvt|xt)\u03c0\u03b8(qt|xt)izt.\n(2)\nThis factorization treats the clarification as condi-\ntional text generation under a"}
{"file": "2510.07593v1.pdf", "section": "4.1 Modeling of Multi-Agent System", "chunk": "xt) =\u03c0 \u03b8(zt|xt)h\n\u03c0\u03b8(\u02dcvt|xt)\u03c0\u03b8(qt|xt)izt.\n(2)\nThis factorization treats the clarification as condi-\ntional text generation under a hard budget on length\nand a lightweight schema constraint; when zt= 0,\nthe message passes through unchanged, and the\nedge terminates immediately.\nAfter executing at, the environment returns a re-\nplyrtwhen zt= 1and updates the local trace and\ncounters, leading to the next edge state xt+1and a\ndeterministic delivery to vt. The full interaction for\none input forms a trajectory \u03c4={(x t, at)}T\nt=1with\na terminal outcome that scores correctness, while\nthe controller accrues latency and tokenized cost\nFigure 3:Overview of AgentAsk.The module operates at theedge level, treating each inter-agent message as a\npotential failure point. The figure illustrates both the architecture and training process: (i)knowledge distillationfrom\nfailure traces using a large evaluator to construct an edge-level corpus, (ii)supervised fine-tuningof a lightweight\nclarifier that decideswhen/what/whom/howto ask, and (iii)reinforcement learningwith E-GRPO for adaptive\nclarification under latency and cost constraints. This pipeline equips AgentAsk with edge-aware monitoring and\nminimal, targeted interventions while remaining architecture-agnostic and easy to integrate into diverse MASs.\nalong the way; we summarize this as a constrained\nobjective over trajectories\nmax\n\u03b8E\u03c4\u223c\u03c0\u03b8[U(\u03c4)]s.t.E \u03c4\u223c\u03c0\u03b8[C(\u03c4)]\u2264B.(3)\nHere U(\u03c4) is a task utility that rewards accurate\nsolutions and well-formed clarifications, C(\u03c4) ag-\ngregates latency and token usage induced by inter-\nventions, and Bis a budget. Within this modeling\nview, the next two stages instantiate the policy \u03c0\u03b8: a\nsupervised stage that transfers edge-level judgment\nand a reinforcement stage that optimizes the ask\ngate and question under the constraint in Eq. (3).\n4.2 Knowledge Transfer via Supervised\nFine-tuning\nWe initialize the edge controller by supervised\nlearning to imitate high-quality clarifications on\nindividual handoffs.\nCorpus construction.The SFT corpus is built\nfrom logged multi-agent traces by"}
{"file": "2510.07593v1.pdf", "section": "4.2 Knowledge Transfer via Supervised", "chunk": ".2 Knowledge Transfer via Supervised\nFine-tuning\nWe initialize the edge controller by supervised\nlearning to imitate high-quality clarifications on\nindividual handoffs.\nCorpus construction.The SFT corpus is built\nfrom logged multi-agent traces by applying a\nteacher judge that assigns, for each edge state\nxifrom Eq. (1), a minimal intervention triple\nyi= (t i, vi, qi)withti\u2208 T ,vi\u2208 {u i, vi}, and\na short questionq i. Concretely, we construct\neD={(x i, yi)}N\ni=1, yi= (t i, vi, qi), (4)\nThe label tiencodes the ask decision and type,\nviis the addressee when asking, and qi=\n(qi,1, . . . , q i,Ti)is a schema-constrained questionwith a token cap; the indicator mi=1[t i\u0338=NONE]\ngates whether an ask is present. We train the fac-\ntored policy in Eq. (2)with three heads: a classifier\nptype\n\u03b8(t|x) , an addressee head paddr\n\u03b8(v|x, t) , and\nan auto-regressive decoderptxt\n\u03b8(q|x, t).\nSupervised transfer.The SFT objective sep-\narates the gate/type supervision from the ask-\nconditioned components and then combines them.\nThe type loss is\nLask=1\nNNX\ni=1mih\n\u2212logpaddr\n\u03b8(vi|xi, ti)\n\u2212TiX\nt=1logptxt\n\u03b8(qi,t|xi, ti, qi,<t)i\n.\n(5)\nThe final SFT objective is a weighted sum that\nmatches the factorization and respects the gate:\nLSFT=L type+\u03bb askLask.(6)\nThis trains the controller to first decide whether and\nwhat to ask, and, only when needed, to select the\nrecipient and generate a concise, well-formed ques-\ntion aligned with edge-local budgets. In particular,\nCG questions can be solved to some extent by the\ntarget address when asking the question.\n4.3 Autonomous Learning via Reinforcement\nOptimization\nSupervised fine-tuning transfers edge-level judg-\nment and question quality from a strong evaluator,\nbut it only imitates the teacher on logged cases.\nImitation is static:"}
{"file": "2510.07593v1.pdf", "section": "4.2 Knowledge Transfer via Supervised", "chunk": "question.\n4.3 Autonomous Learning via Reinforcement\nOptimization\nSupervised fine-tuning transfers edge-level judg-\nment and question quality from a strong evaluator,\nbut it only imitates the teacher on logged cases.\nImitation is static: it is limited by dataset cover-\nage, does not learn when not to ask, and cannot\nbalance accuracy against constraints under a new\nscenario. The ask gate and addressee are discrete\nchoices whose value is revealed only after down-\nstream effects, so credit must flow across the chain.\nWe therefore continue with reinforcement learning\nto adapt the factored controller in Eq. (2) to the con-\nstrained objective in Eq. (3), using shaped rewards\nthat turn these trade-offs into learnable signals.\nReward Design.First, we encourage asking only\nwhen it actually removes downstream uncertainty:\nreff\nt=\uf8f1\n\uf8f4\uf8f2\n\uf8f4\uf8f31, z t= 1\u2227n t+1= 0,\n\u22121, z t= 1\u2227n t+1= 1,\n0, z t= 0.(7)\nSecond, we enforce parsimony to keep clarifica-\ntions scarce and budget-aware via a sliding counter\nct:\nrpar\nt=\u2212\u03bb swmax(c t\u22121,0).(8)\nThird, we stabilize outputs with a light format\nbonus that preserves schema and shortness:\nrfmt\nt=\u03b1 fmt 1[Ft= 1].(9)\nThese signals are aggregated into an edge reward\nthat embodies our \u201cfix-early, fix-locally\u201d principle,\nredge\nt=\u03b1 effreff\nt+rpar\nt+rfmt\nt,(10)\nwhile a terminal score anchors final correctness,\nR=\u03b1 anss.(11)\nTogether, Eqs. (10)\u2013(11) align the controller with\nour objective: intervene only when a concise ques-\ntion can halt DG/RD/SC/CG propagation, avoid\ngratuitous turns, keep outputs well-formed, and\nultimately solve the task.\nOptimization: E-GRPOBefore the episode\nreaches its terminal agent, the final correctness re-\nwardRis unavailable; By following the Group Rel-\native Policy Optimization (GRPO) (DeepSeek-AI\net al., 2025), in this prefix regime, we trainexactly"}
{"file": "2510.07593v1.pdf", "section": "4.3 Autonomous Learning via Reinforcement", "chunk": "its terminal agent, the final correctness re-\nwardRis unavailable; By following the Group Rel-\native Policy Optimization (GRPO) (DeepSeek-AI\net al., 2025), in this prefix regime, we trainexactly\non the edge-level signal by applying a within-edge\nrelative update that selects the best clarification\nusing redge\nt. Once the episode terminates and R\nis observed, weaugmentthe same update with a\nglobal credit that allocates outcome utility backto each edge occurrence, thereby coupling local\ncontainment with end-task success.\nFor each edge t, we form a local advantage Aloc\nt\nby the controller using redge\nt. The system-level\nsignal is distributed to every token at time twith\nnonnegative weights wtand an action-independent\nbaselineb,\nAglob\nt=w t(R\u2212b).(12)\nLet\u03c1t=\u03c0 \u03b8(at|ht)/\u03c0old(at|ht). We maxi-\nmize the E-GRPO surrogate that shares ratios for\nlocal and global credit and regularizes toward a\nreference:\nJE-GRPO (\u03b8) =E th\nmin\u0000\n\u03c1tAloc\nt,clip(\u03c1 t,1\u00b1\u03f5)Aloc\nt\u0001\n+\u03bb Rmin\u0000\n\u03c1tAglob\nt,clip(\u03c1 t,1\u00b1\u03f5)Aglob\nt\u0001i\n\u2212\u03b2E t\u0002\nKL\u0000\n\u03c0\u03b8(\u00b7 |h t)\u2225\u03c0 ref(\u00b7 |h t)\u0001\u0003\n.\n(13)\nWhen Ris not yet available, the second term in\nEq.(13) is simply absent, and training proceeds\npurely on edge-level selection; when Rarrives, it\nis injected uniformly through Aglob\nt, ensuring that\nminimal, successful clarifications remain aligned\nwith end-to-end correctness while updates stay con-\nservative via clipping and reference KL (where\n\u201cKL\u201d is the Kullback\u2013Leibler divergence between\nthe current edge policy and the SFT-initialized ref-\nerence, acting as a trust-region regularizer that pre-\nserves the clarifier\u2019s brevity and schema prior).\n5 Experiments\nWe assess whether a lightweight,edge-levelclari-\nfier improves reliability under unchanged orches-\ntration, how it trades accuracy against latency and\ncost, and why these effects arise. Our"}
{"file": "2510.07593v1.pdf", "section": "4.3 Autonomous Learning via Reinforcement", "chunk": "vity and schema prior).\n5 Experiments\nWe assess whether a lightweight,edge-levelclari-\nfier improves reliability under unchanged orches-\ntration, how it trades accuracy against latency and\ncost, and why these effects arise. Our study centers\non three research questions (RQ):\n\u2022Effectiveness under fixed orchestration (RQ1):\nDoesAgentAskimprove end-task performance\nacross frameworks and datasets without altering\nexisting conditions? (Results onTable 1.)\n\u2022Efficiency at the accuracy frontier (RQ2):\nWhere doesAgentAsksit on the efficiency fron-\ntier relative to a strong evaluator and baselines?\n(Results:Table 2, breakdowns inAppendix C.)\n\u2022Mechanism and Robustness at the edge (RQ3):\nWhich error types dominate and how often are\nthey one-shot resolvable, and how stable are out-\ncomes under edge-level controls? (Summaries:\nFigure 4; cases inAppendix B)\nMethod Settings GSM8K MATH HumanEval MMLU MBPP Average\nIO 89.52 49.23 89.66 79.63 71.38 75.88\nCoT 89.71 48.19 90.64 80.44 70.66 75.93\nGPTSwarmorigin 92.18 51.41 90.63 81.26 76.81 78.46\n+GPT-5 94.11 \u2191+1.93 52.03 \u2191+0.62 92.20 \u2191+1.57 82.34 \u2191+1.08 79.05 \u2191+2.24 79.95\u2191+1.49\n+AgentAsk 92.98 \u2191+0.80 51.29 \u2193-0.12 91.71 \u2191+1.08 81.93 \u2191+0.67 77.23 \u2191+0.42 79.08\u2191+0.62\norigin 91.35 50.89 91.67 82.16 79.63 79.14\nAFlow +GPT-5 94.02 \u2191+2.67 52.10 \u2191+1.21 92.45 \u2191+0.78 83.01 \u2191+0.85 81.15 \u2191+1.52 80"}
{"file": "2510.07593v1.pdf", "section": "5 Experiments", "chunk": "PT-5 94.02 \u2191+2.67 52.10 \u2191+1.21 92.45 \u2191+0.78 83.01 \u2191+0.85 81.15 \u2191+1.52 80.55\u2191+1.41\n+AgentAsk 92.47 \u2191+1.12 51.05 \u2191+0.16 92.02 \u2191+0.35 82.63 \u2191+0.47 80.02 \u2191+0.39 79.64\u2191+0.50\nMaASorigin 92.84 51.08 92.02 82.93 81.22 80.02\n+GPT-5 94.62 \u2191+1.78 52.21 \u2191+1.13 93.41 \u2191+1.39 83.59 \u2191+0.66 82.77 \u2191+1.55 81.32\u2191+1.30\n+AgentAsk 92.72 \u2193-0.12 52.31 \u2191+1.23 92.63 \u2191+0.61 83.21 \u2191+0.28 81.95 \u2191+0.73 80.56\u2191+0.54\norigin 93.26 51.52 91.03 83.19 79.04 79.61\nMasRouter +GPT-5 95.34 \u2191+2.08 52.49 \u2191+0.97 92.90 \u2191+1.87 83.78 \u2191+0.59 82.44 \u2191+3.40 81.39\u2191+1.78\n+AgentAsk 94.72 \u2191+1.46 52.07 \u2191+0.55 91.55 \u2191+0.52 82.86 \u2193-0.33 80.64 \u2191+1.60 80.37\u2191+0.76\nTable 1: Accuracy/Pass@1 across frameworks under three settings ( origin ,+GPT-5 ,+AgentAsk ) on five bench-\nmarks. Bottom-right overlays denote percentage-point deltas vs. each framework\u2019s origin . Each cell overlays a\nbottom-right delta (green\u2191/ red\u2193) relative to the framework\u2019sorigin.\n5.1 Experimental Setup\nWe evaluateAgentAskon five public benchmarks\nspanning math reasoning (GSM8"}
{"file": "2510.07593v1.pdf", "section": "5 Experiments", "chunk": "each framework\u2019s origin . Each cell overlays a\nbottom-right delta (green\u2191/ red\u2193) relative to the framework\u2019sorigin.\n5.1 Experimental Setup\nWe evaluateAgentAskon five public benchmarks\nspanning math reasoning (GSM8K (Cobbe et al.,\n2021), MATH (Hendrycks et al., 2021b)), general-\nknowledge QA (MMLU (Hendrycks et al., 2021a)),\nand code generation (HumanEval (Chen et al.,\n2024a), MBPP (Austin et al., 2021)).Baselines in-\nclude single-model prompting (IO, CoT (Wei et al.,\n2022b)) and four representative multi-agent frame-\nworks (GPTSwarm (Zhuge et al., 2024), AFlow\n(Zhang et al., 2025b), MaAS (Zhang et al., 2025a),\nMasRouter (Yue et al., 2025)).\nFor each framework, we consider three settings\nthat isolate the effect of the clarifier while holding\norchestration constant: (i) origin uses the public\nimplementation; (ii) +GPT-5 substitutes the clarifier\nwithGPT-5 (OpenAI, 2025); (iii) +AgentAsk plugs\nin our lightweight module. The LLM executor is\nGPT-4o-mini-0718 (OpenAI, 2024); +AgentAsk\nusesQwen-3-4B (Yang et al., 2025) as the back-\nbone. Accuracy for GSM8K, MATH, and MMLU\nis reported; Pass@1 for HumanEval and MBPP.\nPrompts are provided in the Appendix D.\n5.2 RQ1: Effectiveness under fixed\norchestration\nObservation. AgentAskimproves the large major-\nity of framework \u00d7dataset cases and closes much\nof the gap to a strong evaluator without changing\nthe pipeline.\nResults and analysis.Table 1 reports accuracy/\nPass@1 on five benchmarks across four frame-\nworks under origin ,+GPT-5 , and+AgentAsk .Settings GSM8K\nAccuracy Latency Extra Cost\norigin 93.26 100 0.0\n+GPT\u20134o\u2013mini 94.62 \u2191+1.36 118 16.0\n+"}
{"file": "2510.07593v1.pdf", "section": "5.2 RQ1: Effectiveness under fixed", "chunk": "Ask .Settings GSM8K\nAccuracy Latency Extra Cost\norigin 93.26 100 0.0\n+GPT\u20134o\u2013mini 94.62 \u2191+1.36 118 16.0\n+GPT\u20135 95.34 \u2191+2.08 134 38.0\nAgentAsk (Llama\u20133.2\u20133B)\nSFT 93.64 \u2191+0.38 106 5.7\n(E\u2013GRPO) 94.23 \u2191+0.97 105 5.0\nAgentAsk (Qwen\u20133\u20134B)\nSFT 93.99 \u2191+0.73 105 5.3\n(E\u2013GRPO) 94.72 \u2191+1.46 103 4.2\nTable 2:Ablations.Experiment on different type of\nclarifier with the MasRouter@GSM8K.\nRelative to origin ,+AgentAsk yields positive\ndeltas in roughly 17/20 cells with mean gains\naround + 0.5 pp while orchestration remains un-\nchanged. Gains appear in math (GSM8K, MATH),\ngeneral knowledge (MMLU), and code (Hu-\nmanEval, MBPP), indicating broadly useful, link-\nlocal clarifications. A small number of non-\nmonotone cells coincide with datasets whose error\nmixture is less amenable to one-shot clarification\n(see RQ3 in Section 5.4). We can see that GPT-5\ngenerally achieves the greatest improvement, but\nour AgentAsk approach also achieves considerable\nperformance gains. However, their performance\non other metrics, such as cost and latency, remains\nto be compared, and we will conduct subsequent\nexperiments and analysis.\n5.3 RQ2: Efficiency at the accuracy frontier\nObservation. AgentAskapproaches the heavy-\nevaluator ceiling at a fraction of the latency and\ncost, while surpassing origin with small overhead.\nFigure 4:(Left)Pareto frontier (x=latency, y=extra cost; bubble size=Acc), showing +AgentAsk nearing+GPT-5 at\nmuch lower overhead.(Middle)Error-Type distributions (DG/SC/RD/CG) across datasets.(Right)Sensitivity to\nwindowHand penalty\u03bb sw, highlighting a stable region near the default and"}
{"file": "2510.07593v1.pdf", "section": "5.3 RQ2: Efficiency at the accuracy frontier", "chunk": "Ask nearing+GPT-5 at\nmuch lower overhead.(Middle)Error-Type distributions (DG/SC/RD/CG) across datasets.(Right)Sensitivity to\nwindowHand penalty\u03bb sw, highlighting a stable region near the default and the accuracy\u2013efficiency trade-off.\nResults and analysis.On MasRouter@GSM8K\n(Table 2), substituting a strong evaluator maxi-\nmizes accuracy (95.34%) but raises latency and\ncost to 134% and 38.0%. In contrast,AgentAsk\n(Qwen-3-4B , E\u2013GRPO) attains 94.72% at103%\nlatency and4.2%extra cost\u201431pp and33.8pp\nlower than +GPT-5 , respectively, with only 0.62 pp\naccuracy gap. The Llama-3.2-3B variant mirrors\nthis trend. Thus,AgentAsksituates on a favor-\nable Pareto frontier:good-enoughaccuracy under\ntight budgets, offering a pragmatic alternative when\nheavy evaluators are cost-prohibitive.\nIn the Pareto view (Figure4 Left), +AgentAsk\nbubbles cluster near the accuracy envelope but re-\nmain close to the origin on latency/cost axes, indi-\ncating that most of the attainable accuracy can be\nrealized without incurring heavy-evaluator spend.\nExperimental results on other datasets and base-\nlines have been presented in the appendix C (See\nTable 7, Table 8, and Table 9) and briefly analyzed.\n5.4 RQ3: Mechanism and Robustness at edge\nObservation.DG/SC dominate and are frequently\none-shot fixable; RD/CG are rarer and harder to\nneutralize in a single turn, matching where gains\nare largest.\nResults and analysis. Figure 4(Middle) summa-\nrizes clarification types. Pooled over events,Data\nGapandSignal Corruptionconstitute the major-\nity (e.g., 36.8%, 30.9%) and exhibit higher one-\nshot resolution (about 72.5%, 69.1%), which cor-\nresponds to the broad positive deltas inTable 1.\nReferential DriftandCapability Gapare less\nfrequent (20.7%, 11.6%) and less often one-shot re-\nsolved (56.4"}
{"file": "2510.07593v1.pdf", "section": "5.4 RQ3: Mechanism and Robustness at edge", "chunk": "which cor-\nresponds to the broad positive deltas inTable 1.\nReferential DriftandCapability Gapare less\nfrequent (20.7%, 11.6%) and less often one-shot re-\nsolved (56.4%, 44.8%), aligning with smaller or oc-\ncasional negative deltas on RD/CG-richer subsets,\nnotably parts of MATH/MMLU. Qualitative casestudies pair each raw trace with the triggered ques-\ntion (when/what/whom) and the corrected down-\nstream step; these examples are shown inAp-\npendix B, making the edge-containment mecha-\nnism concrete.\nSensitivity.We further examine how two key pa-\nrameters shape the accuracy\u2013efficiency balance;\nsummary curves are inFigure 4(Right).(i)\nSliding-window length H.Enlarging Hfrom 2\nto 4 yields a clear accuracy gain (e.g., 94.36% \u2192\n95.10%), while H=5 brings only marginal benefit\nwith higher overhead (accuracy\u224895.05%, latency\n101% \u2192110%, extra cost 3.5% \u21926.7%).(ii)\nParsimony weight \u03bbsw.With H=3 , increasing \u03bbsw\nfrom 0.2 to 0.8 makes the policy thriftier (extra cost\n5.6%\u21923.3%; latency 106% \u2192100%) at a mod-\nest accuracy trade-off (94.85% \u219294.38%). We\ndefault to\u03bb sw=0.4andH=3.\n6 Conclusion\nIn this paper, we presentAgentAsk, a lightweight,\narchitecture-agnostic clarifier that operates at the\nedge level of multi-agent chains to arrest er-\nror propagation with minimal, targeted questions.\nGrounded in a four-type taxonomy, AgentAsk\ncouples supervised distillation with E-GRPO that\naligns local fixes with end-task success under la-\ntency and cost constraints. Across multiple tasks,\nAgentAsk consistently improves performance over\npublic multi-agent system methods while approach-\ning large models\u2019 performance at a fraction of the\noverhead. Beyond empirical gains, our edge-level\nperspective offers a principled recipe for reliable\norchestration that complements role design, search,\nand self-checking. Future work spans uncertainty-\naware ask gating, theory for edge-level interven-\ntions,"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "gains, our edge-level\nperspective offers a principled recipe for reliable\norchestration that complements role design, search,\nand self-checking. Future work spans uncertainty-\naware ask gating, theory for edge-level interven-\ntions, tool- and human-in-the-loop integration, and\nrobustness under adversarial or shifted settings.\nLimitations\nAgentAsk currently inherits much of its judgment\nquality from the underlying LLMs used as the\nteacher (for distillation) and as the lightweight clar-\nifier backbone. As a result, its reliability and granu-\nlarity of clarifications scale with these base models;\nunder weaker backbones, gains may diminish even\nthough the logic of edge-level intervention remains\nsound. Future work will explore model-agnostic\nuncertainty signals and hybrid features to reduce\ndependence on specific LLM strengths.\nReferences\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten\nBosma, Henryk Michalewski, David Dohan, Ellen\nJiang, Carrie Cai, Michael Terry, Quoc Le, and\nCharles Sutton. 2021. Program synthesis with large\nlanguage models.Preprint, arXiv:2108.07732.\nMert Cemri, Melissa Z Pan, Shuyi Yang, Lakshya A\nAgrawal, Bhavya Chopra, Rishabh Tiwari, Kurt\nKeutzer, Aditya Parameswaran, Dan Klein, Kannan\nRamchandran, Matei Zaharia, Joseph E. Gonzalez,\nand Ion Stoica. 2025. Why do multi-agent LLM sys-\ntems fail? InNeurIPS 2025 Workshop on Evaluating\nthe Evolving LLM Lifecycle: Benchmarks, Emergent\nAbilities, and Scaling.\nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu,\nWei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu.\n2024. Chateval: Towards better LLM-based eval-\nuators through multi-agent debate. InThe Twelfth\nInternational Conference on Learning Representa-\ntions.\nLingjiao Chen, Matei Zaharia, and James Zou. 2024a.\nFrugalGPT: How to use large language models while\nreducing cost and improving performance.Transac-\ntions on Machine Learning"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": ".\nLingjiao Chen, Matei Zaharia, and James Zou. 2024a.\nFrugalGPT: How to use large language models while\nreducing cost and improving performance.Transac-\ntions on Machine Learning Research.\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang,\nChenfei Yuan, Chi-Min Chan, Heyang Yu, Yaxi\nLu, Yi-Hsin Hung, Chen Qian, Yujia Qin, Xin\nCong, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and\nJie Zhou. 2024b. Agentverse: Facilitating multi-\nagent collaboration and exploring emergent behav-\niors. 2024:20094\u201320136.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems.Preprint, arXiv:2110.14168.\nDeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang,\nJunxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu,\nShirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang,\nXingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhi-\nhong Shao, Zhuoshu Li, Ziyi Gao, and 181 others.2025. Deepseek-r1: Incentivizing reasoning capa-\nbility in llms via reinforcement learning.Preprint,\narXiv:2501.12948.\nAlireza Ghafarollahi and Markus J. Buehler. 2025. Scia-\ngents: Automating scientific discovery through bioin-\nspired multi-agent intelligent graph reasoning.Ad-\nvanced Materials, 37(22):2413523.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021a. Measuring massive multitask language under-\nstanding. InInternational Conference on Learning\nRepresentations.\nDan Hendrycks,"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": ", Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021a. Measuring massive multitask language under-\nstanding. InInternational Conference on Learning\nRepresentations.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021b. Measuring mathematical\nproblem solving with the MATH dataset. InThirty-\nfifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 2).\nSirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu\nZheng, Yuheng Cheng, Jinlin Wang, Ceyao Zhang,\nZili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang\nZhou, Chenyu Ran, Lingfeng Xiao, Chenglin Wu,\nand J\u00fcrgen Schmidhuber. 2024. MetaGPT: Meta pro-\ngramming for a multi-agent collaborative framework.\nInThe Twelfth International Conference on Learning\nRepresentations.\nDongryeol Lee, Segwang Kim, Minwoo Lee, Hwan-\nhee Lee, Joonsuk Park, Sang-Woo Lee, and Kyomin\nJung. 2023. Asking clarification questions to handle\nambiguity in open-domain QA. InFindings of the\nAssociation for Computational Linguistics: EMNLP\n2023, pages 11526\u201311544, Singapore. Association\nfor Computational Linguistics.\nHyunseok Lee, Seunghyuk Oh, Jaehyung Kim, Jinwoo\nShin, and Jihoon Tack. 2025. ReVISE: Learning to\nrefine at test-time via intrinsic self-verification. In\nForty-second International Conference on Machine\nLearning.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani\nItani, Dmitrii Khizbullin, and Bernard Ghanem. 2023.\nCAMEL: Communicative agents for \u201dmind\u201d explo-\nration of large language model society. InThirty-\nseventh Conference on Neural Information Process-\ning Systems.\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang,\nYan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "seventh Conference on Neural Information Process-\ning Systems.\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang,\nYan Wang, Rui Wang, Yujiu Yang, Shuming Shi, and\nZhaopeng Tu. 2024. Encouraging divergent thinking\nin large language models through multi-agent debate.\nInProceedings of the 2024 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n17889\u201317904, Miami, Florida, USA. Association for\nComputational Linguistics.\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu\nLei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen\nMen, Kejuan Yang, Shudan Zhang, Xiang Deng, Ao-\nhan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng\nShen, Tianjun Zhang, Yu Su, Huan Sun, and 3 others.\n2024. Agentbench: Evaluating LLMs as agents. In\nThe Twelfth International Conference on Learning\nRepresentations.\nAndres M. Bran, Sam Cox, Oliver Schilter, Carlo Bal-\ndassari, Andrew D White, and Philippe Schwaller.\n2024. Augmenting large language models with chem-\nistry tools.Nature Machine Intelligence, 6(5):525\u2013\n535.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nShashank Gupta, Bodhisattwa Prasad Majumder,\nKatherine Hermann, Sean Welleck, Amir Yazdan-\nbakhsh, and Peter Clark. 2023. Self-refine: Itera-\ntive refinement with self-feedback. InAdvances in\nNeural Information Processing Systems, volume 36,\npages 46534\u201346594. Curran Associates, Inc.\nSubhojyoti Mukherjee, Viet Dac Lai, Raghavendra Ad-\ndanki, Ryan Rossi, Seunghyun Yoon, Trung Bui,\nAnup Rao, Jayakumar Subramanian, and Branislav\nKveton. 202"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "Lai, Raghavendra Ad-\ndanki, Ryan Rossi, Seunghyun Yoon, Trung Bui,\nAnup Rao, Jayakumar Subramanian, and Branislav\nKveton. 2025. Learning to clarify by reinforce-\nment learning through reward-weighted fine-tuning.\nPreprint, arXiv:2506.06964.\nOpenAI. 2024. https://openai.com/index/\ngpt-4o-mini-advancing-cost-efficient-intelligence/.\nAccessed: 2025-05-10.\nOpenAI. 2025. https://openai.com/index/\nintroducing-gpt-5/. Accessed: 2025-08-07.\nJoon Sung Park, Joseph O\u2019Brien, Carrie Jun Cai, Mered-\nith Ringel Morris, Percy Liang, and Michael S. Bern-\nstein. 2023. Generative agents: Interactive simulacra\nof human behavior. InProceedings of the 36th An-\nnual ACM Symposium on User Interface Software\nand Technology, UIST \u201923, New York, NY , USA.\nAssociation for Computing Machinery.\nChen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan\nDang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng\nSu, Xin Cong, Juyuan Xu, Dahai Li, Zhiyuan Liu,\nand Maosong Sun. 2024. ChatDev: Communicative\nagents for software development. InProceedings\nof the 62nd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 15174\u201315186, Bangkok, Thailand. Association\nfor Computational Linguistics.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta\nRaileanu, Maria Lomeli, Eric Hambro, Luke Zettle-\nmoyer, Nicola Cancedda, and Thomas Scialom. 2023.\nToolformer: Language models can teach themselves\nto use tools. InAdvances in Neural Information\nProcessing Systems, volume 36, pages 68539\u201368551.\nCurran Associates, Inc.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik Narasimhan, and Shunyu Yao. 2023"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "36, pages 68539\u201368551.\nCurran Associates, Inc.\nNoah Shinn, Federico Cassano, Ashwin Gopinath,\nKarthik Narasimhan, and Shunyu Yao. 2023. Re-\nflexion: language agents with verbal reinforcement\nlearning. InAdvances in Neural Information Process-\ning Systems, volume 36, pages 8634\u20138652. Curran\nAssociates, Inc.Kun Wang, Guibin Zhang, Zhenhong Zhou, Jiahao\nWu, Miao Yu, Shiqian Zhao, Chenlong Yin, Jinhu\nFu, Yibo Yan, Hanjun Luo, and 1 others. 2025.\nA comprehensive survey in llm (-agent) full stack\nsafety: Data, training and deployment.arXiv preprint\narXiv:2504.15585.\nWenxuan Wang, Juluan Shi, Chaozheng Wang, Cheryl\nLee, Youliang Yuan, Jen tse Huang, and Michael R.\nLyu. 2024. Learning to ask: When llms meet unclear\ninstruction.CoRR, abs/2409.00557.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022a. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 24824\u201324837. Curran Associates,\nInc.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022b. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems,\nvolume 35, pages 24824\u201324837. Curran Associates,\nInc.\nAnita Williams Woolley, Christopher F. Chabris, Alex\nPentland, Nada Hashmi, and Thomas W. Malone.\n2010. Evidence for a collective intelligence fac-\ntor in the performance of human groups.Science,\n330(6004):686\u2013688.\nQingyun Wu, Gagan Bansal"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": ", Nada Hashmi, and Thomas W. Malone.\n2010. Evidence for a collective intelligence fac-\ntor in the performance of human groups.Science,\n330(6004):686\u2013688.\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,\nBeibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang,\nShaokun Zhang, Jiale Liu, Ahmed Hassan Awadallah,\nRyen W White, Doug Burger, and Chi Wang. 2024a.\nAutogen: Enabling next-gen LLM applications via\nmulti-agent conversations. InFirst Conference on\nLanguage Modeling.\nZhenyu Wu, Qingkai Zeng, Zhihan Zhang, Zhaoxuan\nTan, Chao Shen, and Meng Jiang. 2024b. Large lan-\nguage models can self-correct with key condition ver-\nification. InProceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing,\npages 12846\u201312867, Miami, Florida, USA. Associa-\ntion for Computational Linguistics.\nAndrea Wynn, Harsh Satija, and Gillian Hadfield. 2025.\nTalk isn\u2019t always cheap: Understanding failure modes\nin multi-agent debate.Preprint, arXiv:2509.05396.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, Chujie Zheng, Day-\niheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao\nGe, Haoran Wei, Huan Lin, Jialong Tang, and 41\nothers. 2025. Qwen3 technical report.Preprint,\narXiv:2505.09388.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2023.\nReact: Synergizing reasoning and acting in language\nmodels.Preprint, arXiv:2210.03629.\nMiao Yu, Fanci Meng, Xinyun Zhou, Shilong Wang,\nJunyuan Mao, Linsey Pan, Tianlong Chen, Kun\nWang, Xinfeng Li, Yongfeng Zhang, Bo An, and"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "iao Yu, Fanci Meng, Xinyun Zhou, Shilong Wang,\nJunyuan Mao, Linsey Pan, Tianlong Chen, Kun\nWang, Xinfeng Li, Yongfeng Zhang, Bo An, and\nQingsong Wen. 2025a. A survey on trustworthy llm\nagents: Threats and countermeasures. InProceed-\nings of the 31st ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining V .2, KDD \u201925, page\n6216\u20136226, New York, NY , USA. Association for\nComputing Machinery.\nMiao Yu, Shilong Wang, Guibin Zhang, Junyuan Mao,\nChenlong Yin, Qijiong Liu, Kun Wang, Qingsong\nWen, and Yang Wang. 2025b. Netsafe: Exploring\nthe topological safety of multi-agent system. InFind-\nings of the Association for Computational Linguis-\ntics: ACL 2025, pages 2905\u20132938, Vienna, Austria.\nAssociation for Computational Linguistics.\nYanwei Yue, Guibin Zhang, Boyang Liu, Guancheng\nWan, Kun Wang, Dawei Cheng, and Yiyan Qi. 2025.\nMasrouter: Learning to route llms for multi-agent\nsystems. InProceedings of the 63rd Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 15549\u201315572. Asso-\nciation for Computational Linguistics.\nGuibin Zhang, Luyang Niu, Junfeng Fang, Kun Wang,\nLei Bai, and Xiang Wang. 2025a. Multi-agent archi-\ntecture search via agentic supernet. InProceedings\nof the 42nd International Conference on Machine\nLearning (ICML 2025). Oral.\nJiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng,\nXiong-Hui Chen, Jiaqi Chen, Mingchen Zhuge, Xin\nCheng, Sirui Hong, Jinlin Wang, Bingnan Zheng,\nBang Liu, Yuyu Luo, and Chenglin Wu. 2025b.\nAflow: Automating agentic workflow generation. In\nThe Thirteenth International Conference on Learning\nRepresentations (IC"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "lin Wang, Bingnan Zheng,\nBang Liu, Yuyu Luo, and Chenglin Wu. 2025b.\nAflow: Automating agentic workflow generation. In\nThe Thirteenth International Conference on Learning\nRepresentations (ICLR 2025). Oral.\nMichael JQ Zhang, W. Bradley Knox, and Eunsol Choi.\n2025c. Modeling future conversation turns to teach\nLLMs to ask clarifying questions. InThe Thirteenth\nInternational Conference on Learning Representa-\ntions.\nShaokun Zhang, Ming Yin, Jieyu Zhang, Jiale Liu,\nZhiguang Han, Jingyang Zhang, Beibin Li, Chi\nWang, Huazheng Wang, Yiran Chen, and Qingyun\nWu. 2025d. Which agent causes task failures and\nwhen? on automated failure attribution of LLM multi-\nagent systems. InForty-second International Confer-\nence on Machine Learning.\nKunlun Zhu, Hongyi Du, Zhaochen Hong, Xiaocheng\nYang, Shuyi Guo, Zhe Wang, Zhenhailong Wang,\nCheng Qian, Xiangru Tang, Heng Ji, and Jiaxuan\nYou. 2025. Multiagentbench: Evaluating the col-\nlaboration and competition of llm agents.Preprint,\narXiv:2503.01935.\nMingchen Zhuge, Wenyi Wang, Louis Kirsch,\nFrancesco Faccio, Dmitrii Khizbullin, and J\u00fcrgen\nSchmidhuber. 2024. GPTSwarm: Language agents\nas optimizable graphs. 235:62743\u201362767.A Algorithm Workflow\nThis appendix concisely instantiates the edge con-\ntroller in Eq. (2)under the constraint in Eq. (3).\nThe details can be seen in Section 4.\nB Case study of Error Taxonomy\nThe following traces provide concrete grounding\nfor the edge-level error taxonomy established in\nSection 3. Each case is presented in a consistent\nfour-column format as shown in Tables 3, 4, 5, and\n6, detailing theQuery, the original agent output\n(Origin), the clarification intervention byAgen-\ntAsk, and the resultingRevised Message. Fig-\nure 5 provides a visual overview and index of our"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": ", 5, and\n6, detailing theQuery, the original agent output\n(Origin), the clarification intervention byAgen-\ntAsk, and the resultingRevised Message. Fig-\nure 5 provides a visual overview and index of our\nedge-level error taxonomy. Its four quadrants each\nsketch a minimal, real trace exemplifying one of\nthe four core error types (i.e., DG, RD, SC, and\nCG), while the central donut reports their preva-\nlence in our annotated logs ( N=824). This figure\nserves as a conceptual guide and navigational aid,\ndirecting the reader to the corresponding compre-\nhensive case studies in Tables 3\u20136. These tables\nprovide the complete, granular detail of each inter-\naction, including the original faulty message, the\nprecise clarifying question posed byAgentAsk, and\nthe subsequent corrected handoff, enabling a thor-\nough examination of the error mitigation process.\nB.1 Data Gap (DG)\nMissing details at an edge force downstream agents\nto guess, which opens a cascade. The clarification\nasks for the exact boundary behavior that is absent\natxtand removes failure before the message flows\nfurther. The details can be seen in Table 3.\nB.2 Signal Corruption (SC)\nA wrong conversion or reference enters the chain\nand is relayed unchanged. The clarification pins\ndown the intended unit or base quantity, which\nstops the propagation locally and earns positive\neffectiveness per Eq. (7). Results are summarized\nin Table 4.\nB.3 Referential Drift (RD)\nAmbiguous symbols drift across turns and break\nconsistency. The clarification fixes names and in-\ndices at the edge and aligns all relations, after\nwhich the downstream solution becomes stable.\nThe details can be seen in Table 5.\nAlgorithm 1AgentAsk: SFT & E-GRPO Training\nRequire: LogsL, teacher J, init policy \u03c0\u03b8, ref\u03c0ref, budget B, lr\u03b7,\u03bbask,\u03bbR,\u03f5,\u03b2, weights wt, baseline b\n1:Stage A: Build SFT Corpus\n2:fortrajectory\u03c4\u2208 Ldo\n3:foredge statex i\u2208\u03c4do\n4:y i\u2190 J(x i) = (t i, vi, qi)\u25b7 t i\u2208 {DG,SC"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "2:fortrajectory\u03c4\u2208 Ldo\n3:foredge statex i\u2208\u03c4do\n4:y i\u2190 J(x i) = (t i, vi, qi)\u25b7 t i\u2208 {DG,SC,RD,CG,NONE}\n5: eD \u2190eD \u222a {(x i, yi)}\n6:end for\n7:end for\n8:Stage B: Supervised Fine-tuning (SFT)\n9:repeat\n10:Sample minibatchB \u2282 eD\n11:Compute headsptype\n\u03b8(t|x),paddr\n\u03b8(v|x, t),ptxt\n\u03b8(q|x, t)\n12:EvaluateL typeandL ask; setL SFT=L type+\u03bb askLask\n13:\u03b8\u2190\u03b8\u2212\u03b7\u2207 \u03b8LSFT\n14:untilvalidation stops improving\n15:Stage C: RL with E-GRPO\n16:repeat\n17:Init episode, counters, buffers; set budgetB\n18:fort= 1, . . . , Tdo\n19:Observex t= (xin\nt, ut, vt, mt, ht)\n20:Samplea t= (z t,\u02dcvt, qt)\u223c\u03c0 \u03b8(\u00b7 |x t)(schema+budget)\n21:Executea t; ifz t=1get replyr t; deliver tov t; obtainx t+1\n22:Computereff\nt, rpar\nt, rfmt\nt, redge\nt\n23:ifepisode not terminatedthen\n24:SampleGcandidates atx t; score byredge\nt\n25:Form local advantagesAloc\nt; update with ratios\u03c1 t(noR)\n26:else\n27:Compute terminalR; setAglob\nt\u2032\u2190w t\u2032(R\u2212b)for all visitedt\u2032\n28:Apply E-GRPO update usingAloc\nt\u2032andAglob\nt\u2032with KL to\u03c0 ref\n29:break\n30:end if\n31:end for\n32:untilRL budget exhausted or convergence\n33:return\u03c0 \u03b8\nB.4 Capability Gap (CG)\nThe role at the edge lacks the skill required by the\nsubtask. The clarification requests a reroute to a\ncapable recipient and specifies the expected output\nform, which resolves the mismatch with a single\nstep"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "4 Capability Gap (CG)\nThe role at the edge lacks the skill required by the\nsubtask. The clarification requests a reroute to a\ncapable recipient and specifies the expected output\nform, which resolves the mismatch with a single\nstep. Table 6 shows the corrected messages.\nC Supplementary Experiments\nThis section extends the main comparison in Ta-\nble 1 and the ablation in Table 2 with full per-\nframework summaries in Tables 7, 8, and 9. Ac-\ncuracy follows the reporting in Section 5. Latency\nand extra cost are normalized to each dataset origin\nas in the main text.Across the three consolidated tables, the\nlightweight clarifier consistently outperforms each\nframework\u2019s origin while staying far leaner than\n+GPT-5 . TheQwen-3-4B + E\u2013GRPO variant is the\nmost stable: it reliably improves over SFT, of-\nten closes most of the accuracy gap to +GPT-5 ,\nand keeps latency and extra cost close to origin .\nTheLlama-3.2-3B version follows the same pat-\ntern with slightly smaller gains. Task-wise, cod-\ning benchmarks (HumanEval, MBPP) show larger\nbenefits but also higher latency/cost than MMLU,\nreflecting longer chains and more clarifications.\nGSM8K and MATH never regress; the few de-\nclines are intentionally sparse, appear in other\ndatasets, and are not adjacent within any single\ncolumn. Overall,E\u2013GRPO never underperforms\nits corresponding SFT, delivering a better accu-\nracy\u2013efficiency trade-off,while +GPT-4o-mini\nsits predictably betweenoriginand+GPT-5.\nAdditionally, it should be noted that our Agen-\ntAsk is trained by plugging into MasRouter during\nthe training phase, and then plugging into the other\nbaseline implementations for experiments. Other-\nwise, we use temperature 0 for the LLMs in our\nexperiments to keep the reproduction.\nD Prompt\nThe complete instruction used by the clarifier is\ndelineated in Figure 6. It explicitly specifieswhen\nto ask,what to ask, andwhom to ask, while keeping\nquestions short and cost-aware.\nFigure 5:The case of our error taxonomy.In the middle shows the fraction of the four types of errors"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "6. It explicitly specifieswhen\nto ask,what to ask, andwhom to ask, while keeping\nquestions short and cost-aware.\nFigure 5:The case of our error taxonomy.In the middle shows the fraction of the four types of errors.\nYou are AgentAsk, an edge-level clarifier between two agents. Your job is to decide whether a minimal clarification\nshould be asked before the message is handed off, so that small mistakes do not spread. If you do ask, decide what\nto ask, whom to ask (sender or receiver), and write one short, concrete question. Keep it brief and low-cost. Do not\nchange the overall workflow.\nUse the following error taxonomy when asking. Choose one type if you decide to ask; if no clarification is needed, set\ntype=\"NONE\" and leave the question empty.\nDG \u2014 Data Gap\nSome required detail is missing, and the next agent would have to guess.\nSignals: missing boundary cases; absent IDs, keys, or columns; unclear ranges or formats; placeholders.\nHow to ask: request the smallest missing piece in one short question.\nWho to ask: usually the sender.\nDo not ask: when the value is obvious from context or does not affect the outcome.\nSC \u2014 Signal Corruption\nAn intermediate value, unit, scale, or structure is wrong or malformed and may be copied forward as truth.\nSignals: unit mismatch; off-by-one indices; broken JSON or tables; inconsistent time zones; impossible magnitudes.\nHow to ask: point to the exact field or structure and confirm or repair it.\nWho to ask: usually the sender; the receiver if they must choose a canonical unit.\nDo not ask: when downstream already normalizes it deterministically.\nRD \u2014 Referential Drift\nNames or symbols do not refer to the same thing across turns, so agents bind to different entities.\nSignals: pronouns like it or they; reused symbols without scope; conflicting aliases; index shifts; unlabeled columns.\nHow to ask: fix a single binding with a clear choice.\nWho to ask: usually the sender; the receiver if they must commit to a binding for later steps.\nDo not ask: when the binding is unambiguous from nearby context.\nCG \u2014 Capability Gap\nThe current addressee lacks the skill or role to complete the step.\nSignals: narrative text where computation is needed; missing tool access; math or code assigned to a planner; required\nAPI not available.\nHow to ask: propose"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "Capability Gap\nThe current addressee lacks the skill or role to complete the step.\nSignals: narrative text where computation is needed; missing tool access; math or code assigned to a planner; required\nAPI not available.\nHow to ask: propose a minimal reroute or ask for the needed computation in one line.\nWho to ask: the receiver if they must accept the reroute; otherwise, the sender to reissue with the right role.\nDo not ask: when a tiny hint lets the current role finish; in that case consider DG, RD, or SC first.\nNONE \u2014 no ask\nChoose NONE when a single short question will not meaningfully reduce uncertainty, when the handoff is already\nsufficient, when policy or privacy would block the question, or when the next agent can fix it deterministically without\nnew information. When using NONE, set to_agent to null and leave question empty. Output only the required JSON.\nFigure 6:Edge-level clarifier prompt.The box defines the instruction used by the clarifier to decide whether to\nask, what to ask, whom to ask, and how to phrase a single, minimal question under budget constraints.\nTable 3:AgentAsk Case (Data Gap).The table shows one code-oriented edge where an upstream message\nomits boundary behavior: the query asks for a shortest-palindrome constructor but the origin message assumes a\nnon-empty string and indexes without guards, creating an empty-input gap. Columns list the original query, the\nerroneous origin message, the minimal edge-level Ask (what/whom to ask), and the corrected handoff that adds\nexplicit length checks and a safe return for the empty string.\nQuery Origin (Data Gap) AgentAsk Revised Message\ndefis_palindrome(string:\nstr) ->bool:\n\"\"\"Test if given string\nis a palindrome.\"\"\"\nreturnstring ==\nstring[::-1]\ndefmake_palindrome(string:\nstr) ->str:\n\"\"\"Find the shortest\npalindrome that\nbegins with a\nsupplied string.\nAlgorithm idea:\n- Find the longest\npostfix of supplied\nstring that is a\npalindrome.\n- Append to the end the\nreverse of the prefix\nbefore that postfix.\n>>> make_palindrome('')\n\"\"\"I will implement\nmake_palindrome\ndirectly from the\ndescription. If the first\nand last characters\nmatch and the whole\nstring is already a\npalindrome, I can\nimmediately return\nit. Otherwise,"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "indrome('')\n\"\"\"I will implement\nmake_palindrome\ndirectly from the\ndescription. If the first\nand last characters\nmatch and the whole\nstring is already a\npalindrome, I can\nimmediately return\nit. Otherwise, I scan\nfrom right to left to\ndetect the longest\npalindromic suffix\nand then append the\nreverse of the missing\nprefix. Concretely, I\nstart with if string[0]\n== string[-1] and\nis_palindrome(string):\nreturn string, then\nproceed into a loop\nto locate the suffix\nboundary. This covers\ntypical inputs like\nwords and short\nphrases. The docstring\nexamples highlight\nthe construction for\nordinary strings, so I\nwill assume the input is\na standard non-empty\nstring and does not\nrequire a guard for\ncorner inputs such\nas the empty string\nor single-character\nstrings. With that in\nmind, I will finalize\nthe suffix search and\nproduce the shortest\npalindrome output\nwithout additional\nchecks or precondi-\ntions.AgentAsk \u2192 Pro-\ngrammingExpert:\n\u201cShould the function\ndefine behavior for the\nempty string explicitly\nand add a quick guard\nfor length 0 or 1 before\nindexingstring[0] ?\u201d\nThe current logic\nindexes string[0] and\nstring[-1] without\nguarding for length,\nwhich will raise Index-\nError on the empty\nstring and is unnec-\nessary on length-1\ninputs. I will add\nexplicit handling up\nfront: if len(string)\n== 0, return \u201d; if\nlen(string) == 1 or\nis_palindrome(string),\nreturn string. Only for\nlonger inputs will I\nsearch for the longest\npalindromic suffix and\nappend the reverse of\nthe missing prefix. This\npreserves correctness\non normal cases while\nensuring the edge case\nis well-defined and\nsafe.\nTable 4:AgentAsk Case (Signal Corruption).Two independent cases illustrate how a wrong conversion or\nmisapplied base quantity can propagate along an edge. Case 1 (pizza units): the origin message forces unit\nunification and converts teaspoons to cups unasked, whereas the Ask clarifies \u201creport as listed,\u201d yielding 26 cups + 8\nteaspoons (34 as count of listed quantities). Case 2"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "): the origin message forces unit\nunification and converts teaspoons to cups unasked, whereas the Ask clarifies \u201creport as listed,\u201d yielding 26 cups + 8\nteaspoons (34 as count of listed quantities). Case 2 (reading remainder): the origin treats Tuesday\u2019s1\n4as a fraction\nof the whole; the Ask pins it to the Monday remainder and yields 60 pages left.\nQuery Origin (Signal Corrup-\ntion)AgentAsk Revised Message\nTo make pizza, to-\ngether with other\ningredients, Kimber\nneeds 10 cups of\nwater, 16 cups of\nflour, and 1/2 times\nas many teaspoons\nof salt as the num-\nber of cups of flour.\nCalculate the com-\nbined total number\nof cups of water,\nflour, and teaspoons\nof salt that she\nneeds to make the\npizza.We have 10 cups of water\nand 16 cups of flour.\nSalt equals half as many\nteaspoons as flour cups,\nso that is1\n2\u00d716 = 8\nteaspoons. Because\nthe prompt asks for a\ncombined total in cups,\nI will convert teaspoons\nto cups and sum in a\nsingle unit for uniformity.\nUsing 1 cup = 48 tsp ,\nwe get 8/48 = 1/6 cup\n\u22480.1667 cup. Hence, the\ntotal is 10+16+1\n6= 261\n6\ncups\u224826.1667 cups. I\nwill report the answer in\ncups only to avoid mixing\nunits.AgentAsk \u2192Algo-\nrithmEngineer:\n\u201cShould teaspoons be\nconverted to cups be-\nfore calculating the to-\ntal?\u201d\nThe prompt specifies\ncups for water and flour\nand teaspoons for salt\nwithout requiring unifi-\ncation. To avoid in-\njecting an unstated pol-\nicy, I will preserve units\nas given and compute\nthe combined totalas\nlisted. Quantities: wa-\nter= 10 cups, flour =\n16cups, salt = 8 tea-\nspoons. Reporting the\ncombined count of listed\nquantities gives 34 (26+\n8).\nA book has 120\npages. On Monday,\na student reads one-\nthird of the book.\nOn Tuesday, the stu-\ndent reads one quar-\nter of theremain-\nder."}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "quantities gives 34 (26+\n8).\nA book has 120\npages. On Monday,\na student reads one-\nthird of the book.\nOn Tuesday, the stu-\ndent reads one quar-\nter of theremain-\nder. How many\npages are left un-\nread after Tuesday?I will compute the total\npages read by treating\nboth fractions as portions\nof the whole book for sim-\nplicity: Monday =1\n3\u00d7\n120 = 40 pages; Tuesday\n=1\n4\u00d7120 = 30 pages.\nTotal read = 70 pages, so\npages left = 120\u221270 =\n50. I will pass along \u201c50\npages left\u201d as the final\nquantity.AgentAsk \u2192Math-\nSolver:\n\u201cDoes \u2018one quarter\u2019 on\nTuesday refer to the re-\nmainder after Monday,\nrather than the whole\nbook?\u201d\nThanks for the clarifi-\ncation. \u201cOne quarter\u201d\nis with respect to\nthe remainder after\nMonday. Compute\nprecisely: Monday\nreads1\n3\u00d7120 = 40\npages; remainder\n= 120\u221240 = 80\npages. Tuesday\nreads1\n4\u00d780 = 20\npages. Total read\n= 40 + 20 = 60 ;\ntherefore pages left\n= 120\u221260 = 60 . I will\ncorrect the intermediate\nassumption and pass\nalong \u201c60 pages left\u201d\nas the final quantity.\nTable 5:AgentAsk Case (Referential Drift).A single math word-problem case where ambiguous references ( T, E\nwith dropped round indices) cause symbol drift across turns and wrong relations like T2=E 1\u221220 . The Ask\nenforces explicit indexing ( T1, T2, E1, E2) and confirms the intended relations ( T2=T1\u221220 ,E2= 2T 2), after\nwhich the downstream solution stabilizes .\nQuery Origin (Referential\nDrift)AgentAsk Revised Message\nIn April, Tank gath-\nered 10 more Easter\neggs than Emma in\ntheir first round of\negg hunt. Emma\ngathered twice as\nmany eggs as Tank\nin their second round,\nwhile Tank\u2019s second-\nround total was 20\nless than in the first\nround. The combined\npile with 6 other peo-\nple was 400 eggs,\nand Emma gathered"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "many eggs as Tank\nin their second round,\nwhile Tank\u2019s second-\nround total was 20\nless than in the first\nround. The combined\npile with 6 other peo-\nple was 400 eggs,\nand Emma gathered\n60 eggs in the sec-\nond round. Find\nhow many eggs the 6\nother egg hunters col-\nlected.MathAnalyst \u2192Mathe-\nmatician: Let TandE\ndenote Tank and Emma.\nI will use T1, E1for\nround one and T2, E2\nfor round two, but for\nbrevity I may drop the\nsubscripts. From the text,\n\u201cTank\u2019s second round\nwas 20 less than she\nhad gathered in the first\nround, \u201d so the difference\nbetween Tank\u2019s second\nand first rounds is 20.\nEmma\u2019s second round\nis \u201ctwice as many as\nTank in their second\nround, \u201d which suggests\na doubling relative\nto Tank\u2019s performance.\nSince Emma has 60 in the\nsecond round, we may\nsetE2= 60 . Because\nE1\u2212T 2= 20 , we can\nwrite T2=E 1\u221220 .\nAlso, \u201cTank gathered\n10 more than Emma in\ntheir first round\u201d gives\nT1=E 1+ 10 . With\nthese, the total across\ntwo rounds plus the six\nothers should reach 400;\nI pass these relations\nforward for solution.AgentAsk \u2192Math-\nAnalyst:\n\u201cPlease confirm un-\nambiguous referents:\nDoes \u201820 less\u2019 apply\nto Tank\u2019s own rounds\n(T2=T 1\u221220) ? Does\n\u2018twice as many\u2019 mean\nE2= 2T 2?\nFix symbols explicitly:\nT1, T2for Tank (rounds\n1,2) and E1, E2for\nEmma (rounds 1,2).\nMap statements to equa-\ntions: T1=E 1+ 10 ;\nT2 =T 1\u221220 ;\nE2= 2T 2; and the\ngiven E2= 60 . From\nE2= 60 we get T2= 30 .\nThen T1= 50 . Using\nT1=E 1+ 10 gives\nE1= 40 . The subtotal\nfor Tank and Emma is\nT1+"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "E2= 60 we get T2= 30 .\nThen T1= 50 . Using\nT1=E 1+ 10 gives\nE1= 40 . The subtotal\nfor Tank and Emma is\nT1+T 2+E 1+E 2=\n50+30+40+60 = 180 .\nTherefore, the six\nother hunters collected\n400\u2212180 = 220\neggs. The earlier drift\ncame from writing\nT2=E 1\u221220 , which\nincorrectly bound the\n\u201c20 less\u201d clause to\nEmma\u2019s first round;\nkeeping indices attached\neliminates the ambiguity\nand yields a consistent\nsolution.\nTable 6:AgentAsk Case (Capability Gap).Two cases where the role at the edge is misaligned with the subtask.\nCase 1 (tree growth): an Economist returns elapsed time (\u201c2 years\u201d) instead of the requested growth amount; the\nAsk reroutes to AlgorithmEngineer, who computes compound growth (21 meters). Case 2 (coding): a non-coding\nrole provides a qualitative description instead of an implementation; the Ask redirects to ProgrammingExpert, who\nsupplies a correct function and complexity note.\nQuery Origin (Capability Gap) AgentAsk Revised Message\nA certain tree\nwas 100 meters\ntall at the end\nof 2017. It will\ngrow 10% more\nthan its previous\nheight each year.\nHow long has\nthe tree grown\nfrom 2017 until\nthe end of 2019?Economist \u2192Engineer:\nThe question asks about the\nperiod from 2017 to the end\nof 2019, which spans two\ncalendar years. Interpret-\ning \u201chow long has the tree\ngrown\u201d as the elapsed time,\nwe simply count the years:\n2018 and 2019. Therefore,\nthe tree has grown for a to-\ntal of 2 years. The rele-\nvant arithmetic is duration-\nbased rather than quanti-\ntative growth, so the final\nanswer is 2, reflecting the\ntime interval rather than\nthe magnitude of height\nchange.AgentAsk \u2192Algo-\nrithmEngineer:\n\u201cThe user asks for the\ngrowth amount(height\nincrease), not the\nelapsed time. Please\nroute to Algorith-\nmEngineer or Scientist\nfor compound-growth\ncomputation. \u201d\nAlgorithmEngineer: We\ncompute the"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": ":\n\u201cThe user asks for the\ngrowth amount(height\nincrease), not the\nelapsed time. Please\nroute to Algorith-\nmEngineer or Scientist\nfor compound-growth\ncomputation. \u201d\nAlgorithmEngineer: We\ncompute the compound\ngrowth from end-2017\nbaseline H0= 100 .\nEach year multiplied\nby1.10. After two years:\nH2= 100\u00d7(1.10)2= 121\nmeters. The growth amount\nis\u2206H=H 2\u2212H 0=\n121\u2212100 = 21 meters.\nThus, the correct quantity\nis a height difference, not\na duration; the previously\nprovided \u201c2 years\u201d reflects\na capability mismatch\nrather than the requested\nquantitative outcome.\nFinal: the tree has grown\nby 21 meters over the\nperiod.\nWrite a function\nsum_even_squares\n(n: int) -> int\nthat returns the\nsum of squares\nof all even inte-\ngers from 1 to n\ninclusive. Pro-\nvide a concise\nimplementation\nand explain its\ncomplexity.MathAnalyst \u2192Scientist:\nTo address the request, we\nconsider how even numbers\nbehave in general and re-\ncall that squaring preserves\nparity. The sum can con-\nceptually be obtained by re-\npeatedly adding the square\nof each even number up to\nthe bound. The main idea\nis to iterate across the rele-\nvant subset and aggregate\nthe contributions. From a\nscientific perspective, the\ncomputation is straightfor-\nward and should be feasi-\nble, though nuances about\nperformance might emerge\nfor very large inputs. In\nsummary, the result equals\nthe accumulated squared\nvalues of all even integers\nno greater thann.AgentAsk \u2192Engi-\nneer:\n\u201cThe user explicitly re-\nquests an implementa-\ntion and complexity dis-\ncussion. Ensure the\nfunction handles small\nn. \u201d\ndefsum_even_squares(n:int) ->\nint:\n\"\"\"Return sum of k^2 for\neven k in [1..n].\"\"\"\nifn < 2:\nreturn0\n# Iterate by step 2 over\neven numbers only\nreturn sum(k*kforkin\nrange(2, n+1, 2))\nProgrammingExpert: The\nfunction iterates over even\nintegers only, avoiding un-\nnecessary checks, and sums\ntheir"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "2 over\neven numbers only\nreturn sum(k*kforkin\nrange(2, n+1, 2))\nProgrammingExpert: The\nfunction iterates over even\nintegers only, avoiding un-\nnecessary checks, and sums\ntheir squares. For example,\nn=6 uses{2,4,6} giving\n4+16+36 = 56 . Time com-\nplexity is O(n/2) =O(n)\nwithO(1) auxiliary space.\nA closed-form alternative\nviaP(2i)2= 4Pi2\nfori= 1..\u230an/2\u230b can\nreduce iteration count with\narithmetic. This reroute\nresolves the capability gap\nby assigning code synthesis\nto the appropriate role.\nSettings GSM8K MATH HumanEval MMLU MBPP\nAcc. Lat. Extra Acc. Lat. Extra Acc. Lat. Extra Acc. Lat. Extra Acc. Lat. Extra\norigin 92.18 100 0.0 51.41 100 0.0 90.63 100 0.0 81.26 100 0.0 76.81 100 0.0\n+GPT\u20134o\u2013mini 93.50 \u2191+1.32 117 16.0 51.84 \u2191+0.43 115 14.8 91.92 \u2191+1.29 121 17.6 82.00 \u2191+0.74 114 14.2 78.30 \u2191+1.49 123 18.5\n+GPT\u20135 94.11 \u2191+1.93 133 37.2 52.03 \u2191+0.62 131 35.8 92.20 \u2191+1.57 139 40.4 82.34 \u2191+1.08 129 33.8 79.05 \u2191+2.24 140 41.2\nAgentAsk (Llama\u20133.2\u20133B)\nSFT 92.60 \u2191+0.42 106 5.6 51.55 \u2191+0.14 106 5.4 91.25 \u2191+0.62 106 5.6 81.65 \u2191+0.39 105 5.0 76.70 \u2193"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "51.55 \u2191+0.14 106 5.4 91.25 \u2191+0.62 106 5.6 81.65 \u2191+0.39 105 5.0 76.70 \u2193-0.11 107 5.7\n(E\u2013GRPO) 92.88 \u2191+0.70 105 5.0 51.65 \u2191+0.24 105 5.1 91.50 \u2191+0.87 105 5.2 81.90 \u2191+0.64 104 4.7 77.05 \u2191+0.24 106 5.2\nAgentAsk (Qwen\u20133\u20134B)\nSFT 92.85 \u2191+0.67 106 5.5 51.41 0.00 106 5.3 91.40 \u2191+0.77 105 5.3 81.80 \u2191+0.54 105 5.0 77.05 \u2191+0.24 106 5.5\n(E\u2013GRPO) 92.98 \u2191+0.80 104 4.9 51.29 \u2193-0.12 104 4.8 91.71 \u2191+1.08 104 4.9 81.93 \u2191+0.67 104 4.6 77.23 \u2191+0.42 105 5.1\nTable 7:GPTSwarm: ablations and efficiency across five benchmarks.\nSettings GSM8K MATH HumanEval MMLU MBPP\nAcc. Lat. Extra Acc. Lat. Extra Acc. Lat. Extra Acc. Lat. Extra Acc. Lat. Extra\norigin 92.84 100 0.0 51.08 100 0.0 92.02 100 0.0 82.93 100 0.0 81.22 100 0.0\n+GPT\u20134o\u2013mini 93.95 \u2191+1.11 118 15.7 51.80 \u2191+0.72 116 14.9 92.80 \u2191+0.78 120 16.6 83.40 \u2191+0.47 115 14.3 82.10 \u2191+0.88 122"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": ".72 116 14.9 92.80 \u2191+0.78 120 16.6 83.40 \u2191+0.47 115 14.3 82.10 \u2191+0.88 122 17.3\n+GPT\u20135 94.62 \u2191+1.78 134 37.9 52.21 \u2191+1.13 132 36.8 93.41 \u2191+1.39 138 39.8 83.59 \u2191+0.66 130 34.9 82.77 \u2191+1.55 139 40.6\nAgentAsk (Llama\u20133.2\u20133B)\nSFT 92.90 \u2191+0.06 106 5.4 51.35 \u2191+0.27 106 5.2 91.90 \u2193-0.12 107 5.6 83.05 \u2191+0.12 105 4.9 81.60 \u2191+0.38 107 5.6\n(E\u2013GRPO) 93.00 \u2191+0.16 105 5.0 51.60 \u2191+0.52 105 5.0 92.20 \u2191+0.18 106 5.3 83.18 \u2191+0.25 104 4.7 81.85 \u2191+0.63 106 5.3\nAgentAsk (Qwen\u20133\u20134B)\nSFT 92.84 0.00 106 5.4 51.90 \u2191+0.82 106 5.3 92.10 \u2191+0.08 106 5.5 82.85 \u2193-0.08 105 5.0 81.70 \u2191+0.48 106 5.4\n(E\u2013GRPO) 92.72 \u2193-0.12 104 4.8 52.31 \u2191+1.23 104 4.8 92.63 \u2191+0.61 104 4.9 83.21 \u2191+0.28 104 4.6 81.95 \u2191+0.73 105 5.0\nTable 8:MaAS: ablations and efficiency across five benchmarks.\nSettings GSM8K MATH HumanEval M"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "+0.28 104 4.6 81.95 \u2191+0.73 105 5.0\nTable 8:MaAS: ablations and efficiency across five benchmarks.\nSettings GSM8K MATH HumanEval MMLU MBPP\nAcc. Lat. Extra Acc. Lat. Extra Acc. Lat. Extra Acc. Lat. Extra Acc. Lat. Extra\norigin 93.26 100 0.0 51.52 100 0.0 91.03 100 0.0 83.19 100 0.0 79.04 100 0.0\n+GPT\u20134o\u2013mini 94.62 \u2191+1.36 118 16.0 52.10 \u2191+0.58 118 15.7 92.10 \u2191+1.07 122 17.3 83.50 \u2191+0.31 116 14.6 81.10 \u2191+2.06 121 17.9\n+GPT\u20135 95.34 \u2191+2.08 134 38.0 52.49 \u2191+0.97 133 37.5 92.90 \u2191+1.87 140 41.0 83.78 \u2191+0.59 131 35.8 82.44 \u2191+3.40 139 40.2\nAgentAsk (Llama\u20133.2\u20133B)\nSFT 93.64 \u2191+0.38 106 5.7 51.85 \u2191+0.33 107 5.8 91.35 \u2191+0.32 106 5.6 83.25 \u2191+0.06 106 5.2 79.80 \u2191+0.76 106 5.9\n(E\u2013GRPO) 94.23 \u2191+0.97 105 5.0 51.95 \u2191+0.43 106 5.4 91.50 \u2191+0.47 105 5.2 83.35 \u2191+0.16 105 5.0 80.20 \u2191+1.16 105 5.4\nAgentAsk (Qwen\u20133\u20134B)\nSFT 93.99 \u2191+0.73 105 5.3 51"}
{"file": "2510.07593v1.pdf", "section": "6 Conclusion", "chunk": "5.0 80.20 \u2191+1.16 105 5.4\nAgentAsk (Qwen\u20133\u20134B)\nSFT 93.99 \u2191+0.73 105 5.3 51.90 \u2191+0.38 106 5.2 91.45 \u2191+0.42 105 5.1 83.10 \u2191+0. -09 104 4.8 80.10 \u2191+1.06 104 4.9\n(E\u2013GRPO) 94.72 \u2191+1.46 103 4.2 52.07 \u2191+0.55 104 4.6 91.55 \u2191+0.52 104 4.5 82.86 \u2193-0.33 103 4.1 80.64 \u2191+1.60 103 4.3\nTable 9:MasRouter: ablations and efficiency across five benchmarks."}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "Traceability and Accountability in Role-Specialized\nMulti-Agent LLM Pipelines\nAmine Barrak\nDepartment of Computer Science and Engineering\nOakland University, Rochester, MI, USA\naminebarrak@oakland.edu\nAbstract\u2014Sequential multi-agent systems built with large lan-\nguage models (LLMs) can automate complex software tasks,\nbut they are hard to trust because errors quietly pass from\none stage to the next. We study a traceable and accountable\npipeline, meaning a system with clear roles, structured handoffs,\nand saved records that let us trace who did what at each\nstep and assign blame when things go wrong. Our setting is\na Planner\u2192Executor\u2192Critic pipeline. We evaluate eight\nconfigurations of three state-of-the-art LLMs on three bench-\nmarks and analyze where errors start, how they spread, and\nhow they can be fixed. Our results show: (1) adding a structured,\naccountable handoff between agents markedly improves accuracy\nand prevents the failures common in simple pipelines; (2) models\nhave clear role-specific strengths and risks (e.g., steady planning\nvs. high-variance critiquing), which we quantify with repair\nand harm rates; and (3) accuracy\u2013cost\u2013latency trade-offs are\ntask-dependent, with heterogeneous pipelines often the most\nefficient. Overall, we provide a practical, data-driven method\nfor designing, tracing, and debugging reliable, predictable, and\naccountable multi-agent systems.\nIndex Terms\u2014Multi-agent LLMs, Sequential pipelines, Role\nBased Reasoning, Agents Collaboration, Traceable Pipeline.\nI. INTRODUCTION\nThe field of software engineering is undergoing a signif-\nicant transformation, driven by the rise of Large Language\nModels (LLMs). This evolution is progressing from LLM-\npowered \u201cco-pilots\u201d that assist developers to fully autonomous,\nmulti-agent systems capable of tackling complex software\ndevelopment tasks with minimal human intervention [1, 2].\nA common architectural pattern is thesequential multi-agent\npipeline, where specialized LLM-based agents collaborate in\na predefined order to perform roles such as planning, devel-\nopment, and testing [3]. Systems like ChatDev and MetaGPT\nhave demonstrated the potential of this approach, where task\ndecomposition allows multiple agents to synergistically solve\nproblems that would be intractable for a single monolithic\nmodel [4, 5"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "]. Systems like ChatDev and MetaGPT\nhave demonstrated the potential of this approach, where task\ndecomposition allows multiple agents to synergistically solve\nproblems that would be intractable for a single monolithic\nmodel [4, 5].\nHowever, this shift to sequential agent pipelines intro-\nduces a critical software engineering challenge: the loss of\ntransparency and the immense difficulty of debugging. When\nagents are chained together, an error introduced by an early\nagent can silently cascade, corrupting the entire workflow and\nleading to a final output that is incorrect for reasons that are\ndifficult to diagnose [3]. As illustrated in Figure 1, this turns\nthe collaborative pipeline into a new kind of opaque system\nwhere failures are observable, but their origins are not. With\nThis is wrong !\nWho is responsible\nfor this error ?!\nLLM-A LLM-B LLM-Y LLM-Z\nWorked\nSolution\nLLM-B LLM-Y\nCritique\n& Fixes \u2026 Large Language Models Agents Pool\nRole Assignment\n(Planner, Developer, Tester, \u2026)\nLLM-A\nSolution\nOutline Planner Executor Critic\nFinal\nAnswer\nUser Prompt /\nTask Spec\nFig. 1: A sequential multi-agent pipeline. A failure in the final\noutput makes debugging and assigning responsibility a critical\nchallenge.\ndevelopers already spending over 50% of their time on debug-\nging traditional software, the introduction of these complex,\nnon-deterministic agentic systems threatens to exacerbate this\nchallenge significantly [6]. Debugging a sequential pipeline of\nagents is therefore a crucial, yet largely unsolved, problem\nstanding in the way of their reliable deployment [7, 8].\nWhile the potential of multi-agent systems is well-\ndocumented, the engineering principles required to make them\ntrustworthy and maintainable are still nascent [9]. Much of the\nexisting research on systems like ChatDev [4], MetaGPT [5],\nand other debugging agents [6] focuses on demonstrating the\ncapabilities of agent collaboration. While these studies are\nfoundational, they often treat the pipeline as a black box,\nevaluating only the final output. Consequently, they do not\nprovide a systematic analysis of the internal system dynamics\nor the root causes of failure. This is a critical omission, as\nrecent work has shown that multi-agent systems can unex-\npectedly underperform strong, yet simpler, single-agent base"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "provide a systematic analysis of the internal system dynamics\nor the root causes of failure. This is a critical omission, as\nrecent work has shown that multi-agent systems can unex-\npectedly underperform strong, yet simpler, single-agent base-\nlines due to coordination overhead and these very cascading\nerrors [10]. The software engineering community has long\nrecognized traceability as a cornerstone of building robust\nsystems [11], yet this principle has not been systematically\napplied to the internal workings of agentic pipelines. This\ngap has recently been framed as the challenge of \u201cautomated\nfailure attribution\u201d\u2014identifying which agent is responsible for\na failure\u2014a task that is critical for debugging but remains a\nlabor-intensive and underexplored problem [12, 13].arXiv:2510.07614v1  [cs.AI]  8 Oct 2025\nTo address this gap, this paper presents an in-depth empir-\nical study of anaccountable sequential pipelineof LLM-\nbased agents. Rather than proposing a new framework, our\ncontribution is a rigorous analysis of the internal dynamics of\nsuch a pipeline. We employ a blame attribution methodology\nto monitor the correctness of a solution as it passes through a\nPlanner\u2192Executor\u2192Criticsequence. This allows\nus to quantify novel, role-specific behaviors such asrepair\n(when an agent corrects an error from a preceding stage) and\nharm(when an agent introduces an error to a previously\ncorrect state). Through a large-scale study involving three\nstate-of-the-art LLMs in eight distinct pipeline configurations\nacross three diverse benchmarks, we systematically investigate\nthe following research questions:\n\u2022RQ1:How do sequential multi-agent pipelines compare\nto the performance of monolithic LLMs?\n\u2022RQ2:How do individual LLMs perform in specialized\nroles, and what are the dynamics of error propagation and\ncorrection within the pipeline?\n\u2022RQ3:What are the trade-offs between accuracy, cost,\nand latency when designing accountable sequential agent\npipelines?\nWe release the dataset used in this study.1\nII. RELATEDWORK\nOur work sits at the intersection of multi-agent LLM sys-\ntems for software development and verifiable feedback loops\nfor agentic workflows, addressing a key gap: the lack of a\nformal, traceable, and debuggable pipeline design.\nA. Multi-Agent Collaboration"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "intersection of multi-agent LLM sys-\ntems for software development and verifiable feedback loops\nfor agentic workflows, addressing a key gap: the lack of a\nformal, traceable, and debuggable pipeline design.\nA. Multi-Agent Collaboration in Autonomous Software Engi-\nneering\nThe inherent limitations of single, monolithic LLMs in\nhandling complex, multi-step tasks are well-established. A\nprominent issue is the \u201dlost in the middle\u201d problem, where\nmodels exhibit degraded performance in utilizing information\npositioned in the center of long context windows [14]. This\nhas catalyzed a shift towards multi-agent systems, where a\ncomplex problem is decomposed and delegated to a society of\nspecialized, collaborative agents.\nWithin software engineering, this paradigm has been suc-\ncessfully instantiated by frameworks that simulate entire devel-\nopment organizations. For instance,ChatDev[4] constructs a\nvirtual software company where LLM agents embody roles\nlike CEO, programmer, and tester, following a structured\nwaterfall process to develop software. Similarly,MetaGPT\n[5] employs agents as product managers and architects, man-\ndating the generation of standardized artifacts like requirement\ndocuments and system designs before implementation begins.\nThis structured approach is facilitated by general-purpose\nframeworks likeAutoGen[15], which provides a toolkit for\norchestrating these complex multi-agent conversations. These\nsystems convincingly demonstrate the feasibility of role-based\ndecomposition for complex software generation.\n1https://sites.google.com/view/mas-gain2025/homeThe choice of LLM for each specialized agent is a critical\ndesign decision with economic and performance implications.\nStronger models deliver higher accuracy and reasoning but\nincur greater cost and latency, while lighter models are faster\nand cheaper but less effective for planning or critical analysis\n[16]. This trade-off compels careful role allocation to balance\ncost, speed, and quality [3]. To automate this process, routing\nframeworks have emerged; for example,CARGOintroduces\na confidence-aware system that dynamically assigns prompts\nto the most suitable model, managing the performance\u2013cost\ntrade-off without manual intervention [17].\nHowever, these frameworks often model entire organizations\nwith complex interaction patterns. In our study, we abstract\naway from specific organizational structures to investigate the\nfundamental effectiveness of a pipelined LLM workflow. We\npropose to isolate and analyze a core, general"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "often model entire organizations\nwith complex interaction patterns. In our study, we abstract\naway from specific organizational structures to investigate the\nfundamental effectiveness of a pipelined LLM workflow. We\npropose to isolate and analyze a core, generalizable process\nby splitting a task into three distinct stages\u2014Plan, Execution,\nand Critic. To formally study the impact of this role-based\ndecomposition, we evaluate its accuracy, cost, and latency\nacross a range of complex tasks foundational to modern\nsoftware engineering.\nB. Traceability and Correction in Agentic Pipelines\nBeyond end-to-end simulation, a parallel stream of research\nhas focused on the feedback mechanisms within agentic work-\nflows. TheSelf-Refine[18] framework established that an\nLLM can iteratively improve its own output by generating\nself-feedback and then acting upon it. This principle has been\nadapted for coding in systems likeSelf-Debug[19], where an\nagent is prompted to first analyze and explain a bug before\nattempting a fix. The role of a critic is further exemplified in\nautomated code review agents [20] and in workflows inspired\nby Test-Driven Development.\nWhile these approaches validate the concept of a critique-\nand-refine loop, a simple pipeline is not sufficient for building\nrobust and reliable systems. A primary challenge in current\nagentic systems is the lack ofobservabilityandtraceability\n[3, 7]. When a pipeline produces a final, incorrect answer, it\nis often difficult to perform root cause analysis and identify\nwhich stage or agent introduced the error. This \u201dblack box\u201d\nnature hinders debugging and iterative improvement. Further-\nmore, recent work on iterative refinement for ML pipelines\nhas shown that modifying and evaluating one component at a\ntime leads to more stable and interpretable improvements [21].\nThis highlights the need for modularity, where a specific LLM\nin the pipeline can be evaluated and replaced if it proves to be\na weak link. The concept of using multiple, diverse verifiers\nto evaluate an output also suggests that a robust critique stage\nis multi-faceted and crucial for reliability [22].\nOur work directly addresses this gap. We contend that a\npipeline must be designed for traceability to track the evolution\nof decisions and artifacts at each stage. This not only facilitates\ndebugging and the modular replacement of underperforming\nagents but also enables early-stage"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "this gap. We contend that a\npipeline must be designed for traceability to track the evolution\nof decisions and artifacts at each stage. This not only facilitates\ndebugging and the modular replacement of underperforming\nagents but also enables early-stage error correction. By evalu-\nating the output of each stage, our pipeline allows a subsequent\nagent to identify and rectify issues before they propagate.\n2\nIII. DESIGNSTUDY\nTo evaluate the effectiveness of the traceable multi-agent\npipeline, we conducted a controlled study across multiple\ndatasets, model configurations, and prompt conditions. This\nsection outlines the design choices and experimental setup.\nA. Prompts Datasets Collection\nWe curated multiple-choice datasets, retaining only prompts\nwith valid ground-truth answers to ensure consistent evaluation\nof accuracy across role configurations and blame attribu-\ntions. All prompts were aligned to the standard answer space\n({A, B, C, D, E}).\nWe evaluate on three complementary benchmarks:\n\u2022PythonIO(127 prompts): programming and algorithmic\nreasoning problems expressed in natural language, stress-\ning systematic reasoning and code-like logic [23].\n\u2022LogiQA(283 prompts): formal logic and deductive rea-\nsoning tasks; originally released by Liu et al. (2020) and\npublicly available via GitHub [24].\n\u2022AGIEval(263 prompts): standardized test\u2013style ques-\ntions (math, reading, commonsense); derived from public\nexam data (e.g. Gaokao, SAT) and repurposed in AGIEval\nbenchmark [25].\nB. Model Selection Rationale\nFor this study, we selected three frontier LLMs that held top\npositions on public leaderboards during our evaluation period:\n\u2022GPT-4o (A).Reported at the top of the Chatbot Arena\n(LMSYS) overall leaderboard in mid\u20132024 [26].Noted\nfor stable, low-variance responses and strong text+code\nreasoning.\n\u2022Claude 3.5 Sonnet (B).Public reports placed it at the\ntop of key LMSYS Chatbot Arena categories shortly after\nrelease (June 2024) [27].Noted for strict instruction-\nfollowing and effective multi-step revision/correction.\n\u2022Gemini 2.5 Pro (C).Covered as leading the LMArena\n(human-preference) leaderboard in early 202"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": ") [27].Noted for strict instruction-\nfollowing and effective multi-step revision/correction.\n\u2022Gemini 2.5 Pro (C).Covered as leading the LMArena\n(human-preference) leaderboard in early 2025 [28].Noted\nfor efficient high-capability reasoning and coding with\nreliable schema compliance in MCQ settings.\nC. Experimental Conditions\nWe compare three inference regimes:\n(1) Single-model baselines. Each model answers end-to-end\nwith the prompt above. These runs establish individual capa-\nbility and serve as reference points.\n(2) Pipelines without traceability. We compose three-step\npipelines (length3) using ordered model triples from\n{A, B, C}, e.g.,AAA,BBB,CCC,ABC,ACB, . . .. Each\nstep consumes the previous step\u2019s artifact (question and current\nanswer) and emits a new one-letter answer. We score only the\nfinal output.\n(3) Pipelines with traceability (blame tracking). We instru-\nment the same configurations to capture stagewise inputs/out-\nputs and deltas against gold labels. For each item we com-\npute whether the intermediate steprepaireda wrong answer,\nharmeda correct answer, or performed ano-op.D. Evaluation Metrics\nTo conduct a comprehensive evaluation of both monolithic\nmodels and the sequential agent pipelines, we compute a\nset of quantitative metrics designed to capture effectiveness,\nefficiency, and operational cost.\nToken statistics We collect detailed token usage for each\nmodel interaction, including the number of input (prompt)\ntokens and output (completion) tokens. This data is essential\nfor analyzing the complexity of the tasks given to each agent\nand serves as the basis for cost computation. We report the\nmedian to provide a robust measure of token consumption.\nAccuracy This is the primary metric for evaluating the cor-\nrectness of the final output. For each task, the generated\nanswer is compared against a ground-truth solution. Accuracy\nis calculated as the percentage of tasks for which the model\nor pipeline produced the correct final answer.\nCost Computation To assess the operational expense of each\napproach, we compute the monetary cost. LetR in(m)and\nRout(m)be the provider\u2019s prices (USD) per 1,000 tokens\nfor input and output respectively for modelm. For a dataset\nD, with total prompt tokensT"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "the monetary cost. LetR in(m)and\nRout(m)be the provider\u2019s prices (USD) per 1,000 tokens\nfor input and output respectively for modelm. For a dataset\nD, with total prompt tokensT in(m, D)and total completion\ntokensT out(m, D)produced by modelm, the exact total cost\nwe report is:\nWhen per-call cost is needed, it is computed itemwise as\nprompti\n1000Rin(m) +completioni\n1000Rout(m)and summarized by the\nmedian across items. For the sequential multi-agent pipeline,\nthe total cost for a single task is the sum of the costs incurred\nby each agent:\nPipeline Cost=Cost planner +Cost executor +Cost critic\nAll reported monetary totals were computed with the fixed\nper-token rates detailed in Table I.\nTABLE I: Pricing comparison of major LLMs with input and\noutput costs per 1K tokens.\nModelInput\n(USD / 1K tokens)Output\n(USD / 1K tokens)Source\nGPT-4o (A) $0.0050 $0.0200 [29]\nClaude Sonnet 4 (B) $0.0030 $0.0150 [30]\nGemini 2.5 Pro (C) $0.00125 $0.0100 [31]\nLatency statistics To measure the efficiency and responsive-\nness of each configuration, we record the end-to-end latency\nfor each task in seconds. This includes the time taken for all\nAPI calls and intermediate processing within the pipeline. We\nreport the median latency to provide a stable measure of the\nsystem\u2019s response time.\nE. Blame Assignment in Multi-Agent Pipelines\nWe use a three\u2013role pipeline: aPlannerproposes an\ninitial answer, anExecutorsolves the task conditioned on\nthe planner\u2019s output, and aCriticreviews or revises the\nexecutor\u2019s answer. The pipeline publishes a single final answer\nby preferring later stages (critic\u2192executor\u2192planner).\nTo analyze accountability, we define ablame functionthat\n3\ncompares each stage\u2019s answer to the ground truth and sets\nbinary flags indicating whether a downstream stagerepaired\na wrong upstream answer orharmeda correct one; theerror\noriginis the earliest stage whose mistake remains unrepaired\nin the final output (NONE"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "\u2019s answer to the ground truth and sets\nbinary flags indicating whether a downstream stagerepaired\na wrong upstream answer orharmeda correct one; theerror\noriginis the earliest stage whose mistake remains unrepaired\nin the final output (NONE if the final answer is correct). The\noperational logic is summarized below.\nAlgorithm 1Traceable Pipeline (Roles and Blame Logic)\nRequire:DatasetD={(x i, yi)}N\ni=1; modelsM P(Planner),\nME(Executor),M C(Critic)\nEnsure:For eachi: stage answers(P, E, C), finalF, blame\nflags, error origin\n1:fori\u21901toNdo\n2:P\u2190M P(xi)\n3:E\u2190M E(xi, P)\n4:C\u2190M C(xi, P, E)\n5:Final selection:\n6:ifCis definedthen\n7:F\u2190C\n8:else ifEis definedthen\n9:F\u2190E\n10:else\n11:F\u2190P\n12:end if\n13:Blame flags:\n14:planner_error[i]\u2190(P\u0338=y i)\n15:executor_repair[i]\u2190(P\u0338=y i)\u2227(E=y i)\n16:executor_harm[i]\u2190(P=y i)\u2227(E\u0338=y i)\n17:critic_repair[i]\u2190(E\u0338=y i)\u2227(C=y i)\n18:critic_harm[i]\u2190(E=y i)\u2227(C\u0338=y i)\n19:Error origin:\n20:ifF=y ithen\n21:origin[i]\u2190NONE\n22:else\n23:if(E=y i)\u2227(C\u0338=y i)then\n24:origin[i]\u2190CRITIC\n25:else if(P=y i)\u2227(E\u0338=y i)then\n26:origin[i]\u2190EXECUTOR\n27:else\n28:origin[i]\u2190PLANNER\n29:end if\n30:end if\n31:Record(P, E, C, F,flags,origin[i])\n32:end for\nIV. RESULTS\nThis section presents the empirical findings from our eval-\nuation of monolithic and sequential multi-agent pipeline con-\nfigurations across three distinct benchmarks: AgiEval (general\nreasoning), PythonIO"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "origin[i])\n32:end for\nIV. RESULTS\nThis section presents the empirical findings from our eval-\nuation of monolithic and sequential multi-agent pipeline con-\nfigurations across three distinct benchmarks: AgiEval (general\nreasoning), PythonIO (code generation), and LogiQA (logical\nreasoning). The results are organized to systematically address\nour three primary research questions.A. RQ1: How do sequential multi-agent pipelines compare to\nthe performance of monolithic LLMs?\nTo justify the use of multi-agent systems, we first estab-\nlish a performance baseline by evaluating each LLM as a\nmonolithic, end-to-end problem solver. We then compare these\nresults against two types of sequential pipelines\u2014a simple,\nunmonitored pipeline and an accountable pipeline with a\nstructured handoff protocol\u2014to determine the impact of both\ncollaboration and accountability.\n1) Baseline Performance of Monolithic Models\nWe began by assessing the individual capabilities of our three\nselected LLMs\u2014Gemini 2.5 Pro (Model C), Claude Sonnet 4\n(Model B), and GPT-4o (Model A)\u2014on each benchmark. The\nresults, shown in Table II, summarizes the accuracy, median\nlatency, and median cost per prompt for each model.\nTABLE II: Baseline Performance of Single Models Across All\nDatasets\nDataset Model Accuracy (%)Median\nLatency (s)Median\nCost/Prompt (USD)\nAgiEvalGemini 2.5 Pro (C) 92.40 11.84 $0.0112\nClaude Sonnet 4 (B) 58.20 4.21 $0.0036\nGPT-4o (A) 43.70 0.74 $0.0006\nPythonIOGemini 2.5 Pro (C) 99.21 11.20 $0.0120\nClaude Sonnet 4 (B) 53.54 5.48 $0.0054\nGPT-4o (A) 69.29 0.71 $0.0009\nLogiQAGemini 2.5 Pro (C) 84.45 14.78 $0.0134\nClaude Sonnet 4 (B) 79.51 6.74 $0.0051\nGPT-4o ("}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "2.5 Pro (C) 84.45 14.78 $0.0134\nClaude Sonnet 4 (B) 79.51 6.74 $0.0051\nGPT-4o (A) 67.49 0.77 $0.0009\nThe results in Table II establish a clear performance hierar-\nchy, with Gemini 2.5 Pro representing the best-case scenario\nfor a single-model, monolithic approach. While its accuracy\nis formidable, the increasing complexity of agentic AI tasks is\nbeginning to challenge the limits of single-agent systems, par-\nticularly for long-horizon problems that require intricate plan-\nning and reasoning [32, 33]. Fields such as autonomous soft-\nware development and multi-step scientific discovery present\nsignificant hurdles for monolithic models [1]. For such intri-\ncate problems, a monolithic model faces significant hurdles:\nit can struggle to maintain context across numerous steps,\nits reasoning process for intermediate sub-problems remains\nopaque, and a single early error can irrevocably corrupt the\nentire solution without any mechanism for verification or\nrecovery. This paradigm of growing task complexity motivates\nour investigation into a decomposed, multi-agent pipeline. The\ncentral hypothesis is not to compete with the raw accuracy\nof the best single model on constrained benchmarks, but to\npropose amodular and verifiable architecturesuited for\ncomplex, multi-step processes. By assigning specialized roles\n(e.g., planning, execution, critique) to different agents, we can\nconstruct systems designed for this purpose.\n4\n2) Impact of Accountable Protocol on Pipeline Performance\nHaving established the monolithic baselines, we next investi-\ngated the impact of the pipeline structure. We compared two\narchitectures: aSimple Pipeline, where the output of one\nagent is passed to the next, and anAccountable Pipeline,\nwhich implements a structured handoff protocol with blame\nattribution. This protocol ensures that each agent\u2019s output is\nvalidated and passed in a consistent format, giving subsequent\nagents a clear state to evaluate and act upon.\nTable III presents a direct comparison of the final accuracy\nof these two pipeline types across all configurations.\nTABLE III: Comparative Accuracy of Simple vs. Accountable\nPipelines Across Datasets (%) \u2014 rows by configuration.\nDataset Config Simple Pipeline Accountable Pipeline Accuracy\u2206\nAgi"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "of the final accuracy\nof these two pipeline types across all configurations.\nTABLE III: Comparative Accuracy of Simple vs. Accountable\nPipelines Across Datasets (%) \u2014 rows by configuration.\nDataset Config Simple Pipeline Accountable Pipeline Accuracy\u2206\nAgiEval\nAAA 57.41 77.19 +19.78\nABC 81.37 90.87 +9.50\nBAB 88.97 86.69\u22122.28\nBBB 88.97 89.73 +0.76\nBCC 91.25 90.49\u22120.76\nCBA 87.07 92.40 +5.33\nCCA 91.25 92.40 +1.15\nCCC 91.63 93.54 +1.91\nPythonIO\nAAA 73.23 88.98 +15.75\nABC 99.21 99.21 0.00\nBAB 63.78 96.85 +33.07\nBBB 61.42 97.64 +36.22\nBCC 99.21 98.43\u22120.78\nCBA 71.65 99.21 +27.56\nCCA 71.65 98.43 +26.78\nCCC 99.21 99.21 0.00\nLogiQA\nAAA 71.48 71.38\u22120.10\nABC 79.58 80.57 +0.99\nBAB 78.52 79.86 +1.34\nBBB 79.58 79.51\u22120.07\nBCC 82.75 83.39 +0.64\nCBA 81.34 83.75 +2.41\nCCA 78.52 82.28 +3.76\nCCC 81.34 85.16 +3.82\nThe data in Table III shows that the introduction of the\naccountable protocol leads to notable changes in pipeline\naccuracy, with the magnitude and direction of the change\nvarying by configuration and dataset.\nOn the AgiEval benchmark, the largest accuracy increase\nwas observed in the AAA configuration, which rose by 19.78\npercentage points from 57.41% to 77.19%. The ABC config-\nuration also saw a significant gain of 9.50"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": ", the largest accuracy increase\nwas observed in the AAA configuration, which rose by 19.78\npercentage points from 57.41% to 77.19%. The ABC config-\nuration also saw a significant gain of 9.50 points. Most other\nconfigurations showed smaller positive changes, such as CCC\n(+1.91 points) and CBA (+5.33 points). Conversely, the BABand BCC configurations saw minor decreases of 2.28 and 0.76\npoints, respectively.\nOn the PythonIO benchmark, the accuracy gains were\neven more substantial for several configurations. The BBB\nconfiguration\u2019s accuracy increased by 36.22 percentage points,\nfrom 61.42% to 97.64%. Similarly large gains were recorded\nfor BAB (+33.07 points), CBA (+27.56 points), and CCA\n(+26.78 points). The top-performing configurations, ABC and\nCCC, showed no change, as both the simple and accountable\nversions achieved 99.21% accuracy. Only one configuration,\nBCC, showed a small decrease of 0.78 points.\nFor the LogiQA benchmark, the changes were generally\npositive but more modest. The largest improvement was seen\nin the CCA configuration, which increased by 6.99 percentage\npoints. Other configurations like CCC and CBA saw smaller\ngains of 3.82 and 2.41 points, respectively. The AAA and\nBBB configurations showed negligible negative changes of -\n0.10 and -0.07 points.\nAgiEval PythonIO LogiQA020406080100Accuracy (%)Monolithic (Gemini)\nAAA (Simple)AAA (Accountable)\nStrong Accountable (best of CCC/CBA)\nFig. 2: Impact of pipeline architecture on accuracy.\nFigure 2 provides a visual summary of these key perfor-\nmance trade-offs, comparing the monolithic baseline against\nthe weakest and strongest pipeline configurations. The chart\nclearly illustrates three core findings across the datasets.\nFirst, the AAA (Simple) pipeline consistently underperforms,\ndemonstrating the significant risk of \u201danti-synergy\u201d where\nunstructured collaboration is actively harmful. Second, the\nAAA (Accountable) pipeline shows a dramatic performance\nimprovement over its simple counterpart on AgiEval and\nPythonIO, highlighting the effectiveness of the accountable\nprotocol in raising the performance floor. Finally,"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "actively harmful. Second, the\nAAA (Accountable) pipeline shows a dramatic performance\nimprovement over its simple counterpart on AgiEval and\nPythonIO, highlighting the effectiveness of the accountable\nprotocol in raising the performance floor. Finally, the Strong\nAccountable pipeline (representing the best of the CCC and\nCBA configurations) consistently performs at or near the level\nof the best monolithic model, and in the case of LogiQA,\nslightly surpasses it.\nFindings for RQ1\nUnstructured pipelines degrade performance.Without\nclear roles and checks, errors compound across stages and\naccuracy falls below a competent single model.\nAccountability lifts the floor and ceiling.A struc-\ntured, accountable handoff reliably improves accuracy and\nstability, with strong accountable pipelines matching or\nsurpassing weaker monolithic baselines.\n5\nB. RQ2: How do individual LLMs perform in specialized\nroles, and what are the dynamics of error propagation and\ncorrection within the pipeline?\nTo addressRQ2, we analyzed the internal dynamics of error\norigination and correction.\n1) Error Origination: The Primacy of the Planner\nOur analysis reveals that the vast majority of unrecoverable\npipeline failures originate from a flawed plan created in the\nvery first step. To quantify this, we measured the error rate for\neach model when it was assigned the Planner role. The \u201dTotal\nCases\u201d in Table IV represents the total number of times each\nmodel acted as the Planner across all relevant configurations\nfor a given dataset. For example, since GPT-4o (Model A) was\nthe Planner in two configurations (AAA and ABC), its total\ncases for the AgiEval benchmark (which has 263 instances) is\n2 * 263 = 526.\nTABLE IV: Planner Error Rate by Model and Dataset.\nPlanner Model Dataset Total CasesErrors\nIntroducedError Rate\n(%)\nGemini 2.5 Pro (C)AgiEval 789 58 7.35\nPythonIO 381 3 0.79\nLogiQA 849 129 15.19\nClaude Sonnet 4 (B)AgiEval 789 106 13.43\nPythonIO 381 15 3.94\nLogiQA 849 187 22.03\nGPT-4o (A)AgiEval"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "4 (B)AgiEval 789 106 13.43\nPythonIO 381 15 3.94\nLogiQA 849 187 22.03\nGPT-4o (A)AgiEval 526 213 40.49\nPythonIO 254 28 11.02\nLogiQA 566 162 28.62\nThe results in Table 3 show a clear and consistent perfor-\nmance hierarchy among the models in the Planner role: Gemini\nis the most reliable, followed by Claude, with GPT-4o being\nthe least reliable. This trend holds across all three datasets.\nThe error rates are also highly task-dependent. On the\nstructured PythonIO coding task, all models performed ex-\nceptionally well as planners. Gemini\u2019s error rate was less than\n1%, and even the weakest planner, GPT-4o, only introduced an\nerror in 11.02% of cases. This suggests that for well-defined\ntasks like code generation, the planning stage is less prone to\nfailure.\nIn contrast, performance degraded significantly on the more\nabstract reasoning tasks. On LogiQA, all models struggled\nmore, with Claude\u2019s error rate climbing to 22.03% and GPT-\n4o\u2019s to 28.62%. The most challenging benchmark for planners\nwas AgiEval, where GPT-4o\u2019s error rate reached a substantial\n40.49%.\n2) Mid-stream Dynamics: Quantifying Repair and Harm\nWhile the Planner sets the stage, the Executor and Critic\nroles determine the pipeline\u2019s resilience. Our blame attributionmethodology allows us to measure two key behaviors: the\nrepair rate (the frequency with which an agent corrects an error\nfrom the preceding stage) and the harm rate (the frequency\nwith which an agent incorrectly modifies a correct output from\nthe preceding stage). Table V summarizes these behavioral\nmetrics, aggregated across all experiments.\nTABLE V: Role-Specific Repair and Harm Rates by Model\n(Aggregated).\nModel RoleRepair Rate\n(%)Harm Rate\n(%)\nGemini 2.5 Pro (C)Executor 1.27 0.25\nCritic 2.66 0.25\nClaude Sonnet 4 (B)Executor 10.01 0.25\nCritic 3.04 1.90\nGPT-"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "27 0.25\nCritic 2.66 0.25\nClaude Sonnet 4 (B)Executor 10.01 0.25\nCritic 3.04 1.90\nGPT-4o (A)Executor 2.28 1.33\nCritic 5.20 0.89\nThe data in Table V reveals distinct behavioral profiles for\neach model in the mid-stream roles.\nGemini 2.5 Pro (Model C) exhibited the lowest harm rates\nacross both roles, at just 0.25%. Its repair rates were also\nthe lowest for an Executor (1.27%) and modest for a Critic\n(2.66%).\nClaude Sonnet 4 (Model B) was the most effective Executor,\nachieving a repair rate of 10.01%, which was significantly\nhigher than any other model in that role. It maintained a low\nharm rate of 0.25% as an Executor. In the Critic role, its repair\nrate was 3.04%, while its harm rate was the highest recorded\nfor that role at 1.90%.\nGPT-4o (Model A) showed the highest repair rate in the\nCritic role, at 5.20%. As an Executor, its repair rate was 2.28%,\nbut it had the highest harm rate in that role at 1.33%.\nFindings for RQ2\nPlanner primacy.The Planner has the highest leverage;\nits error rate is the strongest predictor of pipeline failure.\nRole-specific aptitudes.Models show distinct strengths.\nGemini is a reliable generator but weak corrector; Claude\nis an excellent Executor; GPT-4o is a high-variance, high-\nreward Critic.\nData-driven casting.Cast roles by strengths,Gemini\n(Planner)\u2192Claude(Executor)\u2192GPT-4o(Critic).\nC. RQ3: What are the trade-offs between accuracy, cost,\nand latency when designing accountable sequential agent\npipelines?\nWhile maximizing accuracy is a primary goal, real-world\ndeployments are constrained by operational budgets and re-\nsponse time requirements. Figure 3 visualizes the multi-\ndimensional trade-off space for our accountable pipeline con-\nfigurations across the three benchmarks, plotting accuracy\n6\n0.010 0.015 0"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "operational budgets and re-\nsponse time requirements. Figure 3 visualizes the multi-\ndimensional trade-off space for our accountable pipeline con-\nfigurations across the three benchmarks, plotting accuracy\n6\n0.010 0.015 0.020 0.025 0.030 0.035 0.040\nCost per Prompt ($)788082848688909294Final Accuracy (%)\nAAA (4.74s)ABC (16.22s)\nBAB (8.21s)BBB (9.57s)\nBCC (25.71s)CBA (18.12s)\nCCA (26.54s)CCC (34.82s)\nPareto frontier\n4.717.234.8\nLatency (s)AgiEval\n0.010 0.015 0.020 0.025 0.030 0.035 0.040 0.045\nCost per Prompt ($)9092949698Final Accuracy (%)\nAAA (2.18s)ABC (7.97s)\nBAB (4.83s)BBB (5.21s)BCC (15.52s)CBA (14.02s)\nCCA (20.07s)CCC (25.01s)\nPareto frontier\n2.211.025.0\nLatency (s) PythonIO\n0.005 0.010 0.015 0.020 0.025 0.030 0.035 0.040\nCost per Prompt ($)7274767880828486Final Accuracy (%)\nAAA (2.22s)ABC (8.10s)\nBAB (4.79s)\nBBB (5.19s)BCC (15.53s)CBA (14.06s)CCA (20.06s)\nCCC (24.99s)\nPareto frontier\n2.211.125.0\nLatency (s) LogiQA\nFig. 3: Accuracy vs. cost with latency color-coded; Pareto frontier shown (dashed).\nagainst cost, with latency represented by the color of each\npoint. The dashed line in each plot indicates the Pareto frontier,\nrepresenting the set of configurations that are optimal because\nno other configuration offers higher accuracy at a lower or\nequal cost.\nThe plots in Figure 3 reveal that the optimal configuration"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": ". The dashed line in each plot indicates the Pareto frontier,\nrepresenting the set of configurations that are optimal because\nno other configuration offers higher accuracy at a lower or\nequal cost.\nThe plots in Figure 3 reveal that the optimal configuration\nis highly dependent on the specific task. On AgiEval, there\nis a clear trade-off between accuracy and cost. The CCC\nconfiguration achieves the highest accuracy (93.54%) but is\nalso the most expensive. Configurations like CBA and CCA\nsit on the Pareto frontier, offering near-peak accuracy (92.40%)\nat a significantly lower cost, making them excellent balanced\nchoices. The fastest and cheapest configurations, such as\nAAA and BAB, offer lower accuracy but may be suitable for\napplications where speed and cost are the primary constraints.\nOn PythonIO, the trade-off dynamics are different. Sev-\neral configurations (ABC, CBA, CCC) achieve near-perfect\naccuracy of 99.21%. As the plot shows, these top-tier con-\nfigurations are spread out horizontally, indicating that the\nkey decision is no longer about gaining accuracy but about\nminimizing cost and latency. The ABC configuration is the\nmost efficient of this group, while the CCC configuration is\nby far the most expensive for no additional accuracy gain,\nplacing it well inside the Pareto frontier.\nOn LogiQA, the Pareto frontier is again clearly visible,\nshowing a direct relationship between cost and accuracy. The\nCCA configuration provides the highest accuracy at 85.51%.\nThe CBA configuration offers a slightly lower accuracy\n(83.75%) but is significantly cheaper, making it a strong\nalternative on the frontier. As with the other datasets, the\nAAA configuration is the most economical but also the least\naccurate.\nAcross all three datasets, the plots illustrate that there is\nno single \u201dbest\u201d configuration. The choice depends on the\nspecific requirements of the application, balancing the need for\naccuracy with the constraints of cost and latency. Furthermore,\nthe analysis highlights the value of heterogeneous pipelines, as\nconfigurations like CBA and CCA frequently represent optimal\npoints on the Pareto frontier.Findings for RQ3\nTrade-offs are task-dependent.The optimal balance of\naccuracy, cost, and latency varies by task, reflected in the\ndiffering Pareto frontiers across datasets.\nHeterogeneous pipelines offer robustness.A mixed\nconfiguration"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "for RQ3\nTrade-offs are task-dependent.The optimal balance of\naccuracy, cost, and latency varies by task, reflected in the\ndiffering Pareto frontiers across datasets.\nHeterogeneous pipelines offer robustness.A mixed\nconfiguration such asCBAdelivers strong, balanced per-\nformance and often lies on the Pareto frontier.\nAccountability has a price.The accountable protocol\nadds traceability and stability but can increase operational\ncost.\nV. DISCUSSION ANDTHREATS TOVALIDITY\nOur results show that accountability improves stability and\ninterpretability, but it comes with trade-offs. In some cases,\nstrict handoffs limit the flexibility of later agents, leading\nto small accuracy decreases. Structured outputs and logging\nalso add overhead: compared to simple monolithic baselines,\naccountable pipelines increased costs by about 2\u20133\u00d7and\nraised median latency from\u223c2s in lightweight settings to 20\u2013\n25s in heavier ones (roughly 8\u201310\u00d7slower). Model choice\nfurther shapes this balance: stronger models yield higher\naccuracy but incur higher cost and more latency, while lighter\nmodels improve efficiency at the risk of reduced reasoning\ndepth. Designing reliable pipelines therefore requires tuning\nrole assignments to the task and deployment budget.\nThese findings should be interpreted in light of three\nmain validity concerns. First,construct validity:we rely on\nmultiple-choice benchmarks (PythonIO, LogiQA, AGIEval),\nwhich simplify open-ended tasks into discrete answers. This\nmakes blame attribution feasible but may not reflect the full\ncomplexity of real-world software engineering. Second,inter-\nnal validity:blame assignment assumes access to ground-truth\nlabels, not always available at runtime; practical deployments\nwould require proxies such as self-consistency checks, verifier\nagents, or unit tests. Finally,external validity:results are\nbased on three proprietary LLMs during a fixed evaluation\nwindow. Model updates may alter performance, though we\nrelease prompts, logs, and evaluation dates to support replica-\ntion and encourage extension to open-source models.\n7\nVI. CONCLUSION\nWe present a large-scale study that opens the black box\nof sequential multi-agent LLM pipelines. Across three bench-\nmarks we compare monolithic models, simple pipelines, and\naccountable pipelines. Key insights: (1) accountability is es-\nsential; structured handoffs prevent failure cascades and can"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "sequential multi-agent LLM pipelines. Across three bench-\nmarks we compare monolithic models, simple pipelines, and\naccountable pipelines. Key insights: (1) accountability is es-\nsential; structured handoffs prevent failure cascades and can\nraise accuracy by more than 36 points (PythonIO), (2) role\nspecialization is real; planner quality dominates (best planner\n7.35% error vs worst 40.49%) and models differ by role\n(Executor repair 10.01%, Critic repair 5.20%), which supports\ndata driven casting, and (3) trade-offs among accuracy, cost,\nand latency are task dependent, with heterogeneous pipelines\noften on the Pareto frontier. We move from black box evalu-\nation to a glass box engineering approach, with metrics and\nmethods that help diagnose, debug, and optimize multi-agent\nsystems for robust, predictable performance.\nVII. ACKNOWLEDGEMENT\nThis work was conducted in collaboration withzuvu.ai, an\nindustrial partner developing AI-powered tools.\nREFERENCES\n[1] J. He, C. Treude, and D. Lo, \u201cLlm-based multi-agent systems for\nsoftware engineering: Literature review, vision, and the road ahead,\u201d\nACM Transactions on Software Engineering and Methodology, vol. 34,\nno. 5, pp. 1\u201330, 2025.\n[2] H. Jin, L. Huang, H. Cai, J. Yan, B. Li, and H. Chen, \u201cFrom llms to llm-\nbased agents for software engineering: A survey of current, challenges\nand future,\u201darXiv preprint arXiv:2408.02479, 2024.\n[3] L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen,\nJ. Tang, X. Chen, Y . Linet al., \u201cA survey on large language model based\nautonomous agents,\u201dFrontiers of Computer Science, vol. 18, no. 6, p.\n186345, 2024.\n[4] C. Qian, X. Cong, C. Yang, W. Chen, Y . Su, J. Xu, Z. Liu, and\nM. Sun, \u201cCommunicative agents for software development,\u201darXiv\npreprint arXiv:"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "ian, X. Cong, C. Yang, W. Chen, Y . Su, J. Xu, Z. Liu, and\nM. Sun, \u201cCommunicative agents for software development,\u201darXiv\npreprint arXiv:2307.07924, vol. 6, no. 3, p. 1, 2023.\n[5] S. Hong, M. Zhuge, J. Chen, X. Zheng, Y . Cheng, C. Zhang, J. Wang,\nZ. Wang, S. K. S. Yau, Z. Linet al., \u201cMetagpt: Meta programming for\na multi-agent collaborative framework.\u201d International Conference on\nLearning Representations, ICLR, 2024.\n[6] C. Lee, C. S. Xia, L. Yang, J.-t. Huang, Z. Zhu, L. Zhang, and M. R.\nLyu, \u201cA unified debugging approach via llm-based multi-agent synergy,\u201d\narXiv preprint arXiv:2404.17153, 2024.\n[7] W. Epperson, G. Bansal, V . Dibia, A. Fourney, J. Gerrits, E. Zhu,\nand S. Amershi, \u201cInteractive debugging and steering of multi-agent ai\nsystems,\u201d inProceedings of the 2025 CHI Conference on Human Factors\nin Computing Systems, ser. CHI \u201925, 2025.\n[8] A. Z. Yang, C. Le Goues, R. Martins, and V . Hellendoorn, \u201cLarge\nlanguage models for test-free fault localization,\u201d inProceedings of the\n46th IEEE/ACM International Conference on Software Engineering,\n2024, pp. 1\u201312.\n[9] K. Ronanki, \u201cFacilitating trustworthy human-agent collaboration in llm-\nbased multi-agent system oriented software engineering,\u201d inProceedings\nof the 33rd ACM International Conference on the Foundations of\nSoftware Engineering, 2025, pp. 1333\u20131337.\n[10] M. Z. Pan, M. Cemri, L. A. Agrawal, S. Yang, B. Chopra, R. Tiwari,\nK. Keutzer, A. Parameswaran, K. Ramchandran, D. Kleinet al.,"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "ri, L. A. Agrawal, S. Yang, B. Chopra, R. Tiwari,\nK. Keutzer, A. Parameswaran, K. Ramchandran, D. Kleinet al., \u201cWhy\ndo multiagent systems fail?\u201d inICLR 2025 Workshop on Building Trust\nin Language Models and Applications, 2025.\n[11] J. Cleland-Huang, O. C. Gotel, J. Huffman Hayes, P. M \u00a8ader, and\nA. Zisman, \u201cSoftware traceability: trends and future directions,\u201d in\nFuture of software engineering proceedings, 2014, pp. 55\u201369.\n[12] S. Zhang, M. Yin, J. Zhang, J. Liu, Z. Han, J. Zhang, B. Li, C. Wang,\nH. Wang, Y . Chenet al., \u201cWhich agent causes task failures and when? on\nautomated failure attribution of llm multi-agent systems,\u201darXiv preprint\narXiv:2505.00212, 2025.[13] S. Triantafyllou, A. Singla, and G. Radanovic, \u201cOn blame attribution\nfor accountable multi-agent sequential decision making,\u201dAdvances in\nNeural Information Processing Systems, vol. 34, pp. 15 774\u201315 786,\n2021.\n[14] N. F. Liu, K. Lin, J. Hewitt, A. Paranjape, M. Bevilacqua,\nF. Petroni, and P. Liang, \u201cLost in the middle: How language\nmodels use long contexts,\u201dTransactions of the Association for\nComputational Linguistics, vol. 12, pp. 157\u2013173, 2024. [Online].\nAvailable: https://aclanthology.org/2024.tacl-1.9/\n[15] Q. Wu, G. Bansal, J. Zhang, Y . Wu, B. Li, E. Zhu, L. Jiang, X. Zhang,\nS. Zhang, J. Liuet al., \u201cAutogen: Enabling next-gen llm applications via\nmulti-agent conversations,\u201d inFirst Conference on Language Modeling,\n2024.\n[16] W. Chen, Y . Su, J. Zuo, C. Yang, C. Yuan, C."}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": ": Enabling next-gen llm applications via\nmulti-agent conversations,\u201d inFirst Conference on Language Modeling,\n2024.\n[16] W. Chen, Y . Su, J. Zuo, C. Yang, C. Yuan, C. Qian, C.-M. Chan,\nY . Qin, Y . Lu, R. Xieet al., \u201cAgentverse: Facilitating multi-agent col-\nlaboration and exploring emergent behaviors in agents,\u201darXiv preprint\narXiv:2308.10848, vol. 2, no. 4, p. 6, 2023.\n[17] A. Barrak, Y . Fourati, M. Olchawa, E. Ksontini, and K. Zoghlami,\n\u201cCargo: A framework for confidence-aware routing of large language\nmodels,\u201d inProceedings of the 35th IEEE International Conference on\nCollaborative Advances in Software and Computing (CASCON). IEEE,\n2025.\n[18] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe,\nU. Alon, N. Dziri, S. Prabhumoye, Y . Yanget al., \u201cSelf-refine: Iter-\native refinement with self-feedback,\u201dAdvances in Neural Information\nProcessing Systems, vol. 36, pp. 46 534\u201346 594, 2023.\n[19] X. Chen, M. Lin, N. Schaerli, and D. Zhou, \u201cTeaching large language\nmodels to self-debug,\u201d inThe 61st Annual Meeting Of The Association\nFor Computational Linguistics, 2023.\n[20] R. Tufano, L. Pascarella, M. Tufano, D. Poshyvanyk, and G. Bavota,\n\u201cTowards automating code review activities,\u201d in2021 IEEE/ACM 43rd\nInternational Conference on Software Engineering (ICSE). IEEE, 2021,\npp. 163\u2013174.\n[21] E. Xue, Z. Huang, Y . Ji, and H. Wang, \u201cImprove: Iterative model pipeline\nrefinement and optimization leveraging llm agents,\u201darXiv preprint\narXiv:2502.18530, 2025.\n[22"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": ", Y . Ji, and H. Wang, \u201cImprove: Iterative model pipeline\nrefinement and optimization leveraging llm agents,\u201darXiv preprint\narXiv:2502.18530, 2025.\n[22] S. Lifshitz, S. A. McIlraith, and Y . Du, \u201cMulti-agent verification: Scaling\ntest-time compute with multiple verifiers (abridged),\u201d inWorkshop on\nReasoning and Planning for Large Language Models.\n[23] Z. Zhang, Z. Jiang, L. Xu, H. Hao, and R. Wang, \u201cMultiple-choice\nquestions are efficient and robust llm evaluators,\u201darXiv preprint\narXiv:2405.11966, 2024.\n[24] J. Liu, L. Cui, H. Liu, D. Huang, Y . Wang, and Y . Zhang, \u201cLogiqa:\nA challenge dataset for machine reading comprehension with logical\nreasoning,\u201darXiv preprint arXiv:2007.08124, 2020.\n[25] W. Zhong, R. Cui, Y . Guo, Y . Liang, S. Lu, Y . Wang, A. Saied, W. Chen,\nand N. Duan, \u201cAgieval: A human-centric benchmark for evaluating\nfoundation models,\u201darXiv preprint arXiv:2304.06364, 2023.\n[26] R. Hart. (2024, July) What ai is the best? chatbot\narena relies on millions of votes. Forbes coverage of\nChatbot Arena overall leaders (GPT-4o listed first). [On-\nline]. Available: https://www.forbes.com/sites/roberthart/2024/07/18/\nwhat-ai-is-the-best-chatbot-arena-relies-on-millions-of-human-votes/\n[27] S. Sharma. (2024, June) Anthropic\u2019s claude 3.5 sonnet surges\nto top of ai rankings in lmsys chatbot arena, challenging\nindustry giants. [Online]. Available: https://venturebeat.com/ai/\nanthropic-claude-3-5-sonnet-surges-to-top-of-ai-rankings\n[28] U. Shakir. (2025, March) Google says its new \u2019"}
{"file": "2510.07614v1.pdf", "section": "Introduction", "chunk": "https://venturebeat.com/ai/\nanthropic-claude-3-5-sonnet-surges-to-top-of-ai-rankings\n[28] U. Shakir. (2025, March) Google says its new \u2019reasoning\u2019 gemini\nai models are the best ones yet. Mentions Gemini 2.5 Pro leading\nLMArena with a 39 Elo point margin. [Online]. Available: https://www.\ntheverge.com/news/635502/google-gemini-2-5-reasoning-ai-model\n[29] OpenAI. (2025) Api pricing. [Online]. Available: https://openai.com/\napi/pricing/\n[30] Anthropic. Claude sonnet 4 api pricing. [Online]. Available: https:\n//www.anthropic.com/news/1m-context\n[31] Google AI. (2025) Gemini developer api pricing. [Online]. Available:\nhttps://ai.google.dev/gemini-api/docs/pricing\n[32] G. Wang, Y . Xie, Y . Jiang, A. Mandlekar, C. Xiao, Y . Zhu, L. Fan, and\nA. Anandkumar, \u201cChain of agents: Large language models collaborating\non long-context tasks,\u201darXiv preprint arXiv:2404.18260, 2024.\n[33] A. Mushtaq, M. R. Naeem, I. Ghaznavi, M. I. Taj, I. Hashmi,\nand J. Qadir, \u201cHarnessing multi-agent llms for complex engineer-\ning problem-solving: A framework for senior design projects,\u201darXiv\npreprint arXiv:2501.01205, 2025.\n8"}
{"file": "2510.07733v1.pdf", "section": "Introduction", "chunk": "SurveyG: A Multi-Agent LLM Framework with Hierarchical\nCitation Graph for Automated Survey Generation\nMinh-Anh Nguyen\u2217\nMinh-Duc Nguyen\u2217\nCollege of Engineering and Computer\nScience, VinUniversity\nViet NamNguyen Thi Ha Lan\u2217\nFPT University\nViet NamKieu Hai Dang\nCollege of Engineering and Computer\nScience, VinUniversity\nViet Nam\nNguyen Tien Dong\nCollege of Engineering and Computer\nScience, VinUniversity,\nApplied Institue Technology\nViet NamLe Duy Dung\nCollege of Engineering and Computer\nScience, VinUniversity\nViet Nam\nAbstract\nLarge language models (LLMs) are increasingly adopted for au-\ntomating survey paper generation [ 9,10,15,16,20]. Existing ap-\nproaches typically extract content from a large collection of related\npapers and prompt LLMs to summarize them directly. However,\nsuch methods often overlook the structural relationships among\npapers, resulting in generated surveys that lack a coherent taxon-\nomy and a deeper contextual understanding of research progress.\nTo address these shortcomings, we proposeSurveyG, an LLM-\nbased agent framework that integrateshierarchical citation graph,\nwhere nodes denote research papers and edges capture both cita-\ntion dependencies and semantic relatedness between their contents,\nthereby embedding structural and contextual knowledge into the\nsurvey generation process. The graph is organized into three layers:\nFoundation,Development, andFrontier, to capture the evolu-\ntion of research from seminal works to incremental advances and\nemerging directions. By combining horizontal search within lay-\ners and vertical depth traversal across layers, the agent produces\nmulti-level summaries, which are consolidated into a structured\nsurvey outline. A multi-agent validation stage then ensures consis-\ntency, coverage, and factual accuracy in generating the final survey.\nExperiments, including evaluations by human experts and LLM-\nas-a-judge, demonstrate that SurveyG outperforms state-of-the-art\nframeworks, producing surveys that are more comprehensive and\nbetter structured to the underlying knowledge taxonomy of a field.\nCCS Concepts\n\u2022Computing methodologies\u2192Information extraction.\n\u2217Both authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page"}
{"file": "2510.07733v1.pdf", "section": "Introduction", "chunk": "or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nConference acronym \u2019XX, Woodstock, NY\n\u00a92018 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 978-1-4503-XXXX-X/2018/06\nhttps://doi.org/XXXXXXX.XXXXXXXKeywords\nAutomated Survey Generation, Large Language Model, Literature\nSynthesis, Multi-agent, Hierarchical Graph Representation\nACM Reference Format:\nMinh-Anh Nguyen, Minh-Duc Nguyen, Nguyen Thi Ha Lan, Kieu Hai\nDang, Nguyen Tien Dong, and Le Duy Dung. 2018. SurveyG: A Multi-Agent\nLLM Framework with Hierarchical Citation Graph for Automated Survey\nGeneration. InProceedings of Make sure to enter the correct conference title\nfrom your rights confirmation email (Conference acronym \u2019XX).ACM, New\nYork, NY, USA, 12 pages. https://doi.org/XXXXXXX.XXXXXXX\n1 Introduction\nThe exponential growth of research publications, particularly in\nrapidly evolving fields such as Artificial Intelligence [ 7], has made it\nincreasingly difficult for researchers to keep pace with new develop-\nments [ 15,16]. While survey papers serve as invaluable resources by\nsynthesizing existing knowledge and identifying emerging trends,\ntheir manual construction is costly, time-consuming, and often\nunable to keep up with the overwhelming influx of literature [ 9].\nAlthough large language models (LLMs) offer promising text gener-\nation capabilities, they face critical limitations in handling massive\nreference sets, maintaining academic rigor, and providing up-to-\ndate knowledge [ 6,18]. These challenges underscore the urgent\nneed for an automated survey generation framework that can effi-\nciently retrieve, organize, and synthesize literature into coherent,\nhigh-quality surveys tailored to users\u2019 research interests.\nSome recent studies [ 9,10,"}
{"file": "2510.07733v1.pdf", "section": "1 Introduction", "chunk": "These challenges underscore the urgent\nneed for an automated survey generation framework that can effi-\nciently retrieve, organize, and synthesize literature into coherent,\nhigh-quality surveys tailored to users\u2019 research interests.\nSome recent studies [ 9,10,15,20] have proposed autonomous\nsurvey generation frameworks based on user queries, following\nthe basic pipeline illustrated in Figure 1. While these approaches\nrepresent promising progress, they exhibit two key limitations.\nFirstly,they neglect the relationships between papers, such as\ncitation links, methodological connections, or subtopic dependen-\ncies, which are essential for understanding how works build upon\none another, improve over foundational methods, and collectively\nshape research trends.Secondly,these frameworks employ a naive\nstrategy for constructing structured outlines or full survey papers,\nsimply concatenating summaries of individual papers. This not\nonly exacerbates the long-context problem in LLMs but also fails\nto exploit the hierarchical organization of related works within\nsubtopics.arXiv:2510.07733v1  [cs.AI]  9 Oct 2025\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Minh-Anh et al.\nDatabaseI want to find\npapers in topic\nKnowledge Graph\nEmbedding\nFind rel evant\npaper Summarize\npaperStructured\nOutlineFull paper Evaluation\nFigure 1: Overview of the standard automated survey\npipeline, which involves three core stages: (1) preparing rele-\nvant papers, (2) generating a structured outline that defines\nsections and subsections, and (3) composing the full survey.\nCoverageStructure\nRelevance\nSynthesis\nCritical Analysis20406080100\nAutosurvey\nSurveyX\nSurveyForge\nSurveyG\nFigure 2: Evaluation of generated surveys across multiple\nmetrics using LLM-as-a-judge, validated by human experts.\nTo address these limitations, we proposeSurveyG. This au-\ntonomous survey generation system emphasizes knowledge repre-\nsentation of retrieved papers and employs hierarchical summariza-\ntion to construct well-structured outlines, which are an essential\ncomponent of high-quality surveys. In detail, we design an LLM-\nbased multi-agent framework that represents knowledge using a\nhierarchical citation graph, where nodes correspond to papers and\nedges capture both citation relationships and semantic similarity.\nThe graph is organized into three layers:Foundation, Develop-\nment, andFront"}
{"file": "2510.07733v1.pdf", "section": "1 Introduction", "chunk": "LLM-\nbased multi-agent framework that represents knowledge using a\nhierarchical citation graph, where nodes correspond to papers and\nedges capture both citation relationships and semantic similarity.\nThe graph is organized into three layers:Foundation, Develop-\nment, andFrontierto reflect the progression of research from\nseminal contributions to incremental improvements and emerging\ndirections. By combining horizontal searches within layers and ver-\ntical traversals across layers, our framework generates multi-aspect\nsummaries that are subsequently consolidated into a structured\nsurvey outline via a multi-agent framework.\nWe evaluate SurveyG on 10 computer science topics from the\nSurGE benchmark [ 10], comparing its survey generation perfor-\nmance with existing state-of-the-art frameworks [ 9,15,20]. Fol-\nlowing prior work, we assess outlines along five dimensions:Cov-\nerage,Structure,Relevance,Synthesis,Critical Analysis. Details of\nthe experimental setup are provided in Section 4. As shown in Fig-\nure 2, while baseline methods achieve reasonable performance in\nRelevance and partially in Structure and Coverage, they perform\nsignificantly worse in Synthesis and Critical Analysis, primarilydue to their inability to model inter-paper relationships and the\nlimitations imposed by long-context inputs.\nIn conclusion, this work presents three key contributions to\nautomated survey generation. First, we introduce a hierarchical\ncitation graph representation that models both citation and seman-\ntic relationships among papers. Second, we develop a graph-based\ntraversal mechanism that operates across this hierarchy to produce\ndiverse and multi-aspect summarizations, effectively capturing the\nmethodological foundations, developmental trends, and frontier\ndirections of a research field. Third, we design a multi-agent frame-\nwork that combines retrieval-augmented generation (RAG) with\npre-built hierarchical summaries as memory, allowing the system to\nautomatically construct coherent and comprehensive survey drafts\ngrounded in verifiable evidence.\n2 Related works\n2.1 Long-form Text Generation\nLLMs have achieved remarkable progress, yet generating long-\nform, coherent, and logically structured documents remains a per-\nsistent challenge [ 2,3,6]. Recent works have explored different\nstrategies to address the long-context problem. For example, Chain-\nof-Agents [ 21] introduces a multi-agent collaboration framework\nwhere worker agents process segmented portions of text and a man-\nager agent synthesizes them into coherent outputs, alleviating focus"}
{"file": "2510.07733v1.pdf", "section": "2.1 Long-form Text Generation", "chunk": "address the long-context problem. For example, Chain-\nof-Agents [ 21] introduces a multi-agent collaboration framework\nwhere worker agents process segmented portions of text and a man-\nager agent synthesizes them into coherent outputs, alleviating focus\nissues in long contexts. LongAlign [ 1] proposes a recipe for long\ncontext alignment, combining instruction data construction, effi-\ncient batching, yielding strong gains on queries up to 100k tokens.\nComplementary to these, Xu et al. [ 19] systematically examine the\ntrade-offs between retrieval-augmentation and context-window\nextension, showing that hybrid approaches can outperform both\nstrategies alone. However, existing approaches often rely on raw\nreference texts, leading to inefficient retrieval, limited context uti-\nlization, and poor structural coherence in survey-like outputs.\n2.2 Automatic Survey Generation\nThe automatic generation of literature reviews has been studied\nfor over a decade, starting with multi-document summarization\ntechniques that produced unstructured related work sections. Early\nsystems, such as IBM Science Summarizer [ 4], focused on summa-\nrizing scientific articles, while more recent LLM-based methods like\nChatCite [ 8] and Susnjak et al. [ 12]\u2019s domain-specific fine-tuning\nadvanced the generation of comparative and knowledge-enriched\nreviews. Despite these advances, such methods primarily tackle\nsummarization rather than the creation of fully structured survey\npapers. More recent systems, including AutoSurvey [ 15], Interac-\ntiveSurvey [ 16], SurveyForge [ 20], and SurveyX [ 9], propose end-to-\nend pipelines integrating RAG, clustering, or multi-agent strategies\nto automate survey construction. These methods improve structural\ncoherence and formatting consistency while scaling to long-form\nsurvey content. Nevertheless, most frameworks still restrict users to\nfixed input-output modes, overlooking relationships among papers\nand limiting interactivity, which often results in surveys that lack\nflexibility, relational awareness, and depth.\nSurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nUser Query\nI want a survey\nabout Knowledge\nGraph Embedding\nQuery  Rewriting\nSemantic\nNetwork\nRepresent.Relation\nEmbed.KG\nEmbed.Search\nFilter\n{\"title\":"}
{"file": "2510.07733v1.pdf", "section": "2.2 Automatic Survey Generation", "chunk": ", 2018, Woodstock, NY\nUser Query\nI want a survey\nabout Knowledge\nGraph Embedding\nQuery  Rewriting\nSemantic\nNetwork\nRepresent.Relation\nEmbed.KG\nEmbed.Search\nFilter\n{\"title\": , \"author\":, published_time, cited_by, number_cited,\nSUMMARY: text};{\"title\": , \"author\":, published_time, cited_by, number_cited,\nSUMMARY: text};{\"title\": , \"author\":, published_time, cited_by, number_cited,\nSUMMARY: text};{\"title\": A, \"author\":, published_time, cited_by B,\nnumber_cited, SUMMARY: text};\nPaper\nMeta -Data\nKnowledge Representation\n{B}\n{A cites  B}{A}\nPaper\nMeta -Data\nw\nG\nTraversal on\nGraph G\nWriting\nStructured\nOutlineFull\nPaper\nVertical\nSummarizationHorizontal\nSummarization\nMemoryStore\nDatabase\n{Score,\nAssessment}\nWriting\nAgent\nOutline\nEvaluation\nAgent\nSubsectionsWriting\nAgent Subsection\nReview\nRelatedAugmentEvaluation\nAgent\n[Score, Suggested Queries]\nRAGDatabaseMemory\nPaper FulfillmentOutline\nGeneration\nHorizontal\nSummarization\nWeighted BFS\nQuantile 1:\nFoundation\nQuantile 2:\nDevelopment\nQuantile 3:\nFrontier\nVertical\nSummarization\nLoop \ud835\udc7b\ud835\udc8e\ud835\udc82\ud835\udc99\ntimes\nSummarizations\nw\nSemantic Weight\n\ud835\udc64=\ud835\udc47\ud835\udc52\ud835\udc65\ud835\udc61_\ud835\udc38\ud835\udc5b\ud835\udc50\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc5f (\ud835\udc34,\ud835\udc35)\nWriting Agent\nEvaluation Agent\nOutputLoop \ud835\udc7b\ud835\udc8e\ud835\udc82\ud835\udc99\ntimes\nFigure 3: Starting from a user\u2019s query, SurveyG retrieves and filters relevant papers (step 1-2), builds a hierarchical citation\ngraph, and applies horizontal and vertical traversals to produce multi-aspect summaries (step 3). A multi-agent framework\nthen leverages these pre-built summaries to produce a structured outline and a complete survey paper (step 4).\n3 Methodology\nWe introduceSurveyG, an automated survey generation frame-\nwork that operates in two main phases. ThePreparation Phase\ninvolves retrieving relevant papers, summarizing their"}
{"file": "2510.07733v1.pdf", "section": "2.2 Automatic Survey Generation", "chunk": "structured outline and a complete survey paper (step 4).\n3 Methodology\nWe introduceSurveyG, an automated survey generation frame-\nwork that operates in two main phases. ThePreparation Phase\ninvolves retrieving relevant papers, summarizing their content, con-\nstructing a hierarchical citation graph, and extracting relationships\nby traversing the graph. TheGeneration Phasefocuses on pro-\nducing a structured outline and composing a complete survey by\nintegrating instruction prompting within a multi-agent framework.\nAn overview of SurveyG is illustrated in Figure 3.\n3.1 Preparation Phase\nWe represent the relationships among papers using ahierarchi-\ncal citation graph, where nodes correspond to academic papers\nand edges capture both citation links and semantic similarity, each\nweighted by a value \ud835\udc64. Each node is further assigned to one of\nthree layers:Foundation,Development, orFrontier, which re-\nflect the role of the paper in the progression of research. Formally,\nthe hierarchical citation graph is defined as \ud835\udc3a=(\ud835\udc49,\ud835\udc38,\ud835\udc3f), where\ud835\udc49\ndenotes the set of nodes (papers), \ud835\udc38\u2286\ud835\udc49\u00d7\ud835\udc49 is the set of directed or\nundirected edges encoding citation or semantic relationships, and\n\ud835\udc3f:\ud835\udc49\u2192{Foundation,Development,Frontier} is a layer assign-\nment function. For each node \ud835\udc63\ud835\udc56\u2208\ud835\udc49, we associate a corresponding\ndocument\ud835\udc51\ud835\udc56\u2208\ud835\udc37, where\ud835\udc37is a database storing the complete con-\ntent of all papers. In addition, each node \ud835\udc63\ud835\udc56\u2208\ud835\udc49is equipped withattributes that include a summarization of \ud835\udc51\ud835\udc56as well as metadata\nsuch as the paper\u2019s title and publication year.\n3.1.1 Searching Relevant Paper.Given a user query \ud835\udc5e, our goal is\nto construct a hierarchical citation graph \ud835\udc3athat encompasses all\nrelevant papers while capturing the evolutionary trends of research\nin the field. We first employ an LLM to expand the query into a set\nof diverse keywords {\ud835\udc581,...,\ud835\udc58\ud835\udc5b}=LLM(\ud835\udc5e) . Using these keywords,\nwe retrieve candidate papers through the crawling module. After\ncollecting the relevant papers, we establish"}
{"file": "2510.07733v1.pdf", "section": "3.1.1 Searching Relevant Paper.Given a user query \ud835\udc5e, our goal is", "chunk": "set\nof diverse keywords {\ud835\udc581,...,\ud835\udc58\ud835\udc5b}=LLM(\ud835\udc5e) . Using these keywords,\nwe retrieve candidate papers through the crawling module. After\ncollecting the relevant papers, we establish edges between them\nbased on citation links and quantify their semantic relatedness\nthrough weighted connections. The weight \ud835\udc64assigned to an edge\nconnecting papers\ud835\udc63 \ud835\udc56and\ud835\udc63\ud835\udc57is defined as\n\ud835\udc64=sim\u0000Text_Encoder(\ud835\udc63 \ud835\udc56),Text_Encoder(\ud835\udc63 \ud835\udc57)\u0001,(1)\nwhere sim(\u00b7) denotes the cosine similarity between the text em-\nbedding vectors of the two papers. For computational efficiency,\nembeddings are derived solely from the abstract of each paper,\nwhich captures the core conceptual content while minimizing pro-\ncessing overhead.\nTo better leverage the key content of each paper for survey gen-\neration, every node \ud835\udc63\ud835\udc56is enriched with a summarization derived\nfrom its corresponding document \ud835\udc51\ud835\udc56. We design specialized prompt\ntemplates tailored to different paper types [ 9](e.g., surveys, method-\nological contributions, benchmarks, theoretical works). After this\nphase, we obtain a flat graph \u02c6\ud835\udc3a=(\ud835\udc49,\ud835\udc38) that encompasses the\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Minh-Anh et al.\npapers relevant to the user\u2019s topic along with their relationships.\nEach node\ud835\udc63 \ud835\udc56\u2208\ud835\udc49is associated with a set of attributes defined as\n\ud835\udc34(\ud835\udc63\ud835\udc56)={metadata(\ud835\udc63 \ud835\udc56),summary(\ud835\udc51 \ud835\udc56)},\nwhere metadata(\ud835\udc63 \ud835\udc56)contains bibliographic information such as\nthe paper\u2019s title, authors, and publication year, and summary(\ud835\udc51 \ud835\udc56)\nrepresents the content-based summarization of the corresponding\ndocument. These attributes are also stored in the database \ud835\udc37to\nfacilitate efficient retrieval and analysis.\n3.1.2 Knowledge Representation.To reflect the developmental pro-\ngression of research within a topic, we assign each node in the flat\ngraph \u02c6\ud835\udc3a=(\ud835\udc49,\ud835\udc38) to one of three hierarchical layers via a layer\nassignment"}
{"file": "2510.07733v1.pdf", "section": "3.1.2 Knowledge Representation.To reflect the developmental pro-", "chunk": "Knowledge Representation.To reflect the developmental pro-\ngression of research within a topic, we assign each node in the flat\ngraph \u02c6\ud835\udc3a=(\ud835\udc49,\ud835\udc38) to one of three hierarchical layers via a layer\nassignment function\n\ud835\udc3f:\ud835\udc49\u2192{Foundation,Development,Frontier}.\n(1)Foundation Layer.The foundation layer consists of seminal\nand high-impact works that form the intellectual backbone of the\nfield. For each paper\ud835\udc5d, we define atrending scoreas\ntrend score(\ud835\udc5d)=citation_count(\ud835\udc5d)\n1+year_published(\ud835\udc5d),(2)\nwhere year_published(\ud835\udc5d) denotes the number of years elapsed\nsince the paper\u2019s publication. Papers are ranked by this score, and\nthe top-\ud835\udc3eentries constitute the foundation set:\n\ud835\udc49foundation ={\ud835\udc63\ud835\udc56\u2208\ud835\udc49|trend score(\ud835\udc63\ud835\udc56)\u2264\ud835\udc3e}.\nThese papers are not only highly cited but also serve as conceptual\nanchors that establish key paradigms and problem formulations\nunderpinning later research. (2)Development Layer:The devel-\nopment layer captures the historical evolution of the field before a\ntime landmark \ud835\udc47(eg, 2025), representing works that refine, extend,\nor challenge the foundations. Formally,\n\ud835\udc49development ={\ud835\udc63\ud835\udc56\u2208\ud835\udc49|year(\ud835\udc63 \ud835\udc56)<\ud835\udc47, \ud835\udc63\ud835\udc56\u2209\ud835\udc49foundation},\nThese works are often incremental yet essential: they consolidate\nmethodological frameworks, validate empirical findings, and en-\nable the community to mature foundational ideas into established\nresearch threads. (3)Frontier Layer:The frontier layer reflects\nthe cutting edge of inquiry, consisting of recent contributions that\npoint toward emerging trends and open challenges. It is defined as\n\ud835\udc49frontier ={\ud835\udc63\ud835\udc56\u2208\ud835\udc49|year(\ud835\udc63 \ud835\udc56)\u2265\ud835\udc47, \ud835\udc63\ud835\udc56\u2209\ud835\udc49foundation}.\nUnlike the development layer, frontier works are temporally close\nto the present and thus provide a window into the current momen-\ntum and future trajectories of the domain. After this mapping, the\nhierarchical citation graph is represented as\n\ufffd"}
{"file": "2510.07733v1.pdf", "section": "3.1.2 Knowledge Representation.To reflect the developmental pro-", "chunk": "the development layer, frontier works are temporally close\nto the present and thus provide a window into the current momen-\ntum and future trajectories of the domain. After this mapping, the\nhierarchical citation graph is represented as\n\ud835\udc3a=(\ud835\udc49,\ud835\udc38,\ud835\udc3f), \ud835\udc49=\ud835\udc49 foundation\u222a\ud835\udc49 development\u222a\ud835\udc49 frontier.\nTraversing\ud835\udc3aalong horizontal (intra-layer) and vertical (inter-layer)\nedges then enables the generation of multi-aspect summaries cov-\nering methodologies, developmental trends, and future directions.Algorithm 1Vertical Traversal for Multi-Summarization\n1:Inp:Citation graph \ud835\udc3a=(\ud835\udc49,\ud835\udc38,\ud835\udc3f) , foundation papers \ud835\udc49foundation\n2:Out:{\ud835\udc47(1)\npath,...,\ud835\udc47(\ud835\udc3e)\npath}, where\ud835\udc3e=|\ud835\udc49 foundation|\n3:for all\ud835\udc60\u2208\ud835\udc49 foundation do\n4:\ud835\udc43 1\u2190{Extract(\ud835\udc60)}\n5:\ud835\udc43 2\u2190{Extract(\ud835\udc62)|\ud835\udc62\u2208WBFS(\ud835\udc60,Development)}\n6:\ud835\udc43 3\u2190{Extract(\ud835\udc64)|\ud835\udc64\u2208WBFS(\ud835\udc43 2,Frontier)}\n7:\ud835\udc47 dev\u2190GenerateSummarize(\ud835\udc43 1\u222a\ud835\udc43 2)\n8:\ud835\udc47 path\u2190GenerateSummarize(\ud835\udc47 dev,\ud835\udc433)\n9:Store\ud835\udc47 pathas the summarization for seed\ud835\udc60\n10:end for\n11:return\ud835\udc3esummarizations{\ud835\udc47(1)\npath,...,\ud835\udc47(\ud835\udc3e)\npath}\nYou are a research analyst synthesizing papers on the topic\n[QUERY] .<think> Explain your reasoning for clustering\npapers into 2\u20133 subgroups based on methodology, contri-\nbution, or thematic focus.</think>\nFor each subgroup, summarize the shared methodological\napproaches, thematic contributions, and provide a concise\ncritique comparing the works. Finally, synthesize an overall\nperspective highlighting how these subgroups collectively\noperate in the field.\nFigure 4: Horizontal summarization short version prompt.\n3"}
{"file": "2510.07733v1.pdf", "section": "3.1.2 Knowledge Representation.To reflect the developmental pro-", "chunk": "approaches, thematic contributions, and provide a concise\ncritique comparing the works. Finally, synthesize an overall\nperspective highlighting how these subgroups collectively\noperate in the field.\nFigure 4: Horizontal summarization short version prompt.\n3.1.3 Traversal on Graph Strategy.We propose a two-stage sum-\nmarization framework designed to capture both the breadth and\ndepth of the hierarchical citation graph. In thehorizontal stage, to\ncapture the internal structure of each layer \ud835\udc49\ud835\udc59, we partition it into\ncommunities using the Leiden algorithm [13], yielding\nC\ud835\udc59={\ud835\udc36\ud835\udc59,1,...,\ud835\udc36\ud835\udc59,\ud835\udc5a \ud835\udc59},\ud835\udc5a\ud835\udc59\u00d8\n\ud835\udc57=1\ud835\udc36\ud835\udc59,\ud835\udc57=\ud835\udc49\ud835\udc59.\nEach community \ud835\udc36\ud835\udc59,\ud835\udc57corresponds to a coherent research direction\nformed by citation and semantic proximity. For every community,\nwe query an LLM using a carefully constructed prompt that in-\ntegrates Plan-and-Solve [ 14] strategies, along with paper-specific\nattributes, to generate a synthesized summary:\n\ud835\udc47\ud835\udc59,\ud835\udc57=LLM({\ud835\udc34(\ud835\udc63 \ud835\udc56)|\ud835\udc63\ud835\udc56\u2208\ud835\udc36\ud835\udc59,\ud835\udc57}),\nwhich emphasizes the methodologies and thematic scope of the\npapers while capturing key relationships among them. This process\nuncovers sub-directions within the topic and provides a global\nperspective of how research clusters evolve within each layer. The\ndetailed prompts used for information extraction are provided in\nFigure 4.\nIn thevertical stage, we aim to model cross-layer dependencies.\nFor each foundation paper, we perform a weighted breadth-first\nsearch (WBFS) over its citation paths, where traversal prioritizes\nsemantically relevant nodes according to edge weights. The algo-\nrithmic details are provided in Algorithm 1, the full WBFS procedure\nand prompt design are described in Appendix A. Each resulting path\nSurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\naggregates the node attributes \ud835\udc34(\ud835\udc63) for all\ud835\udc63enc"}
{"file": "2510.07733v1.pdf", "section": "3.1.3 Traversal on Graph Strategy.We propose a two-stage sum-", "chunk": "Framework with Hierarchical Citation Graph for Automated Survey Generation Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\naggregates the node attributes \ud835\udc34(\ud835\udc63) for all\ud835\udc63encountered during\nthe WBFS traversal and places them in the \ud835\udc5d\ud835\udc4e\ud835\udc61\u210e variable. We then\napply hierarchical summarization across layers, exploiting temporal\nprogression to mitigate long-context issues and extract key insights\nmore effectively [ 21]. This process incrementally integrates knowl-\nedge from the Development and Frontier layers into path-specific\nsummaries. After the summarization phase, SurveyG ultimately\nproduces\ud835\udc3e+\ud835\udc41 outputs with \ud835\udc41horizontal layer summaries and \ud835\udc3e\nvertical path summaries.\nIn contrast to earlier frameworks [ 9,15,16,20] that represent\npapers as isolated records in a flat database and depend exclusively\non RAG-based retrieval, SurveyG organizes the literature within a\nhierarchical citation graph \ud835\udc3a. This representation integrates both ci-\ntation and semantic connections among papers, allowing the system\nto capture the logical progression of research topics over time. By\ntraversing this hierarchy, SurveyG produces a series of summariza-\ntions across multiple layers, effectively revealing methodological\ndevelopments, evolutionary patterns, and current research fron-\ntiers. Such a design provides a more coherent and interpretable\nknowledge foundation for automated survey generation.\n3.2 Generation Phase\nWe employ a multi-agent conversational framework [ 17] to guide\nthe generation of survey papers. The system is composed of two\ncomplementary agents: a Writing Agent, equipped with memory\n[11] initialized with \ud835\udc3e+\ud835\udc41 summarizations from the graph traver-\nsal phase, and an Evaluation Agent, which leverages the internal\nreasoning capabilities of LLMs to provide diversity-oriented feed-\nback. Through iterative interaction, the Writing Agent proposes\nstructured content grounded in summarizations, while the Evalua-\ntion Agent critiques and refines these outputs to ensure coherence\nand balance. This cooperative setup enables the agents to jointly\nconstruct and improve survey papers by integrating both exter-\nnal evidence and internal reasoning. An overview of the overall\ngeneration process is provided in Algorithm 2.\n3.2.1 Structured Outline Construction.The Writing Agent con-\nstructs an initial structured outline by grounding"}
{"file": "2510.07733v1.pdf", "section": "3.2 Generation Phase", "chunk": "survey papers by integrating both exter-\nnal evidence and internal reasoning. An overview of the overall\ngeneration process is provided in Algorithm 2.\n3.2.1 Structured Outline Construction.The Writing Agent con-\nstructs an initial structured outline by grounding each section and\nsubsection in the \ud835\udc3e+\ud835\udc41 multi-aspect summarizations, ensuring\nboth factual grounding and thematic coherence. The Evaluation\nAgent then reviews the draft, assessing logical flow and suggest-\ning refinements without altering the overall structure. After one\nor two feedback iterations, the outline converges into a coherent\nand evidence-supported framework. Detailed prompting for both\nagents is provided in Figure 5 and Appendix C. The key innovation\nof SurveyG lies in its ability to manage long-context survey syn-\nthesis without concatenating all reference texts [ 9,15] or relying\non pre-existing human-written surveys [ 20]. Instead, it leverages\nhierarchical summarization from the citation graph \ud835\udc3aas structured\nknowledge injected into the Writing Agent.\n3.2.2 Full Paper Completion.In the writing stage, theWriting\nAgentexpands each subsection based on the structured outline\nand its memory, utilizing grounded summaries to ensure factual\nconsistency and contextual relevance. Meanwhile, theEvaluation\nAgentprovides critical feedback by offering broader perspectivesAlgorithm 2SurveyG Automated Survey Generation\n1:Input:Survey Topic \ud835\udc44, Paper Database \ud835\udc37, Max iterations \ud835\udc47max,\nSummarizations{\ud835\udc47 1,...,\ud835\udc47\ud835\udc3e+\ud835\udc41}\n2:Output:Survey Paper\ud835\udc39\n3:// Initialization\n4:Create Writing Agent (WA) and Evaluation Agent (EA)\n5:Initialize memory\ud835\udc40for WA with{\ud835\udc47 1,...,\ud835\udc47\ud835\udc3e+\ud835\udc41}\n6:// Phase 1: Create Outline\n7:WA generates initial outlineO(0)from\ud835\udc40\n8:for\ud835\udc61=1to\ud835\udc47 maxdo\n9:O(\ud835\udc61)=WA(\ud835\udc40,EA(O(\ud835\udc61\u22121)))\n10:ifquality threshold metthen break\n11:end if\n12:end for\n13:O\u2217\u2190O(\ud835\udc61)\n14:// Phase 2: Write Full Paper\n15:for allsubsection\ud835\udc42 \ud835\udc56\u2208O\u2217do\n16"}
{"file": "2510.07733v1.pdf", "section": "3.2.2 Full Paper Completion.In the writing stage, theWriting", "chunk": "11:end if\n12:end for\n13:O\u2217\u2190O(\ud835\udc61)\n14:// Phase 2: Write Full Paper\n15:for allsubsection\ud835\udc42 \ud835\udc56\u2208O\u2217do\n16:WA produces an initial draft\ud835\udc42(0)\n\ud835\udc56\n17:Refine with EA\u2019s feedback and suggested queries\ud835\udc44:\n18:for\ud835\udc61=1to\ud835\udc47 maxdo\n19:\ud835\udc42(\ud835\udc61)\n\ud835\udc56=WA(\ud835\udc40\u222a\ud835\udc45(\ud835\udc61)\n\ud835\udc56,\ud835\udc38\ud835\udc34(\ud835\udc42(\ud835\udc61\u22121)\n\ud835\udc56))\n20:where\ud835\udc45(\ud835\udc61)\n\ud835\udc56=Retrieve(\ud835\udc44(\ud835\udc61)\n\ud835\udc56,\ud835\udc37)\n21:ifquality threshold metthen break\n22:end if\n23:end for\n24:end for\n25:// Phase 3: Assemble Survey\n26:\ud835\udc39\u2190\u00d0\n\ud835\udc56\ud835\udc42(\ud835\udc61)\n\ud835\udc56\n27:return\ud835\udc39\nCreate a comprehensive literature review outline based on\nthe following taxonomy summaries for three layers (Foun-\ndation, Development, Frontier) and vertical directions.\nHorizontal summary:{summary_layer}.\nVertical direction:{summary_path}.\nBased on the above information, create a detailed outline\nfor a literature review paper, organizing it into sections and\nsubsections.Respond with the outline in JSON format\nwith keys:\n[\u2019section_outline\u2019, \u2019subsection_focus\u2019, \u2019proof_ids\u2019].\nFor each section: Put section and subsection titles in \u2019sec-\ntion_outline\u2019; Add a paragraph in \u2019subsection_focus\u2019 de-\nscribing the main focus of each subsection; Add \u2019proof_ids\u2019\nfrom either taxonomy layer or vertical direction.\nFigure 5: Structured outline creation short version prompt.\nand generating targeted retrieval queries to identify additional rel-\nevant papers from the database \ud835\udc37. This iterative collaboration en-\nsures that the final text is coherent, comprehensive, and rigorously\nsupported by the literature. The key novelty lies in combining RAG-\nbased retrieval, guided by the Evaluation Agent\u2019s global perspective,\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Minh-Anh et al.\nwith pre-built hierarchical summaries that serve as localized know"}
{"file": "2510.07733v1.pdf", "section": "3.2.2 Full Paper Completion.In the writing stage, theWriting", "chunk": "RAG-\nbased retrieval, guided by the Evaluation Agent\u2019s global perspective,\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Minh-Anh et al.\nwith pre-built hierarchical summaries that serve as localized knowl-\nedge, enabling the generation of well-balanced and contextually\nrich subsections.\n4 Experiments\n4.1 Experimental Setup\n4.1.1 Baselines.We compareSurveyGwith three state-of-the-\nart systems.AutoSurvey[ 15] andSurveyX[ 9] represent multi-\nstage frameworks for automated survey generation, with SurveyX\nenhancing AutoSurvey through structured knowledge extraction\nand outline optimization.SurveyForge[ 20], in contrast, leverages\nhuman-written survey papers from related domains as prior knowl-\nedge for heuristic outline generation, guided by a memory-driven\nscholar navigation agent that retrieves high-quality references for\ncomposing new surveys.\n4.1.2 Dataset Evaluation.To assess the generalizability and robust-\nness ofSurveyG, we evaluate it on ten diverse computer science\ntopics from the SurGE benchmark [ 10], which includes 205 ground-\ntruth surveys and over one million papers. We recruited 20 domain\nexperts, CS Ph.D. students from QS 5-star universities and senior\nAI research engineers, to curate high-quality reference papers and\nselect one representative ground-truth survey per topic. The same\nexperts also served as human evaluators, assessing the coherence,\ncoverage, and factual accuracy of generated surveys. Additional\ndetails on ground-truth construction and evaluation protocols are\nprovided in Appendix B.\n4.1.3 Implementation Details.For fair comparison, we strictly fol-\nlow the experimental settings of prior works [ 9,15,20]. Specifically,\nwe retrieve 1,500 candidate papers for outline generation and 60\nrelevant papers for each chapter-writing stage, identical to Sur-\nveyForge [ 20]. All experiments useGPT-4o-mini-2024-07-18as the\nbackbone model for both Writing and Evaluation Agents, consistent\nwith previous studies. We generate surveys for ten predefined topics,\neach with ten independent trials (100 surveys in total), and report\naveraged results for stability. To assess cross-model robustness, we\nadditionally testGemini-2.5-Flash. For evaluation, advanced LLM\njudges:GPT-4o-2024"}
{"file": "2510.07733v1.pdf", "section": "4.1.3 Implementation Details.For fair comparison, we strictly fol-", "chunk": "total), and report\naveraged results for stability. To assess cross-model robustness, we\nadditionally testGemini-2.5-Flash. For evaluation, advanced LLM\njudges:GPT-4o-2024-08-06,Claude-3.5-Sonnet-20241022,DeepSeek-\nV3.2-Exp, andGemini-2.5-Proare used to score both outlines and\nfull survey texts. We set the iteration number \ud835\udc47\ud835\udc40\ud835\udc34\ud835\udc4b to 2, the same\nas AutoSurvey and SurveyForge.\n4.1.4 Evaluation Metrics.We evaluate the generated outputs along\nthree dimensions: outline, content, and citation quality. ForOut-\nline Quality, we follow the evaluation protocol of [ 20], using the\nsame prompt. For content quality, we adopt the five widely used\nmetrics from prior benchmarks [ 9,10,15,20]:Coverage,Struc-\nture,Relevance,Synthesis, andCritical Analysis. Six metrics\nare rated on a 0-100 scale by both LLM and human judges, mea-\nsuring completeness, organization, topical alignment, integrative\nreasoning, and analytical depth. For citation quality [ 5,15], we\nevaluate the factual consistency between claims and their cited\nreferences using a Natural Language Inference model, reporting\nCitation Recall(ratio of supported claims),Citation Precision\n(ratio of valid references), andCitation F1as their harmonic mean.\nDetails of these metrics are provided in Appendix B.4.2 Evaluation on Content Quality\nAs shown in Table 1,SurveyGconsistently achieves the highest\nscores across nearly all metrics and evaluation models, demon-\nstrating strong generalization and robustness under different LLM\njudges. In particular,SurveyGshows notable gains inSynthesis\nandCritical Analysis, reflecting its ability to integrate information\nand identify research gaps through the use of a hierarchical cita-\ntion graph and multi-level summarization prompts.SurveyForge\nranks second overall, outperformingSurveyXandAutoSurveyin\nCoverageandStructuredue to its heuristic use of human-written\nsurveys as prior knowledge. However, it remains less effective than\nSurveyG, which achieves superior organization and analytical\ndepth without relying on human-written inputs, instead leveraging\nstructured summarization and cross-community reasoning within"}
{"file": "2510.07733v1.pdf", "section": "4.1.4 Evaluation Metrics.We evaluate the generated outputs along", "chunk": "heuristic use of human-written\nsurveys as prior knowledge. However, it remains less effective than\nSurveyG, which achieves superior organization and analytical\ndepth without relying on human-written inputs, instead leveraging\nstructured summarization and cross-community reasoning within\nthe hierarchical framework.\n4.3 Evaluation on Ground Truth\nFigure 6 reveals distinct performance patterns among the three\napproaches. In Synthesis, SurveyG achieves the most balanced per-\nformance, closely matching human surveys and showing more\nconsistent scores than SurveyForge across metrics like OOD Detec-\ntion and Hallucination in LLM. For Coverage, while human surveys\nlead with 90 scores, SurveyG demonstrates more stable cross-topic\nperformance compared to SurveyForge\u2019s variable results, particu-\nlarly in specialized areas like Knowledge Graph Embedding and RL\nfor Language Processing. In Critical Analysis, both automated meth-\nods score 70-85, but SurveyG shows less variation between metrics,\nindicating more reliable quality. Overall, while SurveyForge occa-\nsionally peaks higher in individual metrics, SurveyG\u2019s consistently\nuniform polygon shapes across all three dimensions suggest su-\nperior robustness and generalization capability for diverse survey\ngeneration tasks.\n4.4 Evaluation on Citation Quality\nThe experimental results presented in Table 2 clearly indicate that\nthe SurveyG (ours) model sets a new standard for automated survey\ngeneration, demonstrating superior citation quality compared to\nexisting systems. SurveyG achieves the highest Recall at 90.60 and\nthe best F1 Score at 83.49. This high Recall figure is particularly\nnotable, as it is very close to the Ground Truth Recall of 92.53, sug-\ngesting SurveyG is highly effective at comprehensively identifying\nand linking relevant literature. While SurveyX holds the lead in Pre-\ncision (78.12), SurveyG\u2019s significantly better F1 Score confirms its\noverall advantage in balancing the inclusion of necessary citations\nwith the exclusion of irrelevant ones. In summary, SurveyG (ours)\nsurpasses AutoSurvey, SurveyForge, and SurveyX in overall perfor-\nmance, demonstrating a marked improvement in the reliability and\ncomprehensiveness of citations in generated surveys.\n4.5 Human Evaluation\nTo validate our automated evaluation framework, we conducted\na comparative assessment betweenSurveyG(ours) andSurvey-\nForgeacross ten topics. We adopted a win-rate\u2013based evaluation\nprotocol, presenting anonymized outputs"}
{"file": "2510.07733v1.pdf", "section": "4.4 Evaluation on Citation Quality", "chunk": ".\n4.5 Human Evaluation\nTo validate our automated evaluation framework, we conducted\na comparative assessment betweenSurveyG(ours) andSurvey-\nForgeacross ten topics. We adopted a win-rate\u2013based evaluation\nprotocol, presenting anonymized outputs from both systems to do-\nmain experts and the automated evaluation system. Human experts\nwere selected based on topic relevance and possessed extensive\nSurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nLLM Model Coverage Structure Relevance Synthesis Critical Analysis\nClaudeAutoSurvey 73.6 64.5 80.2 51.1 45.8\nSurveyForge 81.8 78.4 89.1 75.4 70.3\nSurveyX 74.2 71.3 82.3 62.4 52.7\nSurveyG88.1 87.9 93.6 80.2 77.3\nGPTAutoSurvey 90.7 86.2 89.3 84.6 82.3\nSurveyForge 94.2 87.3 94.8 88.6 88.5\nSurveyX 89.3 84.4 88.5 79.2 78.4\nSurveyG95.7 88.5 95.1 92.2 90.5\nDeepseekAutoSurvey 86.5 80.4 87.8 77.3 72.4\nSurveyForge89.4 86.592.5 84.1 80.7\nSurveyX 85.4 81.2 89.4 78.2 75.6\nSurveyG 88.7 82.793.4 85.5 81.6\nGeminiAutoSurvey 94.2 70.8 96.5 83.6 84.2\nSurveyForge 94.993.1 98.793.5 94.2\nSurveyX 90.2 71.2 95.6 82.8 84.5\nSurveyG96.289.4 97.693.6 94.9\nTable 1: LLM-as-a-judge evaluation of generated"}
{"file": "2510.07733v1.pdf", "section": "4.5 Human Evaluation", "chunk": ".2 71.2 95.6 82.8 84.5\nSurveyG96.289.4 97.693.6 94.9\nTable 1: LLM-as-a-judge evaluation of generated surveys. Each LLM evaluates four generation models across content quality\ndimensions.\nVisual\nTransformerHallucination\nin LLMOOD DetectionGraph Neural\nNetwork\nRL for\nLanguage Processing\nDeep\nMeta-Learning\nRAG\nfor LLM\nKnowlegde Graph\nEmbeddingExploration\nin RLStabilizing\nGANs20406080100\nCoverage\nVisual\nTransformerHallucination\nin LLMOOD DetectionGraph Neural\nNetwork\nRL for\nLanguage Processing\nDeep\nMeta-Learning\nRAG\nfor LLM\nKnowlegde Graph\nEmbeddingExploration\nin RLStabilizing\nGANs\nSynthesis\nVisual\nTransformerHallucination\nin LLMOOD DetectionGraph Neural\nNetwork\nRL for\nLanguage Processing\nDeep\nMeta-Learning\nRAG\nfor LLM\nKnowlegde Graph\nEmbeddingExploration\nin RLStabilizing\nGANs\nCritical AnalysisSurveyForge SurveyG Human\nFigure 6: LLM-as-a-judge evaluation of human-written ground-truth surveys, SurveyForge, and SurveyG across ten topics using\nGPT-4o as the evaluator.\nModel Recall Precision F1 Score\nAutoSurvey 82.25 77.41 79.76\nSurveyForge 88.34 75.92 81.66\nSurveyX 85.2378.1281.52\nSurveyG (ours)90.6076.3283.49\nGround Truth 92.53 86.42 89.34\nTable 2: Performance comparison of different models on\ncitation quality metrics.\nresearch experience in their respective domains. As summarized\nin Table 3, three complementary metrics were used to quantify\nperformance differences. TheScore Win Ratemeasures how often\na model receives a higher absolute score from the LLM evaluator.\nTheComparative Win Ratereflects the frequency with which the\nLLM selects a model\u2019s paper as superior in pairwise comparisons.\nTheHuman Evaluation Win Raterepresents the proportion of timeshuman experts preferred outputs from one model over the other.\nUnder this framework,SurveyGconsistently outperformsSurvey-\nForge, achieving aScore Win Rate"}
{"file": "2510.07733v1.pdf", "section": "4.5 Human Evaluation", "chunk": "as superior in pairwise comparisons.\nTheHuman Evaluation Win Raterepresents the proportion of timeshuman experts preferred outputs from one model over the other.\nUnder this framework,SurveyGconsistently outperformsSurvey-\nForge, achieving aScore Win Rateof61 .15%, aComparative Win\nRateof72.25%, aHuman Evaluation Win Rateof64 .00%, and an\noverall score of87 .67%. In contrast,SurveyForgerecords38 .85%,\n27.75%,36.00%, and82.55%, respectively. These results demonstrate\nstrong consistency between automated and human evaluations, con-\nfirming that our framework reliably captures expert-level judgment\nwhile maintaining scalability and efficiency.\nAs shown in Table 4, three complementary metrics were adopted\nto evaluate model performance on outline generation. Under this\nevaluation framework, SurveyG (ours) consistently outperforms\nSurveyForge, achieving a Score Win Rate of55 .00%,a Comparative\nWin Rate of58 .00%, a Human Evaluation score of55 .00%, and an\nOverall Score of95 .00%. In contrast, SurveyForge records45 .00%,\n42.00%,45.00%, and90 .00%, respectively. These findings demon-\nstrate that our model produces higher-quality outlines, confirming\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Minh-Anh et al.\nModelScore Win Rate Comparative Win Rate Human Eval Overall Score\nSurveyForge38.85% 27.75% 36.00% 82.55%\nSurveyG (ours)61.15% 72.25% 64.00% 87.67%\nTable 3: Comparison of models across full paper evaluation metrics.\nModelScore Win Rate Comparative Win Rate Human Eval Overall Score\nSurveyForge45.00% 42.00% 45.00% 90.00%\nSurveyG (ours)55.00% 58.00% 55.00% 95.00%\nTable 4: Comparison of models based on outline evaluation metrics.\nthat constructing a Hierarchical Citation Graph for paper retrieval\nin outline generation is a principled and effective approach.\n4.5.1 Details of Human Evaluation.To evaluate reliability, we as-\nsigned two domain experts to each of the ten SurGE topics. For\nevery topic, we randomly sampled ten anonymized outputs from\nSurveyGand ten from"}
{"file": "2510.07733v1.pdf", "section": "4.5 Human Evaluation", "chunk": "approach.\n4.5.1 Details of Human Evaluation.To evaluate reliability, we as-\nsigned two domain experts to each of the ten SurGE topics. For\nevery topic, we randomly sampled ten anonymized outputs from\nSurveyGand ten fromSurveyForge, each obtained from indepen-\ndent generation runs. All outputs were fully anonymized, ensuring\nthat neither the human experts nor the LLM judge (GPT-4o) was\naware of their system of origin. Both experts independently rated\nall 20 outputs per topic using identical evaluation prompts and\ncriteria, which covered five content metricsStructure,Coverage,\nRelevance,Synthesis, andCritical Analysisas well as an outline\nquality score (0 to 100). For each topic, we computed Cohen\u2019s \ud835\udf05to\nmeasure (i) agreement between the LLM and human raters, and\n(ii) agreement between human raters. Table 7 reports topic-wise\nand average \ud835\udf05values. The mean Cohen\u2019s \ud835\udf05for the outline metric\nwas 0.6972 (LLM-human) versus 0.7542 (human-human), and for\ncontent metrics, 0.6062 versus 0.7127, respectively. These results\ndemonstrate substantial inter-rater reliability and confirm that the\nLLM-as-a-judge evaluations align closely with expert assessments.\nEvaluation Pair AspectCohen\ud835\udf05\nLLM vs. Human Outline 0.6972\nLLM vs. Human Content 0.6062\nHuman Cross-Validation Outline 0.7542\nHuman Cross-Validation Content 0.7127\nTable 5: Inter-rater agreement between LLM and human eval-\nuations\n4.6 Cost estimation\nThe SurveyG framework generates survey papers with an average\nlength of approximately 64k tokens, comparable to expert-written\nsurveys. Each subsection is produced using around 12k input to-\nkens and 800 output tokens. Additionally, the Evaluation Agent in\nthe RAG loop performs one assessment per subsection, consuming\napproximately 3.7k input and 700 output tokens. Using this con-\nfiguration, the total cost for generating a full 64k-token survey is\nestimated at $1.5-$1.7, depending on model pricing and API param-\neters. These results highlight the cost-effectiveness and scalabilityVariant Cov. Str. Rel. Syn. C"}
{"file": "2510.07733v1.pdf", "section": "4.6 Cost estimation", "chunk": "generating a full 64k-token survey is\nestimated at $1.5-$1.7, depending on model pricing and API param-\neters. These results highlight the cost-effectiveness and scalabilityVariant Cov. Str. Rel. Syn. C.A\nFull 91.98 86.78 94.81 85.44 83.34\nw/o Vertical Traversal 90.51 85.05 93.91 84.92 84.28\nw/o Horizontal Clustering 91.80 86.22 94.61 83.91 86.01\nw/o MA 89.26 84.36 91.50 83.73 81.43\nTable 6: We test three variants: (1) w/o Vertical Traversal uses\nonly horizontal clustering and summarization within each\nlayer; (2) w/o Horizontal Clustering performs only vertical\npath traversal from foundation papers; (3) w/o MA removes\nthe Multi-Agent component.\nof the SurveyG framework for producing high-quality, large-scale\nliterature surveys.\n4.7 Ablation Studies\nIn Table 6, to evaluate the contribution of each component in our\narchitecture, we present ablation study results comparing our full\nmodel against variants with specific components removed. Our full\nversion achieves the best overall performance, demonstrating the\neffectiveness of our complete architecture. The inclusion of RAG\nsignificantly enhances Coverage (91.98) and Relevance (94.81) com-\npared to the w/o MA variant (89.26 and 91.50), as it supplements\nthe model with additional contextual information. More impor-\ntantly, the full model outperforms both the w/o Vertical Traversal\nand w/o Horizontal Clustering variants, particularly in Structure\nscores (86.78), indicating that having all components working to-\ngether enables superior information synthesis. This complete ar-\nchitecture allows the model to effectively integrate and organize\ninformation from multiple sources, resulting in more coherent and\nwell-structured outputs across all evaluation metrics.\n5 Conclusion\nIn this work, we introduced SurveyG, an automated framework\nfor survey generation that leverages hierarchical knowledge repre-\nsentation and multi-agent collaboration to address the limitations\nof existing LLM-based approaches. By modeling papers through\na three-layer citation-similarity graph and employing both hori-\nzontal and"}
{"file": "2510.07733v1.pdf", "section": "5 Conclusion", "chunk": "generation that leverages hierarchical knowledge repre-\nsentation and multi-agent collaboration to address the limitations\nof existing LLM-based approaches. By modeling papers through\na three-layer citation-similarity graph and employing both hori-\nzontal and vertical traversal strategies, SurveyG captures the struc-\ntural relationships and evolutionary progress of research, enabling\nthe creation of coherent and well-structured outlines. Through\nSurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nextensive evaluations, we demonstrated the effectiveness of our\nframework across diverse computer science topics. On the SurGE\nbenchmark for autonomous computer science survey generation,\nboth LLM-as-a-judge evaluations and human expert assessments\ndemonstrate that SurveyG outperforms state-of-the-art frameworks\nacross multiple dimensions.\nReferences\n[1]Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong,\nand Juanzi Li. [n. d.]. Longalign: A recipe for long context alignment of large\nlanguage models, 2024.URL https://arxiv. org/abs/2401.18058([n. d.]).\n[2]Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang,\nZhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al .2023. Longbench: A\nbilingual, multitask benchmark for long context understanding.arXiv preprint\narXiv:2308.14508(2023).\n[3]Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Bam-\nboo: A comprehensive benchmark for evaluating long text modeling capacities\nof large language models.arXiv preprint arXiv:2309.13345(2023).\n[4]Shai Erera, Michal Shmueli-Scheuer, Guy Feigenblat, Ora Peled Nakash, Odellia\nBoni, Haggai Roitman, Doron Cohen, Bar Weiner, Yosi Mass, Or Rivlin, et al .2019.\nA summarization system for scientific documents.arXiv pre"}
{"file": "2510.07733v1.pdf", "section": "5 Conclusion", "chunk": "led Nakash, Odellia\nBoni, Haggai Roitman, Doron Cohen, Bar Weiner, Yosi Mass, Or Rivlin, et al .2019.\nA summarization system for scientific documents.arXiv preprint arXiv:1908.11152\n(2019).\n[5]Tianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023. Enabling large\nlanguage models to generate text with citations.arXiv preprint arXiv:2305.14627\n(2023).\n[6]Shanshan Han, Qifan Zhang, Yuhang Yao, Weizhao Jin, and Zhaozhuo Xu.\n2024. LLM multi-agent systems: Challenges and open problems.arXiv preprint\narXiv:2402.03578(2024).\n[7]Thien Huynh-The, Quoc-Viet Pham, Xuan-Qui Pham, Thanh Thi Nguyen, Zhu\nHan, and Dong-Seong Kim. 2023. Artificial intelligence for the metaverse: A\nsurvey.Engineering Applications of Artificial Intelligence117 (2023), 105581.\n[8]Yutong Li, Lu Chen, Aiwei Liu, Kai Yu, and Lijie Wen. 2024. ChatCite: LLM\nagent with human workflow guidance for comparative literature summary.arXiv\npreprint arXiv:2403.02574(2024).\n[9]Xun Liang, Jiawei Yang, Yezhaohui Wang, Chen Tang, Zifan Zheng, Shichao Song,\nZehao Lin, Yebin Yang, Simin Niu, Hanyu Wang, et al .2025. Surveyx: Academic\nsurvey automation via large language models.arXiv preprint arXiv:2502.14776\n(2025).\n[10] Weihang Su, Anzhe Xie, Qingyao Ai, Jianming Long, Jiaxin Mao, Ziyi Ye, and\nYiqun Liu. 2025. Benchmarking Computer Science Survey Generation.arXiv\npreprint arXiv:2508.15658(2025).\n[11] Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas Griffiths. 2023"}
{"file": "2510.07733v1.pdf", "section": "5 Conclusion", "chunk": "Science Survey Generation.arXiv\npreprint arXiv:2508.15658(2025).\n[11] Theodore Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas Griffiths. 2023.\nCognitive architectures for language agents.Transactions on Machine Learning\nResearch(2023).\n[12] Teo Susnjak, Peter Hwang, Napoleon Reyes, Andre LC Barczak, Timothy McIn-\ntosh, and Surangika Ranathunga. 2025. Automating research synthesis with\ndomain-specific large language model fine-tuning.ACM Transactions on Knowl-\nedge Discovery from Data19, 3 (2025), 1\u201339.\n[13] Vincent A Traag, Ludo Waltman, and Nees Jan Van Eck. 2019. From Louvain to\nLeiden: guaranteeing well-connected communities.Scientific reports9, 1 (2019),\n1\u201312.\n[14] Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan, Roy Ka-Wei Lee,\nand Ee-Peng Lim. 2023. Plan-and-solve prompting: Improving zero-shot chain-\nof-thought reasoning by large language models.arXiv preprint arXiv:2305.04091\n(2023).\n[15] Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meis-\nhan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, et al .[n. d.]. Autosurvey:\nLarge language models can automatically write surveys, 2024.URL https://arxiv.\norg/abs/2406.10252([n. d.]).\n[16] Zhiyuan Wen, Jiannong Cao, Zian Wang, Beichen Guo, Ruosong Yang, and\nShuaiqi Liu. 2025. Interactivesurvey: An llm-based personalized and interactive\nsurvey paper generation system.arXiv preprint arXiv:2504.08762(2025).\n[17] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li\nJiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al"}
{"file": "2510.07733v1.pdf", "section": "5 Conclusion", "chunk": "] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li\nJiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al .2024. Autogen: Enabling\nnext-gen LLM applications via multi-agent conversations. InFirst Conference on\nLanguage Modeling.\n[18] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. 2025.\nInference scaling laws: An empirical analysis of compute-optimal inference for\nLLM problem-solving. InThe Thirteenth International Conference on Learning\nRepresentations.\n[19] Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu,\nSandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan\nCatanzaro. 2023. Retrieval meets long context large language models.arXiv\npreprint arXiv:2310.03025(2023).Algorithm 3Weighted Breadth-First Search (WBFS)\n1:Input:Start node\ud835\udc60, target layer\u2113\n2:Output:Set of nodes\ud835\udc45in layer\u2113\n3:\ud835\udc63\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc51\u2190{\ud835\udc60},\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52\u2190[\ud835\udc60],\ud835\udc45\u2190\u2205\n4:while\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52\u2260\u2205do\n5:\ud835\udc62\u2190\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52.Deqeue()\n6: for all\ud835\udc63\u2208Successors(\ud835\udc62) sorted by\ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc54\u210e\ud835\udc61(\ud835\udc62,\ud835\udc63) desc do\n7:if\ud835\udc63\u2209\ud835\udc63\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc51then\n8:\ud835\udc63\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc51\u2190\ud835\udc63\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc51\u222a{"}
{"file": "2510.07733v1.pdf", "section": "5 Conclusion", "chunk": "then\n8:\ud835\udc63\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc51\u2190\ud835\udc63\ud835\udc56\ud835\udc60\ud835\udc56\ud835\udc61\ud835\udc52\ud835\udc51\u222a{\ud835\udc63}\n9:if\ud835\udc63.\ud835\udc59\ud835\udc4e\ud835\udc66\ud835\udc52\ud835\udc5f=\u2113then\n10:\ud835\udc45\u2190\ud835\udc45\u222a{\ud835\udc63}\n11:else\n12:\ud835\udc5e\ud835\udc62\ud835\udc52\ud835\udc62\ud835\udc52.Enqeue(\ud835\udc63)\n13:end if\n14:end if\n15:end for\n16:end while\n17:return\ud835\udc45\n[20] Xiangchao Yan, Shiyang Feng, Jiakang Yuan, Renqiu Xia, Bin Wang, Bo Zhang,\nand Lei Bai. 2025. Surveyforge: On the outline heuristics, memory-driven gener-\nation, and multi-dimensional evaluation for automated survey writing.arXiv\npreprint arXiv:2503.04629(2025).\n[21] Yusen Zhang, Ruoxi Sun, Yanfei Chen, Tomas Pfister, Rui Zhang, and Sercan Arik.\n2024. Chain of agents: Large language models collaborating on long-context tasks.\nAdvances in Neural Information Processing Systems37 (2024), 132208\u2013132237.\nA Details about Traversal on Graph\nWe provide a full algorithm of Weighted BFS in Algorithm 3.\nB Experimental Setting\nB.1 Survey Topics\nWe compiled a collection of ten representative survey papers cover-\ning diverse research areas, as summarized in Table 7. Each topic re-\nflects an active line of inquiry within machine learning and natural\nlanguage processing, providing a strong foundation for evaluating\nliterature review generation.\nB.2 Evaluation Metrics\nB.2.1 Metrics about Content Quality.We evaluate both the qual-\nity of the generated outlines and the full survey papers. A well-\nstructured and logically coherent outline is essential for maintaining\nclarity and organization, and we adopt the same evaluation settings\nas in [ 20]. The full paper evaluation serves as a comprehensive\nqualitative benchmark to assess the academic rigor and practical\nutility of the generated surveys. Following the prompt design and\nevaluation protocols from previous studies [ 9,10,15,20], we assess\nsurvey quality"}
{"file": "2510.07733v1.pdf", "section": "5 Conclusion", "chunk": "paper evaluation serves as a comprehensive\nqualitative benchmark to assess the academic rigor and practical\nutility of the generated surveys. Following the prompt design and\nevaluation protocols from previous studies [ 9,10,15,20], we assess\nsurvey quality across five key metrics:Coverage, which measures\nhow thoroughly the survey captures major concepts, foundational\nworks, and emerging trends;Structure, which examines logical\norganization, coherence, and taxonomy quality;Relevance, which\nassesses the alignment of content with the target research topic;\nSynthesis, which evaluates the integration of information from\nmultiple sources into a cohesive and non-redundant narrative; and\nCritical Analysis, which reflects the survey\u2019s ability to identify\nmethodological gaps, highlight trends, and articulate open research\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Minh-Anh et al.\nTopic Ground Truth Survey Citation\nVisual Transformer A Survey of Visual Transformers 405\nHallucination in Large Language Models Siren\u2019s Song in the AI Ocean: A Survey on Hallucination in LLMs 808\nGraph Neural Networks Graph Neural Networks: Taxonomy, Advances, and Trends 129\nDeep Meta-Learning A Survey of Deep Meta-Learning 459\nKnowledge Graph Embedding Knowledge graph embedding: A survey from the perspective of representation\nspaces130\nGeneralized Out-of-Distribution Detection Generalized Out-of-Distribution Detection: A Survey 1406\nReinforcement Learning for Language Processing Survey on reinforcement learning for language processing 206\nExploration Methods in Reinforcement Learning Exploration in Deep Reinforcement Learning: From Single-Agent to Multi-\nAgent Domain194\nStabilizing Generative Adversarial Networks Stabilizing Generative Adversarial Networks: A Survey 149\nRetrieval-Augmented Generation for LLMs Retrieval-Augmented Generation for Large Language Models: A Survey 953\nTable 7: Survey Papers Overview\nchallenges. Each metric is scored on a 0\u2013100 scale by both LLM-\nbased judges and human experts, with higher scores indicating\nstronger performance. The complete evaluation prompts and scor-\ning criteria are detailed in Appendix B.\nB.2.2 Metric about Citation Quality.Following the methodology\nin prior studies [ 5,15], we evaluate the citation quality of the gen-\nerated surveys by measuring both the accuracy and the contextual\nrelevance of cited references. Specifically, we extract"}
{"file": "2510.07733v1.pdf", "section": "5 Conclusion", "chunk": "Metric about Citation Quality.Following the methodology\nin prior studies [ 5,15], we evaluate the citation quality of the gen-\nerated surveys by measuring both the accuracy and the contextual\nrelevance of cited references. Specifically, we extract a set of fac-\ntual claims from each generated survey and verify whether these\nclaims are appropriately supported by their corresponding refer-\nences. To automate this process, we employ a Natural Language\nInference (NLI) model that determines whether the content of each\ncited paper logically supports the associated claim. Based on this\nevaluation, we calculate two key metrics:Citation Recall, which\nreflects the proportion of claims that are correctly supported by\nvalid references, andCitation Precision, which measures the pro-\nportion of cited references that truly substantiate the claims they\nare linked to. Together, these metrics provide a robust measure of\nhow accurately and meaningfully the generated surveys integrate\ncitations within their arguments.\nC Prompt Templates\nThis section presents the prompt templates designed to guide each\nstage of automated literature review generation and evaluation.\nEach template specifies goals, inputs, and evaluation criteria to\nensure consistency and quality across generated outputs.\nC.1 Prompt to generate structured outline\nWe provide a short version of the prompt template (Figure 7) that\ninstructs the model to construct a coherent, hierarchical outline\nthat captures the logical flow of a literature review topic before\ndetailed writing begins.prior to\nC.2 Prompt to evaluate structured outline\nThe prompt in Figure 8 guides the model to write complete, citation-\nbased literature review subsections grounded in the provided focus,\nsummaries, and development directions. The following evaluation\nprompt extends this process to assess individual sections for depth,\nsynthesis, and analytical quality.Goal: Generate a structured Literature Review Outline for:\n\"[QUERY]\"\nINPUT SYNTHESIS DATA\n\u2022Communities:[PAPER_COMMUNITIES]\n\u2022Directions:[DEVELOPMENT_DIRECTIONS]\nREQUIREMENTS & CONSTRAINTS\n1. Structure:\n\u2022Progression: Follow Foundations \u2192Core\u2192Ad-\nvanced\u2192Applications\u2192Future.\n\u2022Mandatory Sections: Must include Introduction,\nFoundational Concepts, and Conclusion.\n\u2022Hierarchy: Use exactlyTWO levels(e.g., 2.1, 2.2).\nNo deeper nesting.\n2. Content & Quality:\n\u2022Create acoherent narrative(evolutionary story,\nnot a list).\n\u2022Group material bymethodological familiesand"}
{"file": "2510.07733v1.pdf", "section": "5 Conclusion", "chunk": "exactlyTWO levels(e.g., 2.1, 2.2).\nNo deeper nesting.\n2. Content & Quality:\n\u2022Create acoherent narrative(evolutionary story,\nnot a list).\n\u2022Group material bymethodological familiesand\nthematic depth.\n\u2022Include dedicated sections for Applications and Fu-\nture Trends/Challenges.\n3. Evidence & Output:\n\u2022Proof IDs: Each subsectionMUSTbe grounded\nwith 1-3 proof_ids (from layer ,community_X , or\nseedIDs).\n\u2022Focus Synthesis: Provide section_focus (broad\ntheme) andsubsection_focus(specific details).\n\u2022Format: Return only aJSON ARRAYof main sec-\ntions and their hierarchical subsections.\nFigure 7: Generate Outline Prompt.\nC.3 Prompt to generate subsections\nThis prompt guides the model to write complete, citation-based\nliterature review subsections grounded in the provided focus, sum-\nmaries, and development directions (Figure 9).\nSurveyG: A Multi-Agent LLM Framework with Hierarchical Citation Graph for Automated Survey Generation Conference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nEvaluate the quality and structure of the following liter-\nature review outline. Assess whether the outline demon-\nstrates meaningful organization of works rather than a sim-\nple concatenation of summaries.Your feedback should\ninclude:\n\u2022Strengths of the outline.\n\u2022Weaknesses or issues (if any).\n\u2022Specific suggestions for improvement (only if issues\nare found).\n\u2022Final score (1-5, with 5 being the maximum) evalu-\nate overall organization, coherence, and coverage.\nOutline to evaluate: {outline_text}\nFigure 8: Prompt to evaluate structured outline\nTask:Write a comprehensive literature review subsection\ntitled[SUBSECTION_TITLE]in LaTeX.\nInputs:\n\u2022Focus:[SUBSECTION_FOCUS]\n\u2022Community summaries:[COMMU-\nNITY_SUMMARY]\n\u2022Development directions:[DEVELOP-\nMENT_DIRECTION]\n\u2022Papers (chronological):[PAPER_INFO]\nGuidelines:\n\u2022Use LaTeX format with citations\n(\\cite{citation_key}).\n\u2022Minimum 400 words, no numbered subsection titles.\n\u2022Focus strictly on the subsection topic.\n\u2022Each paper: 2-3 sentences describing technical con-\ntributions.\n\u2022Connect papers by showing how later work ad-\ndresses earlier limitations.\n\u2022Conclude with remaining challenges or future di-\nrections.\nAvoid:sequential listing"}
{"file": "2510.07733v1.pdf", "section": "5 Conclusion", "chunk": "subsection topic.\n\u2022Each paper: 2-3 sentences describing technical con-\ntributions.\n\u2022Connect papers by showing how later work ad-\ndresses earlier limitations.\n\u2022Conclude with remaining challenges or future di-\nrections.\nAvoid:sequential listing, vague critiques, unsupported\nclaims, isolated descriptions, or ignoring contradictions.\nFigure 9: Generate Subsection Prompt Short Version\nC.4 Improve Section Quality\nAs shown in Figure 10, this prompt systematically assesses litera-\nture review sections across multiple dimensions such as content\ncoverage, synthesis, and critical analysis while offering actionable\nfeedback and retrieval suggestions for refinement.Evaluate the quality of the following literature review sec-\ntion within the context of the overall survey outline. Your\nevaluation should address the following aspects, each rated\nfrom 1-5 (5 = excellent): (1) content coverage, (2) citation\ndensity, (3) academic rigor, (4) synthesis across works, (5)\ncritical analysis, (6) coherence, (7) depth of discussion, and\n(8) specificity of scope.\nIn addition to numeric ratings, provide:\n- A brief natural language summary of the section\u2019s\nstrengths and weaknesses,\n- An overall score (1-5) with justification,\n- Suggestions for improvement, focusing on areas that fall\nshort,\n- A list of search queries that could retrieve additional\nrelevant literature to strengthen the section.\nFigure 10: Section Quality and Retrieval Prompt\nD Case studies\nWe provided a subsection generated bySurveyG(Figure 11) to\nillustrate its ability to synthesize complex research trends in mod-\nular and agentic RAG. Overall, this subsection highlights a clear\nprogression in the RAG landscape from simple retrieval pipelines to-\nward multi-stage, agentic, and modular architectures. The discussed\nworks collectively show how LLMs are evolving from passive gen-\nerators to proactive reasoning agents capable of planning, coordi-\nnation, and self-optimization. The emergence of meta-frameworks\nsuch as AutoRAG and FlashRAG further reflects a shift toward au-\ntomated orchestration of RAG components, underscoring a broader\ntrend toward unified, adaptive systems that integrate retrieval and\nreasoning for scalable knowledge synthesis.\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Minh-Anh et al.\nMulti-stage and Modular RAG Frameworks\nThe foundational paradigm"}
{"file": "2510.07733v1.pdf", "section": "5 Conclusion", "chunk": "systems that integrate retrieval and\nreasoning for scalable knowledge synthesis.\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY Minh-Anh et al.\nMulti-stage and Modular RAG Frameworks\nThe foundational paradigm of Retrieval-Augmented Generation (RAG) typically operates on a straightforward \"retrieve-then-\ngenerate\" sequence [lewis2020pwr]. However, as Large Language Models (LLMs) are increasingly tasked with complex, multi-faceted\nqueries and dynamic information needs, this simple pipeline proves insufficient [huang2024a59, zhao2024931]. This has spurred\nthe evolution of RAG into more sophisticated, multi-stage, and modular architectures, where the LLM transcends a passive role to\nbecome an intelligent agent capable of proactive planning, dynamic decision-making, and the orchestration of various sub-tasks\n[gao20238ea]. This section focuses on frameworks that empower LLMs to actively manage the information-seeking process through\niterative planning, query decomposition, and the dynamic assembly of specialized modules. It is crucial to distinguish these proactive,\nagentic approaches from reactive or corrective mechanisms (e.g., self-correction, re-ranking) that primarily refine retrieval quality,\nwhich are discussed in detail in Section 3.\nA significant advancement in modular RAG involves empowering LLMs to act as sophisticated planning agents, iteratively refining\ntheir information-seeking process and orchestrating multi-round interactions. [lee2024hif] introduced PlanRAG, which extends\nthe popular ReAct framework by incorporating explicit \"Plan\" and \"Re-plan\" steps. This allows LLMs to dynamically generate\nand iteratively refine analytical approaches based on intermediate retrieval results, effectively acting as decision-makers for\ncomplex data analysis tasks. Similarly, [yang20243nb] presented IM-RAG, a multi-round RAG system that leverages learned inner\nmonologues and a multi-agent reinforcement learning approach. In IM-RAG, an LLM-based \"Reasoner\" dynamically switches\nbetween a \"Questioner\" role (crafting queries) and an \"Answerer\" role, guided by mid-step rewards from a \"Progress Tracker, \" leading\nto flexible and interpretable multi-round information gathering. Building on the concept of autonomous interaction, [yu2024c32]\u2019s\nAuto-RAG enables LLMs to engage in multi-turn dialogues with the retriever, systematically planning"}
{"file": "2510.07733v1.pdf", "section": "5 Conclusion", "chunk": "to flexible and interpretable multi-round information gathering. Building on the concept of autonomous interaction, [yu2024c32]\u2019s\nAuto-RAG enables LLMs to engage in multi-turn dialogues with the retriever, systematically planning retrievals and refining queries\nuntil sufficient external information is gathered. This framework highlights the LLM\u2019s powerful decision-making capabilities,\nautonomously adjusting iterations based on query difficulty and knowledge utility. Another approach, [wang2024zt3]\u2019s M-RAG,\nproposes a multi-partition paradigm for external memories, employing a multi-agent reinforcement learning framework with an\n\"Agent-S\" for dynamic partition selection and an \"Agent-R\" for memory refinement. This enables more fine-grained and focused\nretrieval by orchestrating memory access across different knowledge partitions. To further optimize the interaction between these\nmodular components, [li20243nz]\u2019s RAG-DDR (Differentiable Data Rewards) offers an end-to-end training method that aligns data\npreferences between different RAG modules (agents). By collecting rewards and evaluating the impact of perturbations on the\nentire system, RAG-DDR optimizes agents to produce outputs that enhance overall RAG performance, particularly for smaller LLMs.\nThese agentic frameworks collectively transform RAG into a dynamic, adaptive system capable of tackling complex, multi-hop\nqueries that require sophisticated reasoning and iterative information synthesis.\n...\nIn conclusion, the evolution towards multi-stage and modular RAG frameworks marks a significant advancement, transforming RAG\nfrom a simple pipeline into an intelligent, adaptive system. By enabling LLMs to engage in iterative refinement, agentic planning,\nand dynamic orchestration of sub-tasks, these architectures enhance robustness, reduce hallucinations, and improve the depth and\nfaithfulness of generated responses, particularly for complex, multi-hop queries [tang2024i5r]. However, this sophistication often\nintroduces challenges related to increased computational overhead, the complexity of orchestrating multiple modules, and the need\nfor robust evaluation methodologies that can accurately assess the contributions of each stage and the overall system performance.\nBenchmarks like [friel20241ct]\u2019s RAGBench, [krishna2024qsh]\u2019s FRAMES, and [tang2024i5r]\u2019s MultiHop-RAG highlight these\nchallenges, emphasizing the need for explainable metrics and unified frameworks to evaluate the"}
{"file": "2510.07733v1.pdf", "section": "5 Conclusion", "chunk": ", [krishna2024qsh]\u2019s FRAMES, and [tang2024i5r]\u2019s MultiHop-RAG highlight these\nchallenges, emphasizing the need for explainable metrics and unified frameworks to evaluate the intricate interplay of retrieval,\nreasoning, and generation in these advanced systems. Future research will likely focus on optimizing the efficiency of these\nmulti-stage processes, developing more autonomous and self-correcting agents, and creating more generalized frameworks that\ncan seamlessly integrate diverse knowledge sources and reasoning paradigms while addressing the inherent trade-offs between\ncomplexity and efficiency.\nFigure 11: Case studies about the result of generated subsection."}
{"file": "2510.07772v1.pdf", "section": "Introduction", "chunk": "An Approach for Systematic Decomposition of Complex LLM Tasks\nTianle Zhou\u00a7, Jiakai Xu\u00a7, Guanhong Liu\u00a7, Jiaxiang Liu\u00a7, Haonan Wang\u00a7, Eugene Wu\u2217\u00a7\n\u00a7Columbia University\nAbstract\nLarge Language Models (LLMs) suffer from\nreliability issues on complex tasks, as exist-\ning decomposition methods are heuristic and\nrely on agent or manual decomposition. This\nwork introduces a novel, systematic decompo-\nsition framework that we call Analysis of\nCONstraint-Induced Complexity (ACONIC ),\nwhich models the task as a constraint problem\nand leveraging formal complexity measures to\nguide decomposition. On combinatorial (SAT-\nBench) and LLM database querying tasks (Spi-\nder), we find that by decomposing the tasks\nfollowing the measure of complexity, agent can\nperform considerably better (10-40 percentage\npoint).\n1 Introduction\nLarge Language Models (LLMs) have demon-\nstrated impressive competence across a wide range\nof reasoning, programming, and problem-solving\ntasks. Yet, when faced withcomplex tasksthat re-\nquire deep multi-step reasoning or combinatorial\nsearch, even state-of-the-art models often fail to\nproduce correct results in a single forward pass.\nA growing body of work addresses this limita-\ntion throughtask decomposition. Instead of solving\na task monolithically, these methods break it into\nsmaller, more tractable subtasks. One popular line\nof work, starting frorm chain-of-though (Wei et al.,\n2022), is to use LLMs for decomposition (Yao et al.,\n2023; Khot et al., 2023; Pourreza and Rafiei, 2023;\nChen et al., 2024). Other methods rely on domain\nexperts to decompose the task into workflows and\nprovide access to tools that shoulder parts of the\ntask (Wang et al., 2024; Singh et al., 2024). We\nsummarize these methods as the grey path in Fig-\nure 1.\nWhile decomposition seeks to break more com-\nplex tasks into workflows of simpler subtasks, ex-\nisting approaches are largely heuristic. When is a\ntask \u201ccomplex\u201d? How should it be decomposed?\nFigure 1: Given a scheduling task, existing approaches\nuse heuristic LLM or manual task"}
{"file": "2510.07772v1.pdf", "section": "1 Introduction", "chunk": "tasks into workflows of simpler subtasks, ex-\nisting approaches are largely heuristic. When is a\ntask \u201ccomplex\u201d? How should it be decomposed?\nFigure 1: Given a scheduling task, existing approaches\nuse heuristic LLM or manual task decomposition meth-\nods (top row). Our framework reduces the task into a\nconstraint satisfaction problem that allows for a system-\natic decomposition that minimizes the problem com-\nplexity (bottom row).\nA principled measure of task complexity would\nenable systematic decomposition strategies and the\nability to study tasks of comparable difficulty, and\nprovide guidance on when tools are needed.\nIn this paper, we introduce a formal complexity\nframework for LLM tasks by reducing them into\nconstraint satisfaction problems(specifically, 3-\nSAT). We use properties of the induced constraint\ngraph (graph size and treewidth) as measures of\ntask complexity. Building on these measures, we\npropose a decomposition method based on (Bod-\nlaender, 1998) that minimizes subtask complexity\nunder this formalization, yielding decompositions\nthat preserve global satisfiability while maximizing\nlocal solvability. Along the purple path in Figure 1,\nwe first reduce the task\u2014modeled as a context that\ndescribes a set of constraints and a query that must\nreason over the constraints\u2014into a formal con-\nstraint satisfiability problem. We then decompose\nthe constraint problem and construct a workflow\nover the subtasks defined for each subproblem.\nWe use SAT-BENCH(Wei et al., 2025b) and the\nNL2SQL SPIDER(Yu et al., 2019) benchmark to\nstudy this framework. We find that the task mea-\nsures definefrontiers of difficultythat separatearXiv:2510.07772v1  [cs.AI]  9 Oct 2025\ntasks easily solvable by LLMs from those that are\nnearly impossible without structural assistance. In\naddition, decomposition based on complexity im-\nproves task accuracy by 9\u221240% as compared to\nchain-of-thought decomposition.\n2Reduction to Constraint Representation\nAt a high level, our reduction happens in two steps.\nThe first models the agent\u2019s task using a state-based\nframework that captures the interaction between\nthe agent and its observations of the system en-\nvironment. This enables us to model the agent\u2019s\nplanning as a satisf"}
{"file": "2510.07772v1.pdf", "section": "1 Introduction", "chunk": "happens in two steps.\nThe first models the agent\u2019s task using a state-based\nframework that captures the interaction between\nthe agent and its observations of the system en-\nvironment. This enables us to model the agent\u2019s\nplanning as a satisfiability (PaS) problem (Selman\net al.), and further reduce this to a standard con-\nstraint satisfiability problem. In our experiments,\nwe develop automatic reduction procedures for two\nbenchmarks.\n2.1 Reduction to Planning Problems\nThe state based framework of agent operations (e.g.\nAgent Path Finding (Surynek et al., 2016)) can be\nformulated as PaS problems (Selman et al.). The\nframework transfers world states and actions into\nfinite state abstractions\nP=\u27e8F, A, I, G\u27e9,\nwhere Fis a finite set of propositionalfluentsde-\nscribing world facts, Ais a finite set ofactions, and\nI,Gdetermines the initial and goal fluents. For\na given action a,P(a)\u2286F determines the pre-\nconditions, A(a)\u2286F determines the fluents that\nbecome true, and D(a)\u2286F determines the fluents\nthat become false. An agent task becomes deter-\nmining a sequence of actions \u27e8a1, . . . , a k\u27e9such\nthat, starting from I, each aiis applicable (its pre-\nconditions hold), and executing its effects (adding\nand deleting fluents) transfers current state to a\nnew state. The plan is valid if the final state after\nexecuting all actions satisfies all goals inG.\nConsider a toy meeting scheduling problem:\nMotivation Example \u2014 Meeting Scheduling.\nAlice needs to meet with Bob and Charlie separately\nand asks an agent to schedule both meetings. The\nsystem records show:\n\u2022Alice: [\"Alice is available at morning in office r\",\n\"Alice is available in the afternoon at office t\"]\n\u2022Bob: [\"Bob is available in the afternoon at office t\"]\n\u2022Charlie: [\"Charlie is available at morning in office\nr\", \"Charlie is available in the afternoon at office t\"]\nTask.How should the agent compose the invitation\nemails to Alice\u2013Bob and Alice\u2013Charlie?In the toy example above, let us define x(t,\u2113)\npto\nbe a Boolean action variable indicating that person\npis invited to location \u2113at time"}
{"file": "2510.07772v1.pdf", "section": "2.1 Reduction to Planning Problems", "chunk": "the agent compose the invitation\nemails to Alice\u2013Bob and Alice\u2013Charlie?In the toy example above, let us define x(t,\u2113)\npto\nbe a Boolean action variable indicating that person\npis invited to location \u2113at time t, where P,T,\nandLdenote the global set of people, times, and\nlocations, respectively. For each person p, letAp\u2286\nT \u00d7 L be the set of feasible (time, location) pairs.\nFor each pair (p, p\u2032), define the pairwise feasible\nsetFp,p\u2032=A p\u2229Ap\u2032as given in the system record.\nWe model the state as the set of value assign-\nments {x(t,\u2113)\np|p\u2208 P,(t, \u2113)\u2208 T \u00d7 L, x(t,\u2113)\np\u2208\n{0,1}} , indicates whether participant pattends\nslot(t, \u2113) . An action consists of assigning some\nx(t,\u2113)\np= 1.Then a target set can be constructed\nby requiring that for each desired meeting pair\n(p, p\u2032)\u2208 {(A, B),(A, C)} , exactly one common\nfeasible slot is chosen where both pandp\u2032are\npresent. Formally, defining meeting at \u2113, tfor\n(p, p\u2032)to be M(t,\u2113)\np,p\u2032:=x(t,\u2113)\np\u2227x(t,\u2113)\np\u2032, the goal con-\ndition can be written as:\nG:^\n(p,p\u2032)EO\b\nM(t,\u2113)\np,p\u2032|(t, \u2113)\u2208 F p,p\u2032\n\u2227\n^\n(t,\u2113)\u00ac\u0000\nM(t,\u2113)\nA,C\u2227M(t,\u2113)\nA,B\u0001\n,(1)\nwhere EOdenotesExactly One, which can be\neasily represented with at least one, and at most\none. Allx(t,\u2113)\npinitialized to be False.\nThe agent must produce a sequence of actions\nthat comply with the preconditions and effects de-\nfined in P=\u27e8F, A, I, G\u27e9 , thus transiting the states\ntoward a goal-satisfying state.\nOur objective is to find a problem decomposition\nstrategy that provide observations over the subset of\nthe"}
{"file": "2510.07772v1.pdf", "section": "2.1 Reduction to Planning Problems", "chunk": "fined in P=\u27e8F, A, I, G\u27e9 , thus transiting the states\ntoward a goal-satisfying state.\nOur objective is to find a problem decomposition\nstrategy that provide observations over the subset of\nthe problem. It should ensure that the agent\u2019s local\nconsistency over observed subgraphs guarantees\nglobal satisfiability ofG.\n2.2 Reduction to Constraint Satisfaction\nGiven the planning-as-satisfaction formulation\nP=\u27e8F, A, I, G\u27e9 , we can reduce the problem to a\nconstraint satisfaction instance by encoding the pre-\nconditions fp\u2208 P(a) , add effects fa\u2208 A(a) , and\ndelete effects fd\u2208 D(a) of each action as Boolean\ndependent constraints between fluents and actions.\nAdditionally, we include a k-wise constraint in-\nvolving all variables in P(a)\u222a A(a)\u222a D(a) to\nenforce frame consistency and prevent anomalous\nmodels (Selman et al.) where fluents change truth\nvalues without any triggering action. By addition-\nally encoding the goal states Gas constraints, solv-\ning the resulting CSP is equivalent to solving the\noriginal PaS problem.\nIn constraint processing theory, tree decompo-\nsition (Bodlaender, 1998) on a constraint graph\nprovides desirable guarantees of consistency and\nmodularity: when traversing along the tree, any\nvariable assignment made within one subgraph\ndoes not affect previously satisfied clauses except\nthrough shared boundary variables (which is al-\nways included in current subproblem). We there-\nfore chose tree decomposition to separate the CSP\ninto subproblems, the completion of which lead\nto global success of the problem. This ensures\nthat local consistency over the bags implies global\nconsistency of the full CSP solution.\n(Bodlaender, 1998) show existence of a tree de-\ncomposition D\u2217with the smallest maximum bag\nsize (treewidth), which characterizes the intrinsic\nproblem complexity. Leveraging this prior work,\nACONIC uses minimal tree decomposition to parti-\ntion reasoning tasks into the minimal locally consis-\ntent subtasks while preserving global satisfiability.\n3 Experiments\nWe now evaluate our framework against chain-\nof-thought decomposition (Wei et al., 2022) on\ntwo benchmarks: SAT-based story problems (SAT-\nbench) and natural language to SQL (Spider).\n3.1"}
{"file": "2510.07772v1.pdf", "section": "3 Experiments", "chunk": "3 Experiments\nWe now evaluate our framework against chain-\nof-thought decomposition (Wei et al., 2022) on\ntwo benchmarks: SAT-based story problems (SAT-\nbench) and natural language to SQL (Spider).\n3.1 SAT-Bench\nSAT-Bench (Wei et al., 2025a) uses (i) an un-\nderlying SAT problem to construct (ii) a natural-\nlanguage story describing the same constraints, and\n(iii) an alignment from story entities to their SAT\nrepresentations. Instead of asking the LLM for an\nimmediate satisfiability judgment, we prompt it to\nproduce value assignments step-by-step to ensure\na complete understanding of the assignment pro-\ncess. In this process, the state\u2013action dependency is\nmodeled by casting SAT as a PaS problem, where\nthe goal state is defined by the original SAT-Bench\nCNF representation.\nAt each round of variable assignment, the agent\nis provided with the story, the variable to SAT map-\nping rules, and a set of observations. We tested\nagent with two different configurations of observa-\ntions. In the chain-of-thought baseline, the agent\nobserves all condition observations. In the tree-\ndecomposition setup, the agent instead observes\nall conditions belonging to the same subproblem\ninduced by the decomposition.\n(a) Claude baseline\n(b) ClaudeACONIC\n(c) LLaMA baseline\n(d) LLaMAACONIC\nFigure 2: Our complexity measures definefrontiers of\ndifficulty. SOTA models like Claude shifts the bound-\naries towards the right, while ACONIC \u2019s decomposition\nconsistently pushes the frontiers towards more complex\nproblems.\nWe evaluated Llama-3-70B on all tasks and\nClaude3.5-Sonnet on half the tasks (randomly sam-\npled). Figure 2 plots the success/failure of each\ntask by its complexity measures, and exhibits fron-\ntiers of difficulty beyond which tasks are too dif-\nficult or too simple. For the baseline, there ap-\npears to be a fixed \u201ctotal task complexity\u201d evi-\ndenced by the trade-off between problem treewidth\nand number of bags (left column). In contrast,\nACONIC decomposition pushes the frontier outward\nand is able to successfully complete more complex\ntasks. Overall, ACONIC increases the task comple"}
{"file": "2510.07772v1.pdf", "section": "3.1 SAT-Bench", "chunk": "enced by the trade-off between problem treewidth\nand number of bags (left column). In contrast,\nACONIC decomposition pushes the frontier outward\nand is able to successfully complete more complex\ntasks. Overall, ACONIC increases the task comple-\ntion rate from 49.3%\u219258.1% using Claude, and\n21.5%\u219236.5% using LLaMA\u2014a 9\u221215% im-\nprovement on both models.\n3.2 Natural Language to SQL\nNatural language to SQL (NL2SQL) tasks translate\na natural language query over a database into a de-\nsired SQL query. A major challenge is constructing\nthe join condition to connect tables referenced in\nthe query. For example, the schema in Figure 3 con-\ntains 4 tables about singers and concerts. Although\nthe NL query only refers to singers and concerts,\nthe LLM needs to infer that singer_in_concern\nis needed for a valid join path.\nThe complexities of both the query (the num-\nber of tables referenced) and the database schema\nNL Query qHow manysingers haveconcerts inLondon?StadiumPKStadiumIDFKConcertIDSingerSinger_concertConcertExtracttablesSingerPKSingerIDSinger_NameSinger_concertFKConcertIDFKSingerIDConcertPKConcertIDConcert_Location\nStadiumDecomposeSingerSinger_concertqAgentWITH t1 AS (SELECT ...  FROM singer,        singer_concert  WHERE ... )ConcertSinger_concertqAgentWITH t1 AS (SELECT ...  FROM concert,        singer_concert  WHERE ... )AgentVerifierConstraintRepresentationSchema\nSQL QueryFigure 3: Instead of giving the full database schema and\nNL query qto an agent, ACONIC decomposes the NL task\ninto two subtasks, each given a subset of the database\nschema, q, and the output of the previous subtask if any.\n(number of tables and their foreign key graph) af-\nfect the agent\u2019s ability to construct queries with the\nappropriate join graphs. In addition, database the-\nory (Gottlob et al., 2001) already models a database\nas a constraint graph (tables are nodes, and foreign\nkeys are edges) and a query as a subgraph; this\nrepresentation has also been used for NL2SQL\napproaches (Wang et"}
{"file": "2510.07772v1.pdf", "section": "3.2 Natural Language to SQL", "chunk": "2001) already models a database\nas a constraint graph (tables are nodes, and foreign\nkeys are edges) and a query as a subgraph; this\nrepresentation has also been used for NL2SQL\napproaches (Wang et al., 2019). Thus, we use\nNL2SQL to study decomposition.\nWe use the popular Spider NL2SQL benchmark\ndataset (Yu et al., 2019). It contains hundreds\nof databases, each containing up to 37 tables, 90\nforeign keys, and potentially over 100 columns.\nEach task consists of a database schema S, the\nnatural language (NL) query q, and a ground-truth\nSQL query q\u2217. The original benchmark submits\n(S, q) to the LLM, which can result in incorrect\nor invalid join conditions. Given the tables refer-\nenced in q,ACONIC decomposes the schema into\nsubgraphs with minimized maximal complexities\u00e5\nand constructs a workflow (grey arrows). For in-\nstance, the first subtask gives the agent qand the\nschemas for subgraph singer-singer_concern\n(red arrow), and asks it to construct the appropriate\nWITH clause. The next subtask does the same with\nthe second subgraph (orange arrow), but is also\nprovided the output from the previous subtask. A\nverification agent cleans up minor errors to build\nEasy Medium Hard Extra0.00.20.40.60.8Pass@k75.8%\n58.1%62.7%\n37.9%82.8%\n65.3%67.5%\n44.7%85.7%\n69.1%75.7%\n50.9%pass@1 pass@2 pass@3Figure 4:ACONICresult on Llama3-70B\nEasy Medium Hard Extra0.00.10.20.30.4Pass@k42.7%\n38.1%\n36.2%\n19.3%43.5%\n39.5%\n37.4%\n23.5%43.5%\n39.9% 39.7%\n24.1%pass@1 pass@2 pass@3\nFigure 5: Baseline CoT on Llama3-70B\nthe final SQL query.\nFigure 4 and Figure 5 respectively report accu-\nracy for ACONIC and the CoT baseline."}
{"file": "2510.07772v1.pdf", "section": "3.2 Natural Language to SQL", "chunk": "2 pass@3\nFigure 5: Baseline CoT on Llama3-70B\nthe final SQL query.\nFigure 4 and Figure 5 respectively report accu-\nracy for ACONIC and the CoT baseline. Each bar\nreports the best of 1, 2, or 3 agent trajectories. We\nfind that while increasing the number of trajecto-\nries slightly improves the accuracy, decomposition\nusing ACONIC increases the accuracy over CoT by\n\u224830\u221240%across the conditions.\n4 Conclusions\nACONIC is a principled framework for decompos-\ning complex LLM tasks by reducing them to con-\nstraint satisfaction problems and quantifying com-\nplexity via treewidth. Unlike heuristic methods,\nACONIC minimizes local complexity while preserv-\ning global satisfiability. Across SATBench and Spi-\nder, this yieldedfrontiers of task difficultythat\ndefine LLM reasoning limits. Complexity-based\ndecomposition improved completion rates by up\nto15% on SAT-bench and up to 40% on SPIDER.\nThese results suggest a path towards theoretically\ngrounded, reliable multi-step LLM systems.\n5 Limitations\nACONIC provides a principled framework to ana-\nlyze and reduce task complexity, but it does not\nyet constitute a fully autonomous decomposition\nor reasoning system. It is also not intended to be a\ncomplete solution for general task decomposition,\nbut rather as a theoretical and empirical examina-\ntion of howconstraint-induced complexityaffects\nthe problem-solving ability of LLMs.\nOur focus has been on evaluating how\ncomplexity-guided decomposition impacts perfor-\nmance relative to heuristic baselines such as chain-\nof-thought prompting, rather than on direct compar-\nison with other theoretically grounded or learning-\nbased decomposition frameworks. For this rea-\nson, we also do not claim that our final decompo-\nsition system is superior to existing solutions to\nthe benchmark tasks. For instance, SAT-bench can\nbe directly solved using a constraint solver. Simi-\nlarly, the NL2SQL tasks can be readily solved using\nsimple path-finding algorithms over the database\nschema.\nThe tasks that we evaluated can be conveniently\nmodeled as constraint satisfiability problems. Al-\nthough many other practical problems, such as\ndeadlock detection in databases, resource schedul-\ning in operating systems, task placement in dis-"}
{"file": "2510.07772v1.pdf", "section": "5 Limitations", "chunk": "schema.\nThe tasks that we evaluated can be conveniently\nmodeled as constraint satisfiability problems. Al-\nthough many other practical problems, such as\ndeadlock detection in databases, resource schedul-\ning in operating systems, task placement in dis-\ntributed systems, and control/data-flow construc-\ntion in agentic programming, can be formulated as\nconstraint satisfiability problems, they often can-\nnot be logically represented completely, either due\nto question ambiguity, lack of transparancy of the\nagent actions, or fuzzy contextual information. In\nthese cases, future work might study hybrid decom-\nposition approaches that mix logical and common-\nsense constraints in a task.\nReferences\nHans L Bodlaender. 1998. A partial k-arboretum of\ngraphs with bounded treewidth.Theoretical com-\nputer science, 209(1-2):1\u201345.\nZhiruo Chen, Simeng Qiu, Victor Zhong, and Percy\nLiang. 2024. Divide-and-conquer prompting for com-\nplex reasoning.arXiv preprint arXiv:2402.05359.\nGeorg Gottlob, Nicola Leone, and Francesco Scarcello.\n2001. Hypertree decompositions: A survey. InInter-\nnational Symposium on Mathematical Foundations\nof Computer Science, pages 37\u201357. Springer.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2023. Decomposed prompting: A modularapproach for solving complex tasks.arXiv preprint\narXiv:2306.04179.\nMohammadreza Pourreza and Davood Rafiei. 2023.\nDin-sql: Decomposed in-context learning of text-\nto-sql with self-correction.Advances in Neural Infor-\nmation Processing Systems, 36:36339\u201336348.\nBart Selman et al. Planning as satisfiability.\nAmanpreet Singh, Chenglei Si, Kurt Shuster, Jing Xu,\nand Jason Weston. 2024. Agent\u2013computer interfaces:\nAn approach for task-grounded agent evaluation. In\nNeurIPS.\nPavel Surynek, Ariel Felner, Roni Stern, and Eli Bo-\nyarski. 2016. Efficient sat approach to multi-agent\npath finding under the sum of costs"}
{"file": "2510.07772v1.pdf", "section": "5 Limitations", "chunk": "ed agent evaluation. In\nNeurIPS.\nPavel Surynek, Ariel Felner, Roni Stern, and Eli Bo-\nyarski. 2016. Efficient sat approach to multi-agent\npath finding under the sum of costs objective. InPro-\nceedings of the twenty-second european conference\non artificial intelligence, pages 810\u2013818.\nBailin Wang, Richard Shin, Xiaodong Liu, Oleksandr\nPolozov, and Matthew Richardson. 2019. Rat-sql:\nRelation-aware schema encoding and linking for text-\nto-sql parsers.arXiv preprint arXiv:1911.04942.\nXingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang,\nYunzhu Li, Hao Peng, and Heng Ji. 2024. Executable\ncode actions elicit better llm agents.arXiv preprint\narXiv:2402.01030.\nAnjiang Wei, Yuheng Wu, Yingjia Wan, Tarun Suresh,\nHuanmi Tan, Zhanke Zhou, Sanmi Koyejo, Ke Wang,\nand Alex Aiken. 2025a. Satbench: Benchmarking\nllms\u2019 logical reasoning via automated puzzle genera-\ntion from sat formulas.\nAnjiang Wei, Yuheng Wu, Yingjia Wan, Tarun Suresh,\nHuanmi Tan, Zhanke Zhou, Sanmi Koyejo, Ke Wang,\nand Alex Aiken. 2025b. Satbench: Benchmark-\ning llms\u2019 logical reasoning via automated puz-\nzle generation from sat formulas.arXiv preprint\narXiv:2505.14615.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2022. Chain-of-thought prompting\nelicits reasoning in large language models.arXiv\npreprint arXiv:2201.11903.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nTom Griffiths, Pengfei Cao, Karthik Narasimhan, and\nYingyu Song. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models.arXiv\npreprint"}
{"file": "2510.07772v1.pdf", "section": "5 Limitations", "chunk": ",\nTom Griffiths, Pengfei Cao, Karthik Narasimhan, and\nYingyu Song. 2023. Tree of thoughts: Deliberate\nproblem solving with large language models.arXiv\npreprint arXiv:2305.10601.\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,\nDongxu Wang, Zifan Li, James Ma, Irene Li, Qingn-\ning Yao, Shanelle Roman, Zilin Zhang, and Dragomir\nRadev. 2019. Spider: A large-scale human-labeled\ndataset for complex and cross-domain semantic pars-\ning and text-to-sql task."}
{"file": "2510.08149v1.pdf", "section": "Introduction", "chunk": "AI Knowledge Assist: An Automated Approach for the Creation of\nKnowledge Bases for Conversational AI Agents\nMd Tahmid Rahman Laskar, Julien Bouvier Tremblay\nXue-Yong Fu,Cheng Chen,Shashi Bhushan TN\nDialpad Inc.\n{tahmid.rahman,julien.bouviertremblay,xue-yong,cchen,sbhushan}@dialpad.com\nAbstract\nThe utilization of conversational AI systems\nby leveraging Retrieval Augmented Generation\n(RAG) techniques to solve customer problems\nhas been on the rise with the rapid progress\nof Large Language Models (LLMs). How-\never, the absence of a company-specific ded-\nicated knowledge base is a major barrier to\nthe integration of conversational AI systems\nin contact centers. To this end, we introduce\nAI Knowledge Assist, a system that extracts\nknowledge in the form of question\u2013answer\n(QA) pairs from historical customer -agent con-\nversations to automatically build a knowledge\nbase. Fine -tuning a lightweight LLM on in-\nternal data demonstrates state-of-the-art per-\nformance, outperforming larger closed-source\nLLMs. More specifically, empirical evalua-\ntion on 20 companies demonstrates that the\nproposed AI Knowledge Assist system that\nleverages the LLaMA-3.1-8B model can elim-\ninate the cold -start gap in contact centers\nby achieving above 90% accuracy in extract-\ning information -seeking question-answer pairs\nfrom conversations. This enables immediate\ndeployment of RAG-powered chatbots.\n1 Introduction\nGenerative AI can revolutionize many industries,\nincluding the contact center industry1. With the\ngrowing demand for high-quality customer service,\ncontact centers are constantly seeking ways to im-\nprove their processes (Laskar et al., 2023b). One\nway to achieve this goal is by building conversa-\ntional agents to help answer customer questions\n(Ferraro et al., 2024). Although in real-world sce-\nnarios, contact center virtual agents often rely on a\ncomprehensive knowledge base of question-answer\n(QA) pairs to handle customer inquiries, many en-\nterprises may face a cold start problem if informa-\ntion (e.g., help center articles) related to customer\n1https://www.salesforce.com/ca/service/\ncontact-center/ai/\nFigure 1:"}
{"file": "2510.08149v1.pdf", "section": "1 Introduction", "chunk": "customer inquiries, many en-\nterprises may face a cold start problem if informa-\ntion (e.g., help center articles) related to customer\n1https://www.salesforce.com/ca/service/\ncontact-center/ai/\nFigure 1: An example of knowledge extracted from\ntranscripts in the form of QA pairs.\nquestions are not found in the knowledge base, or if\nthe contact center does not have a knowledge base\nto begin with (Zheng et al., 2023b). This severely\nlimits the adoption of conversational AI agents\nin industries. Meanwhile, building a knowledge\nbase from scratch is time-consuming and deters the\nadoption of such conversational AI systems.\nNonetheless, contact centers may possess a\nwealth of customer service conversation logs (call\ntranscripts and chat histories) that contain repeated\ninformation-seeking questions alongside their res-\nolutions (Laskar et al., 2023b). Turning past in-\nteractions in such historical conversations into an\nFAQ-style knowledge repository can be useful to\ndevelop a knowledge base off the shelf (Agrawal\net al., 2024). This may result in more adoption\nof the chatbot feature, increasing agent efficiency\nby handling customer concerns with the help of a\ndedicated knowledge base, which may ultimately\nlead to improving customer satisfaction.\nIn this paper, we address the cold start problem\nin conversational AI agents. To this end, we present\nAI Knowledge Assist , a Generative AI-powered\nsystem that automatically builds knowledge basesarXiv:2510.08149v1  [cs.CL]  9 Oct 2025\nFigure 2:An overview of our proposed AI Knowledge Assist. First, QA pairs are extracted from historical transcripts. Then\nclustering is applied to group similar QA pairs. Finally, from each cluster, representative QA pairs are constructed and then\nrecommended for the knowledge base (a knowledge manager may review the recommended knowledge before insertion)\nfrom past conversations. More specifically, we\nleverage cost-effective LLMs (Wan et al., 2024) to\nanalyze historical customer-agent conversations to\nextract knowledge in the form of QA pairs (see Fig-\nure 1 for an example) and save them in a knowledge\nbase to address the cold start problem. This paper\ncontains a detailed description of our development\nand evaluation methodology to deployAI Knowl-\nedge Assistin real-world contact"}
{"file": "2510.08149v1.pdf", "section": "1 Introduction", "chunk": "Fig-\nure 1 for an example) and save them in a knowledge\nbase to address the cold start problem. This paper\ncontains a detailed description of our development\nand evaluation methodology to deployAI Knowl-\nedge Assistin real-world contact centers to address\ncustomer concerns. Extensive experiments on real-\nworld datasets demonstrate that the proposedAI\nKnowledge Assistsystem can significantly boost\nthe capabilities of Contact Center AI chatbots to\nbetter handle customer concerns.\n2 Related Work\nThe recent success of LLMs in zero-shot scenarios\nin a wide range of tasks (Laskar et al., 2023a) has\nopened the avenue for new application areas in real-\nworld industrial settings (Zhang et al., 2025; Otani\net al., 2025). This inspires researchers and practi-\ntioners to use LLMs in solving complex tasks that\nrequire the analysis of noisy conversational tran-\nscripts (Saini et al., 2025; Zhu et al., 2025; Laskar\net al., 2023c, 2024b). Moreover, since LLMs gen-\nerate human-like responses, the development of\nconversational AI agents is also on the rise2.\nNonetheless, prior studies on building conversa-\ntional AI agents have several limitations: (i) miss-\ning discussions on how to tackle the cold start prob-\nlem when organizations do not have a dedicated\nknowledge base (Agrawal et al., 2024; Xu et al.,\n2024), (ii) requiring human-annotated large train-\ning datasets (Zheng et al., 2023b) to build models\n2https://www.genesys.com/definitions/\nwhat-is-conversational-ai-for-call-centersfor information extraction from transcripts, which\nis difficult to obtain in real-world industrial scenar-\nios (Fu et al., 2022), (iii) limiting the evaluation\nonly on chat logs (Zheng et al., 2023b), ignoring\nnoisy voice transcripts (Fu et al., 2022).\nWith prior research demonstrating that LLMs\nare effective in analyzing noisy conversational tran-\nscripts (Laskar et al., 2023c), in this paper, we\nproposeAI Knowledge Assist, a system that lever-\nages LLMs to analyze the call transcripts in contact\ncenters and"}
{"file": "2510.08149v1.pdf", "section": "2 Related Work", "chunk": "analyzing noisy conversational tran-\nscripts (Laskar et al., 2023c), in this paper, we\nproposeAI Knowledge Assist, a system that lever-\nages LLMs to analyze the call transcripts in contact\ncenters and extracts relevant knowledge from these\nconversational data in the form of QA pairs. The\nextracted QA pairs are then stored in a knowledge\nbase to address the cold start problem. Contrary\nto prior work, our study focuses on addressing the\ncold start problem in real-world industrial scenar-\nios, with the system being entirely developed in a\ncost-effective manner from noisy transcripts.\n3 Our Proposed Approach\nTheAI Knowledge Assistsystem employs a three-\nstage pipeline, as demonstrated below (also see\nFigure 2).\n3.1 Knowledge Extraction from Transcripts\nThe initial step focuses on extracting potential ques-\ntion and answer pairs from historical call tran-\nscripts. Given a call transcript, an LLM is prompted\nto extract information-seeking questions from cus-\ntomers alongside the corresponding answers pro-\nvided by the agents. Since we utilize voice tran-\nscripts, the LLM is also instructed to rewrite the\nquestion and the answer instead of mere extraction\nwhen needed, such that the QA pairs can be under-\nstood without reading the full conversation. The\nLLM is expected to extract the QA pair as follows:\n{(Qi, Ai)}N(T)\ni=1 =LLM(T;\u03b8)(1)\nHere, model parameters are denoted by \u03b8, which\nsimultaneously extracts and rewrites each QA pair.\nN(T) is the number of QA pairs that the model\nfinds in the transcript T, and (Qi, Ai)denotes the\ni-th QA pair. In this way, we extract QA pairs from\nMtranscripts (T 1, T2, . . . , T M).\n3.2 Clustering for Deduplication\nOnce QA pairs are extracted from different tran-\nscripts, they may exhibit redundancy (e.g., seman-\ntically similar QA pairs may appear in multiple\ntranscripts). If it is not managed, the knowledge\nbase may contain many redundant QA pairs. There-\nfore, our second step involves clustering these QA\npairs into semantically similar groups to facilitate\nthe deduplication and filtering of closely related\nQA pairs. For this purpose, we first measure the\npairwise cosine distance between the question em"}
{"file": "2510.08149v1.pdf", "section": "3.2 Clustering for Deduplication", "chunk": "-\nfore, our second step involves clustering these QA\npairs into semantically similar groups to facilitate\nthe deduplication and filtering of closely related\nQA pairs. For this purpose, we first measure the\npairwise cosine distance between the question em-\nbeddings of every QA pair as follows:\ndist(q i, qj) = 1\u2212qi\u00b7qj\n\u2225qi\u2225\u2225qj\u2225(2)\nHere, qiandqjdenote the embeddings of the ques-\ntions in the ithandjthQA pairs. Finally, a clus-\ntering algorithm is applied to group the QA pairs\nby minimizing the intra-cluster distance and maxi-\nmizing the inter-cluster distance.\n3.3 Recommending Representative QA Pairs\nIn the final step, we again leverage an LLM to pro-\ncess each cluster of QA pairs. For each cluster, the\nmodel selects one or more representative QA pairs\nthat best encapsulate the information in that cluster.\nThis step serves a dual purpose:deduplicationand\nfiltering, by ensuring that highly similar questions\ndon\u2019t lead to redundant entries; and recommenda-\ntion, by proposing well-formed informative QA\npairs for inclusion in the final knowledge base. The\nrepresentative QA pairs in the kthcluster can be\ndefined as follows:\nRk=LLM\u0000\nCk;\u03b8\u0001\n(3)\nHere, Ckis the k-th cluster in the 1, . . . , K clusters\nof QA pairs, LLM(\u00b7;\u03b8) denotes the LLM with pa-\nrameters\u03b8, andR kis the set of representative QA\npairs selected for that cluster. These representative\npairs can either be directly inserted into a knowl-\nedge base or recommended to a Knowledge Man-\nager for human review before final incorporation\ninto the knowledge base.4 Experimental Settings\n4.1 Dataset\nWe collected real-world data over a month (Novem-\nber 2024) from contact centers across 20 client\ncompanies of Dialpad3that consist of customer-\nagent call conversation transcripts generated using\nAutomatic Speech Recognition systems. On aver-\nage, each transcript contains 855 words. To ensure\ncustomer data privacy, the dataset is anonymized\nusing Google Cloud Data Loss Prevention4ser-\nvice. Note that in real-world settings, obtaining\nhuman-annotated data is challenging, which be-\ncomes even more difficult in"}
{"file": "2510.08149v1.pdf", "section": "4.1 Dataset", "chunk": ". To ensure\ncustomer data privacy, the dataset is anonymized\nusing Google Cloud Data Loss Prevention4ser-\nvice. Note that in real-world settings, obtaining\nhuman-annotated data is challenging, which be-\ncomes even more difficult in the context of noisy\nbusiness conversations (Laskar et al., 2022). Con-\nsidering these challenges, alongside the customer\u2019s\ndata privacy concerns, we annotate our collected\ndataset using theGemini-2.5-Pro5model by follow-\ning our proposed approach: (i) QA pair extraction\nfrom transcripts usingGemini-2.5-Pro, (ii) Clus-\ntering the extracted QA pair using the DBSCAN\nalgorithm (Schubert et al., 2017) on the question\nembeddings generated by the BGE-Large6(Chen\net al., 2024) model, and finally (iii) Representative\nQA pair selection using theGemini-2.5-Promodel.\nIn this way, we annotate 27500 instances: 12500\nfor knowledge extraction (5500 for training and\n7000 for evaluation) and 15000 for the recommen-\ndation of representative QA pairs (2500 for training\nand 12500 for evaluation).\n4.2 Model Selection\nSince our focus is to deploy this solution in a\nreal-world industrial setting, we select the model\nto develop the system that can achieve good ac-\ncuracy with faster inference speed and low cost\n(see Appendix A for cost analysis). Therefore, by\nconsidering the accuracy and efficiency of open-\nsource LLMs in real-world settings (Laskar et al.,\n2023c; Fu et al., 2024), we select the LLMs for\nknowledge extraction and final recommendation\nthat has at least 7B parameters (and does not ex-\nceed 10B parameters). More specifically, we used\nthe LLaMA-3.1-8B (Dubey et al., 2024) model due\nto its widespread utilization in real-world industrial\napplications7. For closed-source models, the most\n3https://www.dialpad.com/\n4https://cloud.google.com/security/products/\n5https://deepmind.google/models/gemini/pro/\n6https://hf.co/BAAI/bge-large-en-v1.5\n7https://about.fb.com/news/2025/01"}
{"file": "2510.08149v1.pdf", "section": "4.2 Model Selection", "chunk": "4https://cloud.google.com/security/products/\n5https://deepmind.google/models/gemini/pro/\n6https://hf.co/BAAI/bge-large-en-v1.5\n7https://about.fb.com/news/2025/01/\norganizations-using-llama-solve-industry-challenges/\nModel Precision Recall F1-Score ROUGE-1 ROUGE-2 ROUGE-L BERTScore # QA Pairs\nKnowledge-Assist-8B-SFT 84.88 84.85 84.8641.26 19.68 23.87 60.12 24K\nLLaMA-3.1-8B-Instruct58.29 57.98 58.13 42.37 18.25 25.38 60.80 24K\nDeepSeek-R1-LLaMA-8B51.43 48.10 49.71 39.79 15.45 22.49 58.58 21K\nGPT-4o-Mini74.62 68.68 71.53 49.13 23.79 29.09 67.95 22K\nGemini-2.0-Flash82.29 60.31 69.60 47.14 24.19 28.59 62.81 18K\nGemini-2.0-Flash-Lite72.30 58.81 64.86 47.07 23.70 28.81 62.09 20K\nGemini-2.5-Flash-Lite76.72 70.88 73.6854.17 25.42 28.74 66.8622K\nTable 1:Performance in theKnowledge Extraction from Transcriptsstep. Here, \u2018#\u2019 denotes the number of extracted QA pairs.\nModel Precision Recall F1-Score # QA Pairs\nKnowledge-Assist-8B-SFT 91.4 92.2 91.814K\nGemini-2.5-Flash-Lite81.1 78.1 79.6 13K\nTable 2:End-to-End Performance based onFinal Recom-\nmendation of Representative QA Pairs. Here, \u2018#\u2019 denotes the\nnumber of representative QA pairs that are recommended.\ncost-effective versions"}
{"file": "2510.08149v1.pdf", "section": "4.2 Model Selection", "chunk": ".6 13K\nTable 2:End-to-End Performance based onFinal Recom-\nmendation of Representative QA Pairs. Here, \u2018#\u2019 denotes the\nnumber of representative QA pairs that are recommended.\ncost-effective versions are also preferred. More\nspecifically, we select the mini8versions from Ope-\nnAI and the Flash9versions from Google\u2019s Gemini\nseries models. For clustering, we use the DBSCAN\n(Schubert et al., 2017) algorithm with BGE-Large\n(Xiao et al., 2024; Chen et al., 2024) embeddings\nsince it demonstrates better performance than other\napproaches (e.g., K-Means (Lloyd, 1982)) when\nevaluated in our data (see Appendix B)\n4.3 Implementation\nFor the open-source models, we use HuggingFace\n(Wolf et al., 2020) for implementation, and use\nthe respective API providers for the closed-source\nmodels. For supervised fine-tuning, we use the\nLLaMA-3.1-8B model. A total of 3epochs were\nrun, with the maximum sequence length being set\nto8000 tokens: 4000 for input and 4000 for output.\nThe learning rate was tuned between 2e\u22124 and\n2e\u22126 (inclusive). For response generation, we\nuse the default decoding parameters of each model\n(HuggingFace for open-source, and the official API\nof OpenAI and Google Gemini for closed-source),\nbut keep the input and output token limits similar to\nwhat we use for fine-tuning. All experiments were\nrun on a machine using 8 NVIDIA A100 GPUs.\n4.4 Evaluation Settings\nSince our dataset is annotated byGemini-2.5-Pro,\nwe did not limit the evaluation of our models within\nreference-wise metrics like ROUGE (Lin, 2004) or\nBERTScore (Zhang et al., 2019) due to the pos-\nsibility of the presence of bias in our fine-tuned\n8https://openai.com/index/\ngpt-4o-mini-advancing-cost-efficient-intelligence/\n9https://deepmind.google/models/gemini/flash/models when compared with Gemini annotations.\nInspired by the success of LLMs-as-the-judge (Gu\net al., 2024;"}
{"file": "2510.08149v1.pdf", "section": "4.4 Evaluation Settings", "chunk": "-advancing-cost-efficient-intelligence/\n9https://deepmind.google/models/gemini/flash/models when compared with Gemini annotations.\nInspired by the success of LLMs-as-the-judge (Gu\net al., 2024; Laskar et al., 2024a, 2025), we also pro-\npose the use of an LLM judge for the evaluation of\nLLM-generated outputs in reference-free settings.\nTo avoid any self-enhancement bias (Zheng et al.,\n2023a; Ye et al., 2024) for the models trained using\nourGemini-2.5-Proannotated training data, we did\nnot use Gemini series models as the judge. Instead,\nwe useGPT-4o(OpenAI, 2023) as the judge due to\nits effectiveness in various evaluation tasks (Xiong\net al., 2025). We specifically instructed the LLM\njudge to evaluate the following:\n(i)For the knowledge extraction step, identify\nthe number of QA pairs extracted correctly from\nthe given transcript by following the rules.\n(ii)For the final recommendation step, identify\nthe number of representative QA pairs extracted\ncorrectly from the given cluster following the rules.\nBased on the above information, we compute\nthe Precision, Recall, and F1 scores. For clus-\ntering models\u2019 evaluation, we use the Silhouette\n(Rousseeuw, 1987) metric.\n4.5 Prompt Construction\nTo construct prompts for the knowledge extraction\nstep and knowledge recommendation step, as well\nas for their evaluation using an LLM judge, we con-\nduct extensive prompt engineering on some sam-\npled data to select the best prompt. The selected\nprompts that we use throughout our experiments\ncan be found in Appendix C.\n5 Results and Discussions\nIn this section, we present our experimental find-\nings. We denote our supervised fine-tuned (SFT)\nmodel based on LLaMA-3.1-8B asKnowledge-\nAssist-8B-SFTand compare its performance with\nvarious cost-efficient open-source (LLaMA-3.1-\n8B-Instruct and Deepseek-Distilled-R1-LLaMA-\n8B) and closed-source (GPT-4o-Mini and Gemini-\nFlash) LLMs. The results of our experiments, as\nFigure 3:F1-Score per"}
{"file": "2510.08149v1.pdf", "section": "5 Results and Discussions", "chunk": "seek-Distilled-R1-LLaMA-\n8B) and closed-source (GPT-4o-Mini and Gemini-\nFlash) LLMs. The results of our experiments, as\nFigure 3:F1-Score per Company type for the Knowledge-\nAssist-8B-SFT model in terms of theFinal Recommended\nRepresentative QA Pairs.\ndetailed below, highlight the performance of our\nproposed system in the key stages of knowledge\nextraction and recommendation.\n5.1 Performance on Knowledge Extraction\nfrom Transcripts\nAs shown in Table 1, our fine-tuned model,\nKnowledge-Assist-8B-SFT, which utilizes the\nLLaMA-3.1-8B as the backbone, achieves the best\nperformance in the knowledge extraction task in\nterms of Precision, Recall, and F1-Score, out-\nperforming both closed-source and open-source\nzero-shot baselines. More specifically, our model\nachieved an F1-Score of 84.86%, surpassing\nGPT-4o-Mini (71.53%) and Gemini-2.5-Flash-Lite\n(73.68%). This indicates the efficacy of fine-tuning\non larger LLM-annotated internal datasets for this\ntask. In the reference-wise setting, some closed-\nsource models like Gemini-2.5-Flash-Lite show\nstrong performance in terms of automatic metrics\n(i.e., ROUGE and BERTScore). However, the ref-\nerences in our evaluation dataset were annotated\nby the most powerful model in the Gemini series,\nthe Gemini-2.5-Pro model. On the contrary, our\nKnowledge-Assist-8B-SFT model maintains a com-\npetitive edge in the reference-free setting when\nevaluated by an independent LLM-Judge (i.e., GPT-\n4o). These findings encourage the use of reference-\nfree metrics in real-world settings to mitigate biases\nin evaluation datasets when the datasets are anno-\ntated using LLMs.\n5.2 Clustering Results for Deduplication\nThe next step in ourAI Knowledge Assistsystem\nis clustering, where we group similar QA pairs\nin the same cluster based on the similarity be-\ntween question-question embeddings. We selectModel P R F1\nKnowledge-Assist-8B-SFT 84.88 84.85 84.86"}
{"file": "2510.08149v1.pdf", "section": "5.2 Clustering Results for Deduplication", "chunk": "group similar QA pairs\nin the same cluster based on the similarity be-\ntween question-question embeddings. We selectModel P R F1\nKnowledge-Assist-8B-SFT 84.88 84.85 84.86\n- replace backbone (LLaMA-3.1-8B with Qwen3-8B)77.12 72.48 74.73\n- replace annotator (Gemini-2.5-Pro with GPT-4o)80.45 55.89 65.95\nTable 3:Impact of Model choice on theKnowledge Extrac-\ntionstep. Here, \u2018P\u2019 and \u2018R\u2019 denote \u2018Precision\u2019 and \u2018Recall\u2019,\nrespectively.\nthe top two models (Knowledge-Assist-8B-SFT\nmodel, and Gemini-2.5-Flash-Lite model) from the\nknowledge extraction step (see Table 1) to group\nthe extracted QA pairs that are semantically similar\nwithin the same cluster using DBSCAN (Schubert\net al., 2017) with BGE-Large Embeddings (Chen\net al., 2024). From our clustering experiments, we\nfind 1578 clusters for the Gemini-2.5-Flash-Lite\nmodel and 1429 clusters for the Knowledge-Assist-\n8B-SFT model. All the QA pairs in each cluster\nare then given as input to the LLM to construct\nthe representative QA pairs which are then recom-\nmended for insertion in the knowledge base. In the\nfollowing, we present our findings for the final and\nmost crucial step of our system, recommending\nrepresentative QA pairs for the knowledge base.\n5.3 Performance on Recommending\nRepresentative QA Pairs\nIn this section, we present the end-to-end perfor-\nmance of ourAI Knowledge Assistsystem in Ta-\nble 2 and find that the Knowledge-Assist-8B-SFT\nmodel again demonstrates superior performance by\nachieving an impressive F1-Score of 91.8%, outper-\nforming the Gemini-2.5-Flash-Lite, which scored\n79.6%. The high precision and recall in the final\nstage are critical, as they ensure that the knowl-\nedge base is populated with accurate and relevant\ninformation, directly impacting the performance\nof the conversational AI agent. Overall accuracy"}
{"file": "2510.08149v1.pdf", "section": "5.3 Performance on Recommending", "chunk": ".6%. The high precision and recall in the final\nstage are critical, as they ensure that the knowl-\nedge base is populated with accurate and relevant\ninformation, directly impacting the performance\nof the conversational AI agent. Overall accuracy\nabove 90% confirms that our system can effectively\nbridge the cold start gap for companies. We also\nfind that for the majority of the companies, the\nF1-Score is also above 90% (see Figure 3).\n5.4 Impact of Model Choice\nWe conduct some experiments to investigate the\nimportance of model selection:\n(i) Does the backbone model choice for fine-\ntuning impact the performance?\n(ii) Does the annotator model variation impact\nthe performance?\nTable 3 presents the results of our experiments.\nWe find that replacing the model from LLaMA-3.1-\nModel Human Preference Total Approved\nKnowledge-Assist-8B-SFT25% 107\nGemini-2.5-Flash-Lite17% 98\nTable 4:Results based on Human Evaluation onFinal Rec-\nommended QA Pairs. Here, 58% of the responses in the\npreference test were rated as \u2018Tie\u2019 by Humans.\n8B with Qwen3-8B (Yang et al., 2025) resulted\nin a notable drop in all metrics, with the F1-score\nbeing dropped to 74.73%, underscoring the impor-\ntance of the choice of the base model. Furthermore,\nwhen we replaced our data annotator from Gemini-\n2.5-Pro to GPT-4o, the performance of the model\ntrained on this data decreased significantly to a\n65.95% F1-Score. This underscores the importance\nof model choice for data annotation.\n5.5 Human Evaluation\nWe further conduct human evaluations on the\nfinal representative QA pairs recommended by\nKnowledge-Assist-8B-SFTandGemini-2.5-Flash-\nLite. For human evaluation, we randomly select\n100 conversationsto check the following:\n(i) Which model-recommended QA pairs are\npreferred by humans (\u2018Tie\u2019 if both are preferred)?\n(ii) How many of the final recommended QA\npairs are approved by humans (this mimics real-\nworld scenarios where knowledge base managers\nwould approve the final recommended QA pairs)?\n(iii) What is the agreement between LLM Judge\nand Human judgments"}
{"file": "2510.08149v1.pdf", "section": "5.5 Human Evaluation", "chunk": ") How many of the final recommended QA\npairs are approved by humans (this mimics real-\nworld scenarios where knowledge base managers\nwould approve the final recommended QA pairs)?\n(iii) What is the agreement between LLM Judge\nand Human judgments on the final recommended\npairs (we measure the exact match between the\nnumber of representative QA pairs recommended\nfor each transcript that are annotated as correct by\nboth the LLM Judge and the Human Annotator)?\nThis evaluation was conducted by two humans\nhaving expertise in data science and computational\nlinguistics. Based on the results presented in Ta-\nble 4, we find that in 25% cases, Knowledge-\nAssist-8B-SFT recommended QA pairs are pre-\nferred. On the contrary, only in 17% of cases\nthe Gemini-2.5-Flash-Lite recommended QA pairs\nare preferred. Moreover, the number of final rec-\nommended QA pairs that are accepted by the hu-\nman evaluators for storing in the knowledge base\nis 107 for Knowledge-Assist-8B-SFT, while 98\nfor Gemini-2.5-Flash-Lite. Therefore, the fine-\ntuned model also demonstrates superiority based\non human evaluation, similar to the LLM-judge.\nFurthormore, we find about 90% agreement be-\ntween human-annotated judgments and GPT-4o\njudgments, indicating the reliability of using GPT-\nFigure 4:A simple demo of AI Knowledge Assist.\n4o as the LLM Judge (see Appendix D for some\nexamples of LLM Judge error cases). We also\nconducted the Wilcoxon signed-rank test (Wool-\nson, 2007) on the number of approved QA pairs\nfor each model across the 100 samples. With\nourknowledge-assist-8B-SFTmodel generating\na higher number of approved pairs compared to\nthe baseline model, we find that this difference is\nstatistically significant (p\u22640.05). These\nfindings further provide strong evidence that our\nsystem outperforms the baseline in producing con-\ntent that human evaluators consider correct and\nvaluable for adding to a knowledge base.\n6Real World Deployment and Utilization\nWe deploy theAI Knowledge Assistsystem using\nKubeflow on the Google Vertex AI Platform10(re-\nquires 1 L4 GPU). This setup"}
{"file": "2510.08149v1.pdf", "section": "5.5 Human Evaluation", "chunk": "valuable for adding to a knowledge base.\n6Real World Deployment and Utilization\nWe deploy theAI Knowledge Assistsystem using\nKubeflow on the Google Vertex AI Platform10(re-\nquires 1 L4 GPU). This setup enables automatic\nexecution of the entire system, from data process-\ning to model inference. We show a simple demon-\nstration ofAI Knowledge Assistin Figure 4 where\nthe users select the contact center and the time-\nframe, and then the Kubeflow pipeline automati-\ncally analyzes all transcripts in the given timeframe\nto recommend QA pairs for the knowledge base.\nA key feature of the deployed system should be\nthe ability for the knowledge base to self-update.\nThis can be achieved by continuously processing\nnew call transcripts to extract potential new QA\npairs. After applying clustering to construct rep-\nresentative QA pairs from the extracted QA pairs,\nthe questions in the representative QA pairs can\nbe compared against the existing questions in the\nknowledge base by measuring question-question\nsimilarity using embeddings. If the similarity score\nis below a predefined threshold, it indicates a signif-\nicantly different new customer issue. In such cases,\nthe newly extracted QA pair can be flagged and\nautomatically added to the knowledge base or, for\nhigher-quality control, routed to a Knowledge Man-\nager for review. Moreover, the answer-answer sim-\nilarity can also be measured for similar questions,\nand if the similarity score is below a pre-defined\nthreshold, it may indicate that the answer in the\n10https://cloud.google.com/vertex-ai\nknowledge base is obsolete (e.g., changes in the\nfeature/product). This self-updating mechanism\nensures that the knowledge base remains current\nand continuously improves over time by adapting\nto new customer issues and product changes.\n7 Conclusion\nIn this paper, we presentedAI Knowledge Assist, an\nLLM-powered system that addresses the cold start\nproblem in conversational AI agents.With exten-\nsive experiments, we find that our proposed system\ndemonstrates significant effectiveness in automat-\nically creating a knowledge base from historical\nconversation transcripts. This allows contact cen-\nters without an existing knowledge base to still\nbuild conversational AI systems by leveragingAI\nKnowledge Assist. Moreover, we discuss how such\nsystems can be reliably developed, evaluated, and\ndeployed, which can lead to an improved customer\nexperience and agent performance by allowing con-\nversational"}
{"file": "2510.08149v1.pdf", "section": "7 Conclusion", "chunk": "build conversational AI systems by leveragingAI\nKnowledge Assist. Moreover, we discuss how such\nsystems can be reliably developed, evaluated, and\ndeployed, which can lead to an improved customer\nexperience and agent performance by allowing con-\nversational agents to resolve customer issues more\neffectively and efficiently by leveraging the con-\nstructed knowledge base. In the future, we will\nbuild new benchmarks to study how to efficiently\nupdate existing knowledge bases.\nLimitations\nAs our models are trained on customer-agent con-\nversations, they might not be suitable for use in\nother domains without further prompt engineering\nor fine-tuning. Since in this work, we use propri-\netary data for the development and evaluation of\nthe system, the dataset is not released. However, to\nmaximize methodological reproducibility, we have\nprovided extensive details, including the specific\nopen-source models being used, fine-tuning param-\neters, and the full verbatim prompts that we used\nin our experiments.\nEthics Statement\n\u2022Compensation for Human Evaluation:\nHuman evaluation was performed by internal\nscientists who have expertise in computa-\ntional linguistics. Therefore, no additional\ncompensation was required. Moreover, our\nin-house employees conducted the human\nevaluation because of the challenging nature\nof our proprietary datasets, which contain\nnoisy business conversational transcripts\ngenerated by our internal ASR system.\u2022Data Privacy:There is a data retention\npolicy available that allows the user to not\ngrant permission to use their call transcripts\nfor model development. To protect user\nprivacy, sensitive data such as personally\nidentifiable information (e.g., credit card\nnumber, phone number) was removed while\ncollecting the data.\n\u2022License:We maintained the licensing require-\nments accordingly while using different tools\n(e.g., HuggingFace).\nReferences\nGarima Agrawal, Sashank Gummuluri, and Cosimo\nSpera. 2024. Beyond-rag: Question identification\nand answer generation in real-time conversations.\narXiv preprint arXiv:2410.10136.\nJianlyu Chen, Shitao Xiao, Peitian Zhang, Kun\nLuo, Defu Lian, and Zheng Liu. 2024. M3-\nembedding: Multi-linguality, multi-functionality,\nmulti-granularity text embeddings through self-\nknowledge distillation. InFindings of the As"}
{"file": "2510.08149v1.pdf", "section": "7 Conclusion", "chunk": "Luo, Defu Lian, and Zheng Liu. 2024. M3-\nembedding: Multi-linguality, multi-functionality,\nmulti-granularity text embeddings through self-\nknowledge distillation. InFindings of the Asso-\nciation for Computational Linguistics: ACL 2024,\npages 2318\u20132335, Bangkok, Thailand. Association\nfor Computational Linguistics.\nAbhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey,\nAbhishek Kadian, Ahmad Al-Dahle, Aiesha Letman,\nAkhil Mathur, Alan Schelten, Amy Yang, Angela\nFan, et al. 2024. The llama 3 herd of models.arXiv\npreprint arXiv:2407.21783.\nCarla Ferraro, Vlad Demsar, Sean Sands, Mariluz Re-\nstrepo, and Colin Campbell. 2024. The paradoxes of\ngenerative ai-enabled customer service: A guide for\nmanagers.Business Horizons, 67(5):549\u2013559.\nXue-Yong Fu, Cheng Chen, Md Tahmid Rahman Laskar,\nTN Shashi Bhushan, and Simon Corston-Oliver. 2022.\nAn effective, performant named entity recognition\nsystem for noisy business telephone conversation\ntranscripts. InProceedings of 8th Workshop on Noisy\nUser-generated Text (W-NUT 2022), page 96.\nXue-Yong Fu, Md Tahmid Rahman Laskar, Elena\nKhasanova, Cheng Chen, and Shashi Tn. 2024. Tiny\ntitans: Can smaller large language models punch\nabove their weight in the real world for meeting sum-\nmarization? InProceedings of the 2024 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies (Volume 6: Industry Track), pages 387\u2013394,\nMexico City, Mexico. Association for Computational\nLinguistics.\nJiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan,\nXuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen,\nShengjie Ma, Honghao Liu, et al. 2024. A survey on\nllm-as-a-judge.arXiv preprint arXiv"}
{"file": "2510.08149v1.pdf", "section": "7 Conclusion", "chunk": "ao Zhai, Chengjin Xu, Wei Li, Yinghan Shen,\nShengjie Ma, Honghao Liu, et al. 2024. A survey on\nllm-as-a-judge.arXiv preprint arXiv:2411.15594.\nMd Tahmid Rahman Laskar, Sawsan Alqahtani, M Sai-\nful Bari, Mizanur Rahman, Mohammad Abdul-\nlah Matin Khan, Haidar Khan, Israt Jahan, Amran\nBhuiyan, Chee Wei Tan, Md Rizwan Parvez, et al.\n2024a. A systematic survey and critical review on\nevaluating large language models: Challenges, limi-\ntations, and recommendations. InProceedings of the\n2024 Conference on Empirical Methods in Natural\nLanguage Processing, pages 13785\u201313816.\nMd Tahmid Rahman Laskar, M Saiful Bari, Mizanur\nRahman, Md Amran Hossen Bhuiyan, Shafiq Joty,\nand Jimmy Huang. 2023a. A systematic study and\ncomprehensive evaluation of ChatGPT on benchmark\ndatasets. InFindings of the Association for Com-\nputational Linguistics: ACL 2023, pages 431\u2013469,\nToronto, Canada. Association for Computational Lin-\nguistics.\nMd Tahmid Rahman Laskar, Cheng Chen, Xue-yong Fu,\nMahsa Azizi, Shashi Bhushan, and Simon Corston-\noliver. 2023b. AI coach assist: An automated ap-\nproach for call recommendation in contact centers\nfor agent coaching. InProceedings of the 61st An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track), pages 599\u2013\n607, Toronto, Canada. Association for Computational\nLinguistics.\nMd Tahmid Rahman Laskar, Cheng Chen, Xue-yong\nFu, and Shashi Bhushan Tn. 2022. Improving named\nentity recognition in telephone conversations via ef-\nfective active learning with human in the loop. In\nProceedings of the Fourth Workshop on Data Sci-\nence with Human-in-the-Loop (Language Advances),\npages 88\u201393, Abu Dhabi, United Arab Emirates (Hy-\nbrid). Association for Computational Linguistics.\nMd Tahmid Rahman Lask"}
{"file": "2510.08149v1.pdf", "section": "2024 Conference on Empirical Methods in Natural", "chunk": "Proceedings of the Fourth Workshop on Data Sci-\nence with Human-in-the-Loop (Language Advances),\npages 88\u201393, Abu Dhabi, United Arab Emirates (Hy-\nbrid). Association for Computational Linguistics.\nMd Tahmid Rahman Laskar, Xue-Yong Fu, Cheng Chen,\nand Shashi Bhushan TN. 2023c. Building real-world\nmeeting summarization systems using large language\nmodels: A practical perspective. InProceedings\nof the 2023 Conference on Empirical Methods in\nNatural Language Processing: Industry Track, pages\n343\u2013352, Singapore. Association for Computational\nLinguistics.\nMd Tahmid Rahman Laskar, Israt Jahan, Elham\nDolatabadi, Chun Peng, Enamul Hoque, and Jimmy\nHuang. 2025. Improving automatic evaluation of\nlarge language models (LLMs) in biomedical relation\nextraction via LLMs-as-the-judge. InProceedings\nof the 63rd Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 25483\u201325497, Vienna, Austria. Association\nfor Computational Linguistics.\nMd Tahmid Rahman Laskar, Elena Khasanova, Xue-\nYong Fu, Cheng Chen, and Shashi Bhushan Tn.\n2024b. Query-OPT: Optimizing inference of large\nlanguage models via multi-query instructions in meet-\ning summarization. InProceedings of the 2024 Con-\nference on Empirical Methods in Natural LanguageProcessing: Industry Track, pages 1140\u20131151, Mi-\nami, Florida, US. Association for Computational Lin-\nguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. InText summarization\nbranches out, pages 74\u201381.\nStuart Lloyd. 1982. Least squares quantization in pcm.\nIEEE transactions on information theory, 28(2):129\u2013\n137.\nOpenAI. 2023. Gpt-4 technical report.\nNaoki Otani, Nikita Bhutani, and Estevam Hruschka.\n2025. Natural language processing for human re-\nsources: A survey. InProceedings of the 2025 Con-\nference of the Nations of the Americas Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (Volume"}
{"file": "2510.08149v1.pdf", "section": "2024 Conference on Empirical Methods in Natural", "chunk": "ka.\n2025. Natural language processing for human re-\nsources: A survey. InProceedings of the 2025 Con-\nference of the Nations of the Americas Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies (Volume 3: Industry Track),\npages 583\u2013597, Albuquerque, New Mexico. Associa-\ntion for Computational Linguistics.\nPeter J Rousseeuw. 1987. Silhouettes: a graphical aid\nto the interpretation and validation of cluster analysis.\nJournal of computational and applied mathematics,\n20:53\u201365.\nHarsh Saini, Md Tahmid Rahman Laskar, Cheng Chen,\nElham Mohammadi, and David Rossouw. 2025. Llm\nevaluate: An industry-focused evaluation tool for\nlarge language models. InProceedings of the 31st\nInternational Conference on Computational Linguis-\ntics: Industry Track, pages 286\u2013294.\nErich Schubert, J\u00f6rg Sander, Martin Ester, Hans Peter\nKriegel, and Xiaowei Xu. 2017. Dbscan revisited,\nrevisited: why and how you should (still) use dbscan.\nACM Transactions on Database Systems (TODS),\n42(3):1\u201321.\nZhongwei Wan, Xin Wang, Che Liu, Samiul Alam,\nYu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan,\nYi Zhu, Quanlu Zhang, et al. 2024. Efficient large\nlanguage models: A survey.Trans. Mach. Learn.\nRes.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. InProceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38\u201345.\nRobert F Woolson. 2007. Wilcoxon signed-rank test.\nWiley encyclopedia of clinical trials, pages 1\u20133.\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muen-\nnighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack:\nPacked resources for general"}
{"file": "2510.08149v1.pdf", "section": "2024 Conference on Empirical Methods in Natural", "chunk": "pages 1\u20133.\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muen-\nnighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack:\nPacked resources for general chinese embeddings. In\nProceedings of the 47th international ACM SIGIR\nconference on research and development in informa-\ntion retrieval, pages 641\u2013649.\nTianyi Xiong, Xiyao Wang, Dong Guo, Qinghao Ye,\nHaoqi Fan, Quanquan Gu, Heng Huang, and Chun-\nyuan Li. 2025. Llava-critic: Learning to evaluate\nmultimodal models. InProceedings of the Computer\nVision and Pattern Recognition Conference, pages\n13618\u201313628.\nZhentao Xu, Mark Jerome Cruz, Matthew Guevara,\nTie Wang, Manasi Deshpande, Xiaofeng Wang, and\nZheng Li. 2024. Retrieval-augmented generation\nwith knowledge graphs for customer service question\nanswering. InProceedings of the 47th international\nACM SIGIR conference on research and development\nin information retrieval, pages 2905\u20132909.\nAn Yang, Anfeng Li, Baosong Yang, Beichen Zhang,\nBinyuan Hui, Bo Zheng, Bowen Yu, Chang Gao,\nChengen Huang, Chenxu Lv, et al. 2025. Qwen3\ntechnical report.arXiv preprint arXiv:2505.09388.\nJiayi Ye, Yanbo Wang, Yue Huang, Dongping Chen,\nQihui Zhang, Nuno Moniz, Tian Gao, Werner Geyer,\nChao Huang, Pin-Yu Chen, Nitesh V Chawla, and\nXiangliang Zhang. 2024. Justice or prejudice? quan-\ntifying biases in llm-as-a-judge.\nBing Zhang, Mikio Takeuchi, Ryo Kawahara, Shubhi\nAsthana, Maruf Hossain, Guang-Jie Ren, Kate Soule,\nYifan Mai, and Yada Zhu. 2025. Evaluating large\nlanguage models with enterprise benchmarks. In\nProceedings of the 2025 Conference of the Nations of\nthe Americas Chapter of the Association for Compu-\ntational Lingu"}
{"file": "2510.08149v1.pdf", "section": "2024 Conference on Empirical Methods in Natural", "chunk": "an Mai, and Yada Zhu. 2025. Evaluating large\nlanguage models with enterprise benchmarks. In\nProceedings of the 2025 Conference of the Nations of\nthe Americas Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies\n(Volume 3: Industry Track), pages 485\u2013505.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019. Bertscore: Evaluating\ntext generation with bert. InInternational Confer-\nence on Learning Representations.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, Hao Zhang,\nJoseph E Gonzalez, and Ion Stoica. 2023a. Judging\nllm-as-a-judge with mt-bench and chatbot arena. In\nAdvances in Neural Information Processing Systems,\nvolume 36, pages 46595\u201346623. Curran Associates,\nInc.\nXin Zheng, Tianyu Liu, Haoran Meng, Xu Wang, Yufan\nJiang, Mengliang Rao, Binghuai Lin, Yunbo Cao,\nand Zhifang Sui. 2023b. Dialogqae: N-to-n question\nanswer pair extraction from customer service chatlog.\nInFindings of the Association for Computational\nLinguistics: EMNLP 2023, pages 6540\u20136558.\nXiliang Zhu, Elena Khasanova, and Cheng Chen. 2025.\nCan post-training quantization benefit from an addi-\ntional qlora integration? InProceedings of the 2025\nConference of the Nations of the Americas Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies (Volume 3: Industry\nTrack), pages 506\u2013514.\nFigure 5: Performance Comparisons between Clustering\nApproaches.\nA Cost Analysis\nAll open-source models in this study have fewer\nthan 10 billion parameters and fit comfortably on\na single NVIDIA L4 GPU (Laskar et al., 2023c).\nFor proprietary models, Gemini-2.0-Flash-Lite is\nthe most cost-effective at $0.075 per million to-\nkens for"}
{"file": "2510.08149v1.pdf", "section": "2024 Conference on Empirical Methods in Natural", "chunk": "a single NVIDIA L4 GPU (Laskar et al., 2023c).\nFor proprietary models, Gemini-2.0-Flash-Lite is\nthe most cost-effective at $0.075 per million to-\nkens for input and $0.30 for output. Gemini-2.0-\nFlash costs $0.15 for input and $0.60 for output.\nWe omit Gemini-2.5-Flash due to its much higher\n$0.30 / $2.50 pricing. Among the OpenAI models,\nGPT-4o-mini is modestly higher at $0.15 / $0.60,\nwhereas our evaluation model GPT-4o costs $2.50\n/ $10. For data annotation, we rely on Gemini-2.5-\nPro at $1.25 / $10 per million tokens.\nB Clustering Model Evaluation\nFor clustering, we compare the DBSCAN (Schu-\nbert et al., 2017) with K-Means (Lloyd, 1982) us-\ning the Silhouette (Rousseeuw, 1987) metric. By\ncomparing various embedding models, we find that\nBGE embeddings (Xiao et al., 2024; Chen et al.,\n2024) work best by ensuring both efficiency and\naccuracy. Based on the results given in Figure 5\nusing the best configurations for different BGE em-\nbedding models with the minimum sample size of\n2 per cluster, we find that the DBSCAN algorithm\nperforms much better than K-Means. Moreover,\nDBSCAN automatically determines the number of\nclusters, a significant advantage given the varying\ncall volumes across companies and the high hetero-\ngeneity of underlying cluster structures. In contrast,\nthe K-Means method requires a predefined num-\nber of clustersk, making it unsuitable for datasets\nwhose optimal number of clusters is unknown.\nC Sample Prompts\nThe sample prompts for the LLMs for Knowledge\nExtraction and Recommendation, as well as for\nevaluation using LLM Judge, are given below.\nPrompt: Knowledge Extraction from\nTranscript using LLMs\nYou are given a call transcript that has two speakers:\n\u201cCustomer\u201d (the person seeking help) and \u201cAgent\u201d\n(the customer service representative).\nYour goal is to extract factually correct high-quality\nknowledge from this call in the form of \u201cquestions"}
{"file": "2510.08149v1.pdf", "section": "2024 Conference on Empirical Methods in Natural", "chunk": "You are given a call transcript that has two speakers:\n\u201cCustomer\u201d (the person seeking help) and \u201cAgent\u201d\n(the customer service representative).\nYour goal is to extract factually correct high-quality\nknowledge from this call in the form of \u201cquestions\u201d\nand \u201canswers\u201d that will be uploaded to a knowledge\nbase such that if similar questions are asked by a\ncustomer in a future conversation, the knowledge\nbase can be used to address the customer concern.\nSince the transcript may contain stilted or colloquial\nphrasing due to being transcribed from spoken\naudio, you can rewrite the knowledge (i.e., QA\npair) extracted from the conversation such that the\nextracted knowledge is human-readable.\nFor knowledge extraction, you should follow these\nrules:\n1.Only extract the knowledge that isnon-sensitive\nand doesnotcontain personally identifiable in-\nformation (PII).\n2.The extracted knowledge should begeneral,\ninformation-seekingin nature and applicable to\nfuture customers with similar needs. Chit-chats\nor rapport-building type questions that are not\ninformation-seeking should be avoided.\n3.Do not extract those answers that aretime sen-\nsitive. For instance, if an answer is only appli-\ncable till a certain date, ignore such types of\nknowledge extraction.\n4.If the question is related to any product, then\nthe product name must be mentioned in the se-\nlected QA pairs such that they areunderstand-\nablewhen added to the knowledge base.\nOutput Format:\nOnly return a JSON array of objects without any\nadditional text, where each object has three keys: (i)\n\u201cQuestion\u201d, (ii) \u201cAnswer\u201d, and (iii) Justification.\nIn the above, \u201cJustification\u201d refers to the rationale\nbehind why the specific \u201cQuestion\u201d and \u201cAnswer\u201d\npairs are selected and how they strictly follow all the\nrules. In addition, \u201cJustification\u201d may also contain\nsome snippets from the conversation transcript\nthat support the extracted knowledge (i.e., the\nQuestion-Answer pairs). If no knowledge pair is\nextracted, just return an empty JSON array.\nTranscript:\n[Call Conversation Transcript]Prompt: Recommending Representative\nQA Pairs\nYou are given a cluster of question-answer (QA) pairs. The cluster\nis constructed in a way such that similar questions are grouped\ntogether in the same cluster. These QA pairs are extracted from\ndifferent customer-agent conversations and will be"}
{"file": "2510.08149v1.pdf", "section": "2024 Conference on Empirical Methods in Natural", "chunk": "You are given a cluster of question-answer (QA) pairs. The cluster\nis constructed in a way such that similar questions are grouped\ntogether in the same cluster. These QA pairs are extracted from\ndifferent customer-agent conversations and will be stored in a\ncompany knowledge base.\nIn this task, your goal is to filter the QA pairs in the cluster by\nconstructing representative QA pairs. For this purpose, you should\nfollow the following rules:\n1.No Duplicates: If there are multiple duplicate QA pairs in the\ncluster, only extract the QA pair that can be the representative\nfor this cluster.\n2.Rewrite or Extract: The representative QA pairs can either\nbe extracted directly from the cluster, or you can rewrite\nthem if that makes them better understandable.\n3.Not time-sensitive: QA pairs that are only applicable for a\ncertain time period (e.g., if something is due today) cannot\nbe representative.\n4.Non-personalized: QA pairs that are specific to certain\ncustomers or contain PII information, like account-specific\ndetails (e.g., billing info, addresses) cannot be representative.\n5.Universal: QA pairs that are not general and cannot be\napplicable to future customers with similar needs cannot be\nrepresentative.\n6.Usefulness: QA pairs that agents cannot use in the com-\npany knowledge base to address questions asked by other\ncustomers in the future cannot be representative.\n7.Information-Seeking: You should only select the\ninformation-seeking QA pairs. Personal questions (e.g., what\nis your name, address, etc.), chit-chat, or rapport-building\ntype QA pairs should not be included.\n8.Understandable: If the question in the QA pair is related to\nany product, then the product name must be clearly specified\nin the representative QA pairs so that it is understandable.\nAfter strictly following the above rules, generate your answer in an\narray of JSON format, with the following keys:\n(i) Representative Question\n(ii) Representative Answer\n(ii) Type\n(iv) Explanation\nHere, the value for type should be either \"Rewritten\u201d or\n\u201cExtracted\u201d , where\"Rewritten\u201d means you rewrite it, while\n\"Extracted\u201d means it has been extracted without any rewrite.\nMoreover, Explanation will contain the reasoning behind\n\u201crewriting\", and it should be \u201cN/A\u201d if\u201cExtracted\u201d.\nNote that you should only rewrite in"}
{"file": "2510.08149v1.pdf", "section": "2024 Conference on Empirical Methods in Natural", "chunk": ", while\n\"Extracted\u201d means it has been extracted without any rewrite.\nMoreover, Explanation will contain the reasoning behind\n\u201crewriting\", and it should be \u201cN/A\u201d if\u201cExtracted\u201d.\nNote that you should only rewrite in case of urgency. For rewriting,\nyou can mix information from multiple questions and answers to\ncreate the representative QA pair (if relevant) but ensure that your\nrepresentative QA pairs do not lose any important information.\nIf similar questions have different answers, you can keep both of\nthem if merging multiple answers into one representative is not\npossible.\nThe question cluster is given below. Please construct the\nrepresentative QA pairs.\nQuestion Cluster: [List of QA Pairs]\nPrompt: Evaluation of the Knowledge\nExtraction and Recommendation Models\nusing LLM Judge\nYou are given the knowledge extracted from a customer-agent con-\nversation in the form of question-answer (QA) pairs that would be\nstored in the company\u2019s knowledge base. This is done so that in\na future conversation between a new customer and a new agent in\nthe company, the agent can use the knowledge base to answer the\ncustomer\u2019s question if similar questions or concerns are asked by the\ncustomer.\nYour goal is to identify how many of the extracted QA pairs strictly\nfollow the following rules:\n1.Not Time-Sensitive: QA pairs that are only applicable for a\ncertain time period (e.g., if something is due today) must not\nbe there.\n2.Non-personalized and No PII: QA pairs that are specific\nto certain customers and contain personally identifiable in-\nformation (PII) or account-specific details (e.g., billing info,\naddresses) must not be there.\n3.Universal: QA pairs that are not general and not applicable\nto future customers with similar needs, such that agents\ncannot use the QA pairs in the company knowledge base to\naddress questions asked by any new customers, must not be\nthere.\n4.Information-Seeking: Personal questions (e.g., what is your\nname, address, etc.), chit-chat, or rapport-building type QA\npairs must not be there.\n5.Customer-Focused: The selected questions must be some-\nthing that are asked by the customer, not the agent.\n6.Understandable: If the question is related to any product,\nthen the product name must be mentioned in the selected QA\npairs such that they are understandable when added to the"}
{"file": "2510.08149v1.pdf", "section": "2024 Conference on Empirical Methods in Natural", "chunk": "thing that are asked by the customer, not the agent.\n6.Understandable: If the question is related to any product,\nthen the product name must be mentioned in the selected QA\npairs such that they are understandable when added to the\nknowledge base.\n7.Factually Correct: In comparison to the conversation con-\ntext, the QA pair should be factually correct.\nNow, your goal is to identify the QA pairs that fulfill the above\ncriteria. For this, you are provided with the Conversation Transcript\nand the list of QA pairs that are extracted from it. Please generate\nyour answer in the JSON format, with the following keys:\n(i)Total Correct\n(ii)Total Predicted\n(iii)Justification\nHere,Total Correct denotes the number of correct QA pairs,\nTotal Predicted denotes the total number of QA pairs provided,\nwhileJustificationdenotes your reasoning behind your answer.\nTranscript: [Call Conversation Transcript]\nExtracted Questions: [List of QA Pairs]D Sample Errors\nError Examples: LLM Judge Labelling\nCorrect QA Pairs as Incorrect\nExample 1:\nRepresentative Question:What is [PLAT-\nFORM X] and what are its features?\nRepresentative Answer:[PLATFORM X] is a\ndigital platform for administering assessments.\nIt allows users to purchase digital assessment\nadministrations and reports. Users can conduct tests\nin person, send a remote completion link to clients, or\nmanually enter responses from physical assessments\nfor digital scoring. The platform ensures secure\ndata transmission through encryption and adheres to\nsecurity guidelines.\nType:Extracted.\nExplanation:N/A.\nExample 2:\nRepresentative Question:Do customers\nneed to purchase score reports separately when using\nspecific tests or assessments in an online format?\nRepresentative Answer:Yes, score reports need\nto be purchased separately even if using an online\nformat.\nType:Extracted.\nExplanation:N/A.\nExample 3:\nRepresentative Question:What should a\ncustomer do if they encounter a problem or technical\nissue while placing an order online?\nRepresentative Answer:If a customer encounters\na problem or technical issue while placing an order\nonline, they have a couple of options. They can try\nclearing their browser\u2019s cache, history, and cookies,\nand then restarting the browser. Alternatively, they\ncan contact customer service for assistance, and the\nagent can help them place the order over the phone\nor manually process it.\nType"}
{"file": "2510.08149v1.pdf", "section": "2024 Conference on Empirical Methods in Natural", "chunk": "clearing their browser\u2019s cache, history, and cookies,\nand then restarting the browser. Alternatively, they\ncan contact customer service for assistance, and the\nagent can help them place the order over the phone\nor manually process it.\nType:Rewritten.\nExplanation:The representative question is\na combination of several similar questions to\nencompass all the nuances of the issues. The\nrepresentative answer synthesizes the recommended\nsolutions from multiple QA pairs, providing both\nself-service troubleshooting steps (clearing cache)\nand direct support options (contacting customer\nservice for manual order placement). This approach\ncovers the different solutions offered and is more\ncomprehensive."}
{"file": "2510.08255v1.pdf", "section": "Introduction", "chunk": "OPPONENTSHAPING INLLM AGENTS\nMarta Emili Garcia Segura1, 2, Stephen Hailes1, Mirco Musolesi1, 2, 3\n1Department of Computer Science, University College London\n2Centre for Artificial Intelligence, University College London\n3Department of Computer Science, University of Bologna\n{marta.segura22, s.hailes, m.musolesi}@ucl.ac.uk\nABSTRACT\nLarge Language Models (LLMs) are increasingly being deployed as autonomous\nagents in real-world environments. As these deployments scale, multi-agent inter-\nactions become inevitable, making it essential to understand strategic behavior in\nsuch systems. A central open question is whether LLM agents, like reinforcement\nlearning agents, can shape the learning dynamics and influence the behavior of\nothers through interaction alone. In this paper, we present the first investigation of\nopponent shaping (OS) with LLM-based agents. Existing OS algorithms cannot\nbe directly applied to LLMs, as they require higher-order derivatives, face scala-\nbility constraints, or depend on architectural components that are absent in trans-\nformers. To address this gap, we introduce ShapeLLM, an adaptation of model-\nfree OS methods tailored for transformer-based agents. Using ShapeLLM, we\nexamine whether LLM agents can influence co-players\u2019 learning dynamics across\ndiverse game-theoretic environments. We demonstrate that LLM agents can suc-\ncessfully guide opponents toward exploitable equilibria in competitive games (It-\nerated Prisoner\u2019s Dilemma, Matching Pennies, and Chicken) and promote coor-\ndination and improve collective welfare in cooperative games (Iterated Stag Hunt\nand a cooperative version of the Prisoner\u2019s Dilemma). Our findings show that\nLLM agents can both shape and be shaped through interaction, establishing oppo-\nnent shaping as a key dimension of multi-agent LLM research.\n1 INTRODUCTION\nLarge language models (LLMs) have evolved rapidly in recent years, demonstrating remarkable ca-\npabilities in reasoning, planning and goal-directed behavior that make them increasingly suitable for\ndeployment as autonomous agents (Zhao et al., 2023; Anthropic, 2025a; OpenAI, 2025b; Xi et al.,\n2025). Already, LLM-based agents are being adopted for complex tasks such as web navigation"}
{"file": "2510.08255v1.pdf", "section": "1 INTRODUCTION", "chunk": "agents (Zhao et al., 2023; Anthropic, 2025a; OpenAI, 2025b; Xi et al.,\n2025). Already, LLM-based agents are being adopted for complex tasks such as web navigation and\ncode generation (Anthropic, 2025b; OpenAI, 2025a). As deployment scales, these agents will be\nless likely to operate in isolation. Instead, they will increasingly interact with other learning agents\nin shared environments, collaborating on tasks, competing for resources, or pursuing independent\nobjectives. There is growing interest in understanding the opportunities and challenges associated\nwith multi-agent LLM systems (Fourney et al., 2024; Ghafarollahi & Buehler, 2025; Pan et al., 2025;\nRosser & Foerster, 2025). However, most approaches treat LLMs as static entities, overlooking the\nstrategic dynamics that emerge when agents continuously adapt to one another.\nMulti-agent reinforcement learning (MARL) has long been concerned with the interaction of multi-\nple learners in shared environments (Busoniu et al., 2008). A core difficulty in MARL is that agents\noften treat each other as static parts of the environment, which can yield poor collective outcomes.\nFor instance, in the Iterated Prisoner\u2019s Dilemma (IPD, Axelrod & Hamilton (1981)), independent\nlearners reliably converge to mutual defection, which is the worst collective outcome (Harper et al.,\n2017; Foerster et al., 2018). To mitigate such failures, the field of opponent shaping develops agents\nthat actively anticipate and influence their co-players\u2019 learning dynamics, steering learned behav-\nior toward more favorable equilibria (Foerster et al., 2018; Lu et al., 2022). While these methods\nhave proven effective in RL settings, it remains unclear whether they extend to LLM agents. Unlike\nRL agents, these models process rich semantic information, exhibit complex reasoning capabilities\n(Xu et al., 2025), and can adapt through in-context learning (Brown et al., 2020; Bubeck et al.,\n1arXiv:2510.08255v1  [cs.LG]  9 Oct 2025\n2023). This raises uncertainty about whether traditional shaping methods will transfer to LLM"}
{"file": "2510.08255v1.pdf", "section": "1 INTRODUCTION", "chunk": "; Bubeck et al.,\n1arXiv:2510.08255v1  [cs.LG]  9 Oct 2025\n2023). This raises uncertainty about whether traditional shaping methods will transfer to LLM\nagents. Moreover, existing algorithms are poorly suited to LLMs, as they rely on estimating higher-\norder derivatives, adopt a dual-agentic structure, or require architectural components unavailable in\ntransformers (Foerster et al., 2018; Lu et al., 2022; Khan et al., 2024).\nOur work addresses this gap, being the first exploration of opponent shaping with LLM agents.\nUnderstanding the extent to which LLMs can engage in opponent shaping is critical as they are in-\ncreasingly deployed in real-world, multi-agent settings. This capability has dual implications: LLM\nagents may be vulnerable to exploitation by adversaries who strategically influence their learning\ndynamics, while shaping could also be a tool to foster prosocial behavior and enable coordination.\nWe study whether LLM agents can strategically influence each other\u2019s learning dynamics in repeated\nmatrix games, which capture core strategic incentives while allowing precise outcome quantification.\nAs a baseline, we train two LLM agents independently using Proximal Policy Optimization (PPO,\nSchulman et al. (2017)) to maximize their individual returns. We then introduce opponent shaping\nby turning one of those agents into ashaperthat aims to alter the learning dynamics of its co-player.\nThe shaper is trained usingShapeLLM, our proposed algorithm that adapts model-free approaches\n(Lu et al., 2022; Khan et al., 2024) to transformer architectures. We evaluate the efficacy of shaping\nin both exploitative scenarios, where an agent seeks unilateral advantage, and prosocial scenarios,\nwhere shaping fosters cooperation. Our contributions are the following:\n\u2022 We provide the first investigation of opponent shaping in LLM agents, demonstrating that\nthey can strategically influence each other\u2019s learning dynamics through interaction alone.\n\u2022 We proposeShapeLLM, a model-free opponent shaping algorithm for transformer archi-\ntectures leveraging structured natural language prompts.\n\u2022 We evaluate shaping across diverse game-theoretic environments and show that LLM\nagents can successfully exploit opponents in competitive settings and guide interactions\ntoward mutually beneficial outcomes in cooperative ones.\n2 BACKGROUND\n2.1 LLM AGENTS\nAnLLM agentcan be thought of as any system leveraging"}
{"file": "2510.08255v1.pdf", "section": "1 INTRODUCTION", "chunk": "and show that LLM\nagents can successfully exploit opponents in competitive settings and guide interactions\ntoward mutually beneficial outcomes in cooperative ones.\n2 BACKGROUND\n2.1 LLM AGENTS\nAnLLM agentcan be thought of as any system leveraging LLMs as the core computational unit\nfor reasoning, planning, and decision-making (Sumers et al., 2023). The architectures of these\nagents vary significantly in the literature, with some systems integrating reasoning frameworks (Yao\net al., 2023), memory banks (Vezhnevets et al., 2023), or tool-access via APIs (Schick et al., 2023;\nPatil et al., 2024), both in single and multi-agent settings (Park et al., 2023; Wang et al., 2024).\nIn the latter, game-theoretic environments provide a natural testbed for studying strategic dynamics.\nRecent work has used these environments to investigate LLM agents\u2019 cooperation (Piatti et al., 2024;\nAkata et al., 2025), rationality (Fan et al., 2024), and strategic reasoning (Gandhi et al., 2023; Duan\net al., 2024; Huang et al., 2025). Beyond observational studies, these environments can be used to\ntrain LLMs towards specific objectives such as moral alignment (Tennant et al., 2025).\n2.2 FINE-TUNINGLLMS WITHREINFORCEMENTLEARNING\nThe use of reinforcement learning (RL) in LLM training was first popularized through Reinforce-\nment Learning from Human Feedback (RLHF, Ziegler et al. (2019); Stiennon et al. (2020); Ouyang\net al. (2022)), which typically employs Proximal Policy Optimization (PPO, Schulman et al. (2017))\nas the RL algorithm. PPO is an on-policy, actor-critic method that uses a learned value function for\nadvantage estimation. When applied to LLMs, it is customary to include a Kullback-Leibler (KL)\npenalty in the reward signal to prevent the model\u2019s output distribution from diverging too far from the\npre-trained one. Several alternatives to standard RLHF with PPO have since been proposed, includ-\ning Group Relative Policy Optimization (GRPO, Shao et al."}
{"file": "2510.08255v1.pdf", "section": "2.2 FINE-TUNINGLLMS WITHREINFORCEMENTLEARNING", "chunk": "signal to prevent the model\u2019s output distribution from diverging too far from the\npre-trained one. Several alternatives to standard RLHF with PPO have since been proposed, includ-\ning Group Relative Policy Optimization (GRPO, Shao et al. (2024)), which estimates advantages\nvia Monte Carlo rollouts, and Direct Preference Optimization (DPO, Rafailov et al. (2024)), which\nconverts the RLHF objective into a supervised learning loss. All three approaches are typically used\nin contextual bandit settings (Llama Team, AI@Meta, 2024; Qwen Team, 2025; Mistral-AI, 2025),\nwhere each model response is treated as an independent episode with immediate reward feedback.\n2\nThe application of multi-turn RL to LLMs remains an active area of research due to challenges in\npreference collection, reward modeling, and ambiguity in action space definition (Shani et al., 2024;\nZhou et al., 2024; Zeng et al., 2025). Nevertheless, underlying algorithms such as PPO are inher-\nently designed to handle temporally structured environments, making them suitable for multi-agent\nstrategic settings where actions have long-term consequences.\n2.3 OPPONENTSHAPING\nThe opponent shaping literature is characterized by two primary approaches: methods that explicitly\naccount for the opponent\u2019s updates in the agent\u2019s learning rule (Foerster et al., 2018; Letcher et al.,\n2019; Willi et al., 2022), and meta-learning approaches that learn to shape opponents by observing\nhow their actions influence their opponent\u2019s parameter updates (Lu et al., 2022; Balaguer et al.,\n2022; Khan et al., 2024). The most notable method in the first category is Learning with Opponent-\nLearning Awareness (LOLA, Foerster et al. (2018)). In LOLA, the opponent\u2019s learning rule is\nincorporated into the agent\u2019s update, accounting for the effect of the agent\u2019s action on the opponent\u2019s\nparameter updates. This method has demonstrated notable successes, such as the emergence of Tit-\nfor-Tat (TFT) in the iterated prisoner\u2019s dilemma (IPD) through self-play. However, LOLA faces\nsignificant limitations: it assumes knowledge of the opponent\u2019s learning rule, relies on high-variance\nhigher order derivatives, and only"}
{"file": "2510.08255v1.pdf", "section": "2.3 OPPONENTSHAPING", "chunk": "(TFT) in the iterated prisoner\u2019s dilemma (IPD) through self-play. However, LOLA faces\nsignificant limitations: it assumes knowledge of the opponent\u2019s learning rule, relies on high-variance\nhigher order derivatives, and only considers immediate effects on the opponent\u2019s updates. While\nseveral LOLA refinements have been proposed (Letcher et al., 2019; Willi et al., 2022), they exhibit\nthe same core limitations.\nThe second category of methods, exemplified by Model-Free Opponent Shaping (M-FOS, Lu et al.\n(2022)), was developed specifically to overcome these challenges. M-FOS bypasses these limi-\ntations, particularly LOLA\u2019s myopic perspective, by framing opponent shaping as a meta-learning\nproblem. In doing so, it decouples the task of interacting with the environment from that of influenc-\ning the opponent\u2019s learning dynamics. This decoupling is achieved via a bi-level agent architecture:\nan inner agent that interacts with its co-players, and an outer agent that updates or conditions the\ninner agent\u2019s policy. The outer agent operates in a meta-game, where the meta-state consists of all\nplayers\u2019 parameters, and the meta-action determines the inner agent\u2019s policy. Between episodes,\nother players update their parameters using their respective learning algorithms. This formulation\nenables the meta-agent to optimize for long-term opponent shaping effects.\nWhile M-FOS has demonstrated strong empirical results, including outperforming LOLA-based\nagents in the IPD, it presents scalability challenges due to its dual-agent architecture. To address\nthese, Khan et al. (2024) propose SHAPER, which simplifies M-FOS\u2019 architecture by collapsing\nthe shaping agent into a single recurrent neural network (RNN). The key insight is the distinction\nbetween history and context within opponent shaping. History captures intra-episode information\nnecessary for implementing conditional strategies such as TFT, while context captures inter-episode\ninformation about the opponent\u2019s learning dynamics. SHAPER captures history through the RNN\u2019s\ninputs and context through its hidden state, which persists across episodes within a trial. This unified\narchitecture eliminates the dual action spaces of M-FOS, allowing the agent to operate directly in\nthe environment\u2019s original action space. However, SHAPER is inherently tied to RNN architectures,\nwith its mechanism relying on distinct memory streams for capturing history and context.\n3 METHODO"}
{"file": "2510.08255v1.pdf", "section": "2.3 OPPONENTSHAPING", "chunk": "spaces of M-FOS, allowing the agent to operate directly in\nthe environment\u2019s original action space. However, SHAPER is inherently tied to RNN architectures,\nwith its mechanism relying on distinct memory streams for capturing history and context.\n3 METHODOLOGY\n3.1 PRELIMINARIES\nAgents interact with the environment by generating text. LetVdenote the model\u2019s vocabulary, and\nw1:L:= (w 1, . . . , w L), withw l\u2208 V,\u2200l\u2208 {1, . . . , L}. At each interaction, the agent\u2019s action is a\nsequence of tokens sampled from:\n\u03c1\u03b8(w1:L|c) =LY\nl=1\u03c1\u03b8(wl|c, w <l),(1)\nwherew <l:=w 1:l\u22121 , the contextcis the environment\u2019s description, andLis the generation length.\nFor simplicity, we setL= 1and define the distribution\u03c1 \u03b8(w|c), withw\u2208 V, as our agent\u2019s\npolicy. We deliberately avoid constrained decoding (Beurer-Kellner et al., 2024; Ugare et al., 2024)\n3\nor format-specific fine-tuning, relying instead on textual instructions to guide the model toward the\ndesired format.\nWe formalize our environments asrepeated normal-form games(Fudenberg & Tirole, 1991). Let\nM= (I,{A i}i\u2208I,{Ri}i\u2208I)denote a base game, whereI={1, . . . n}represents the set of players.\nEach playeri\u2208Ihas an associated action spaceA iand reward functionR i:A1\u00d7. . .\u00d7A n\u2192R. A\nrepeated normal-form game consists of playingMforTtime steps, whereTcan be finite or infinite.\nIn this paper, we focus on the finite case. At each time stept, players simultaneously choose actions\nat\nifori\u2208I. The resulting joint actionat= (at\n1, . . . , at\nn)determines the rewardrt\ni=R i(at)\nthat each player receives. The actionsat\niare sampled from player-specific policies\u03c1 \u03b8i(w|f(ht)),\nwheref(ht)is any function of the joint action historyht= (a1,a2"}
{"file": "2510.08255v1.pdf", "section": "3.1 PRELIMINARIES", "chunk": "rt\ni=R i(at)\nthat each player receives. The actionsat\niare sampled from player-specific policies\u03c1 \u03b8i(w|f(ht)),\nwheref(ht)is any function of the joint action historyht= (a1,a2, . . . ,at\u22121)(e.g., the previous\njoint actionat\u22121).\n3.2 OPPONENTSHAPING INLLM AGENTS\nWe introduceShapeLLM, a model-free opponent shaping algorithm designed to leverage the natural\nlanguage capabilities of LLMs. ShapeLLM condenses both history and context into structured nat-\nural language prompts, explicitly capturing the two forms of memory required for shaping into one\ninformation stream. As in existing model-free algorithms (Lu et al., 2022; Khan et al., 2024), in-\nteractions are organized into trials. Each trial comprisesn games parallel environments, where agents\nengage inEepisodes, each comprisingTrounds of the specified matrix game (Figure 1 provides a\nschematic representation of a trial).\ne = 1e = 2n = 1e = E\u2026n = 2\u2026n = ngames\u2026\u2026\n\u2026\u2026\u2026T roundsT roundst = 1t = 2t = T\nFigure 1: Schematic representation of a trial. Each box corresponds to an episode (a game played\nforTrounds). Same-colored boxes represent episodes within the same parallel environment. Within\neach environment, episodes occur sequentially as indicated by the arrows. The shaper updates its\nparameters using the experience collected throughout the entire trial.\nLett\u2208 {1, . . . , T}denote the round within an episode, ande\u2208 {1, . . . , E}denote the episode\nwithin a trial. For notational simplicity, we collapse the pair(e, t)into a single timescale index\n\u03c4\u2208 {1, . . . , E\u00d7T}. We formalize the shaping task as a POMDP( \u00afS,\u00afA,\u00afP,\u00afR,\u00af\u2126,\u00afO,\u00af\u03b3). The state\n\u00afs\u03c4={\u03b8\u03c4\u22121\ni, c\u03c4\u22121\ni}i\u2208I\u2208\u00afSencodes the parameters and conditioning prompts of all LLM agents\nfrom the previous timestep. The action space \u00afAand reward function \u00afRare"}
{"file": "2510.08255v1.pdf", "section": "3.2 OPPONENTSHAPING INLLM AGENTS", "chunk": "\u22121\ni, c\u03c4\u22121\ni}i\u2208I\u2208\u00afSencodes the parameters and conditioning prompts of all LLM agents\nfrom the previous timestep. The action space \u00afAand reward function \u00afRare equivalent to those of the\nunderlying repeated normal-form game. The observation\u00afo\u03c4=f(a1,a2, . . . ,a\u03c4\u22121)is a function of\nthe joint actions across all past timesteps. Lastly, \u00afPand \u00afOdenote the state transition and observation\nfunctions, respectively.\nAt the beginning of training, players are initialized with policies{\u03c10\n\u03b8i}i\u2208Iand receive initial ob-\nservations{c0\ni}i\u2208Ispecifying the game characteristics (number of players, action space, and re-\nward matrix) and action labels. At the\u03c4-th round of a trial, the shaper receives an observationc\u03c4\nj\nthat concatenates two components: the most recent joint action (a\u03c4\u22121) and a compressed natural\nlanguage representation of all the previous joint actions in the trial (f(a1, . . . ,a\u03c4\u22122)). This sep-\naration captures the distinction between history and context. The shaper then samples an action\na\u03c4\nj\u223c\u03c1\u03b8j(w|c\u03c4\nj)and receives the corresponding rewardr\u03c4\njand next observationc\u03c4+1\nj.\n4\nOpponents update their policy parameters between episodes (i.e., when\u03c4is a multiple ofT) using\nthe experience collected in the preceding episode. Consequently, within each trial, the shaper is\nexposed toEopponent updates, though only indirectly through the evolving summaries of joint\nactions that persist across episodes. By contrast, the shaper\u2019s own parameters\u03b8 jare updated only\nat trial finalization to maximize the cumulative trial return \u00afJ=PE\u00d7T\n\u03c4=1r\u03c4\nj. While we present this\nformulation in the context of repeated normal-form games, the ShapeLLM framework generalizes\nto any environment that can be formulated as a partially observable stochastic game.\n4 EXPERIMENTALSETTINGS\n4.1 ENVIRONMENTS\nWe investigate opponent shaping on iterated versions of four canonical2\u00d72games. These environ-\nments were selected to represent diverse incentive structures across strategic interactions.\nIterated Prisoner\u2019s D"}
{"file": "2510.08255v1.pdf", "section": "4 EXPERIMENTALSETTINGS", "chunk": "EXPERIMENTALSETTINGS\n4.1 ENVIRONMENTS\nWe investigate opponent shaping on iterated versions of four canonical2\u00d72games. These environ-\nments were selected to represent diverse incentive structures across strategic interactions.\nIterated Prisoner\u2019s Dilemma (IPD). Players choose between cooperation (C) and defection (D).\nMutual cooperation yields the highest collective payoff, but each player faces individual incentives\nto defect and exploit cooperative opponents (Rapoport, 1974; Axelrod & Hamilton, 1981).\nIterated Matching Pennies (IMP). A zero-sum game where players choose between heads (H) and\ntails (T). One player receives a positive payoff when actions match, whereas the other is rewarded\nwhen they differ. This environment is purely adversarial.\nIterated Chicken Game (ICG). Players can either Swerve (S) or Go straight (G). Going straight\nyields an advantage against a swerving opponent, but mutual aggression results in catastrophic out-\ncomes for both, creating a coordination problem under risk (Rapoport & Chammah, 1966).\nIterated Stag Hunt (ISH). Players choose between Stag (S) and Hare (H). Hunting stag yields the\nhighest payoff but only if both players coordinate, while hunting hare offers a lower but guaranteed\nreward. This creates a coordination problem with multiple equilibria.\nWe assign a single tokenw aito each actiona i\u2208 A(e.g. \u201cC\u201d for cooperate and \u201cD\u201d for defect in\nthe IPD), and treat any other generation as an illegal action (a null). Choosinga nullincurs a penalty\nrnull, and the transition is excluded from both players\u2019 game histories (see Appendix A.2).\n4.2 IMPLEMENTATIONDETAILS\nOur base model isgemma-2-2b-it(Gemma Team, 2024), a small, instruction-tuned, open-source\nlanguage model. We focus on small models for computational efficiency and choose instruction-\ntuned variants as they are more goal-directed (Ouyang et al., 2022) and benefit from coding data\nexposure (Duan et al., 2024). To keep the agent architecture minimal, we restrict memory to the\ncontext window of the model and avoid additional reasoning scaffolds such as chain-of-thought\n(CoT) prompting (Wei et al., 2022b),"}
{"file": "2510.08255v1.pdf", "section": "4.2 IMPLEMENTATIONDETAILS", "chunk": "2024). To keep the agent architecture minimal, we restrict memory to the\ncontext window of the model and avoid additional reasoning scaffolds such as chain-of-thought\n(CoT) prompting (Wei et al., 2022b), which are less effective in small models (Wei et al., 2022a).\nWe train our agents using QLoRA (Dettmers et al., 2023), with the base model quantised to 4-bit\nprecision via theBitsAndBytespackage (Dettmers & von Koeller, 2022), and adapters of rank\nr= 2implemented through thePEFTlibrary (Mangrulkar et al., 2022). The learnable parameters\ncomprise the LoRA adapters for the query/value projections and the value head parameters. All\nmodels are fine-tuned using a custom implementation of PPO that inherits from theTRLpackage1\n(von Werra et al., 2020). We run PPO training for 200-300 epochs, withn games = 5parallel envi-\nronments,E= 5episodes, andT= 20rounds per episode. For the shapers, we express context\nvia cumulative state visitation counts2(e.g., in the IPD: \u201cCC: 1, CD: 1, DC: 2, DD: 3\u201d). All training\nwas done on a single A100 GPU with 40G of VRAM. The full specification of the hyperparameters,\nreward matrices, and training prompts used is provided in Appendices A.3, A.1, A.9, respectively.\n1The default implementation is only compatible with contextual bandits.\n2We represent the context via visitation counts instead of full trajectories to prevent the token length from\ngrowing linearly with the number of rounds.\n5\n5 SHAPING INEXPLOITATIVESETTINGS\nWe consider two core training configurations for our agents across the IPD, IMP and ICG.\nBaseline. We establish the baseline performance using two LLM-basednaive learners(NL) that\ntreat their opponent as a stationary component of the environment. Each agent\u2019s conditioning prompt\ncontains the game description and the most recent joint action. Once the episode is finished, both\nplayers simultaneously update their parameters via PPO to maximize episodic returns. This baseline\nestablishes expected behavior when no opponent shaping occurs.\nShaper vs."}
{"file": "2510.08255v1.pdf", "section": "5 SHAPING INEXPLOITATIVESETTINGS", "chunk": "contains the game description and the most recent joint action. Once the episode is finished, both\nplayers simultaneously update their parameters via PPO to maximize episodic returns. This baseline\nestablishes expected behavior when no opponent shaping occurs.\nShaper vs. naive learner. An LLM-based shaper interacts with an LLM naive learner (with the\nsame configuration as in the baseline). As described in Section 3.2, the shaper updates its parameters\nonly at trial completion, after having observed multiple opponent parameter updates.\nFollowing training, we evaluate performance by having each trained pair of agents play 100 games\nwith the same episode length used during training (T= 20).\n5.1 SHAPING IN THEIPD, IMPANDICG\nWe run experiments across 5 random seeds3using action labelsw a1=C, w a2=D for the IPD,\nwa1=H, w a2=T for the IMP, andw a1=S, w a2=G for the ICG. Figure 2 illustrates the\ntraining dynamics for the shaper experiments across the three games (the corresponding figures for\nthe baselines are shown in Appendix A.8).\n0 50 100 150 20001234Average Reward per stepIPD\nShaper\nOpponent\n0 50 100 150 2001.0\n0.5\n0.00.51.0IMP\nShaper\nOpponent\n0 50 100 150 2001.01.52.02.53.0ICG\nShaper\nOpponent\n0 50 100 150 200\nEpoch0.000.250.500.751.00State VisitationCC\nCD\nDC\nDD\nI\n0 50 100 150 200\nEpoch0.000.250.500.751.00\nHH\nHT\nTH\nTT\nI\n0 50 100 150 200\nEpoch0.000.250.500.751.00\nSS\nSG\nGS\nGG\nI\nFigure 2: Average reward per step (top row) and state visitation (bottom row) during training for the\nshaping experiments across the IPD, IMP, and ICG. In the state visitation figures, the outcome \u201cI\u201d\nencompasses all transitions where either player chosea null. Results are reported along with a 95%"}
{"file": "2510.08255v1.pdf", "section": "5.1 SHAPING IN THEIPD, IMPANDICG", "chunk": "for the\nshaping experiments across the IPD, IMP, and ICG. In the state visitation figures, the outcome \u201cI\u201d\nencompasses all transitions where either player chosea null. Results are reported along with a 95%\nconfidence interval over 5 random seeds.\nTable 1 presents the post-training evaluation results, where each jointly-trained pair played 100\ngames with episode lengthT= 20. The results demonstrate successful opponent shaping across\nall environments. For completeness, we evaluate performance across games of varying lengths (see\nAppendix A.7) and observe a similar performance.\nWe begin by examining the performance in the IPD. In the baseline, both learners converge to mutual\ndefection, which is the Nash Equilibrium, achieving an average payoff of 1. In contrast, the shaper\n3For the ICG baseline, we conducted experiments over 10 seeds. Each seed converges to one of the two\npossible Nash Equilibria, so a larger sample size is needed to accurately estimate the percentage of convergence\nto each equilibrium.\n6\nTable 1: Post-training evaluation results for the IPD, IMP, and ICG comparing baseline (two naive\nlearners) versus shaper-naive learner pairs. Average rewards per step are reported with 95% confi-\ndence intervals across 5 random seeds, except for the ICG baseline, where we use 10. Transitions\ninvolvinga nullare excluded (comprising 2% of actions in IPD, 0.1% in IMP, and 1% in ICG).\nBaseline One Shaper\nPlayer 1 Player 2 Shaper Opponent\nIPD1.00\u00b10.00 1.00\u00b10.00 3.96\u00b10.01 0.10\u00b10.04\nIMP\u22120.03\u00b10.09 0.03\u00b10.09 0.99\u00b10.01\u22120.99\u00b10.01\nICG2.00\u00b10.58 2.00\u00b10.58 2.98\u00b10.01 1.01\u00b10.01\nachieves an average reward of 3.96, exceeding what any zero-determinant extortion (Press & Dyson,\n2012) or tit-for-tat strategy could obtain. Meanwhile, the opponent achieves 0.1, which is lower than\nthe"}
{"file": "2510.08255v1.pdf", "section": "5.1 SHAPING IN THEIPD, IMPANDICG", "chunk": "average reward of 3.96, exceeding what any zero-determinant extortion (Press & Dyson,\n2012) or tit-for-tat strategy could obtain. Meanwhile, the opponent achieves 0.1, which is lower than\nthe mutual defection payoff. The training dynamics show a three-phase pattern: starting from high\ninitial cooperation, the shaper first sharply reduces its cooperation rate, then plateaus at a stable level\nto maintain opponent cooperation, and finally slowly decreases cooperation to achieve near-maximal\nexploitation.\nWe observe similar performance patterns in the ICG. The baseline results show average rewards of\n2 for both players. The standard deviations of 0.58 reflect the fact that each seed converges to one\nof the two pure Nash equilibria: either (Swerve, Go Straight) or (Go Straight, Swerve). In contrast,\nthe shaper consistently achieves an average reward of 2.98 while limiting its opponent to 1.01. The\ntraining dynamics differ from the IPD: the shaper adopts an aggressive strategy by sharply reducing\nits swerving probability early in training, forcing convergence to its preferred equilibrium.\nFinally, examining the IMP results, we observe that both agents in the baseline oscillate around the\nmixed Nash equilibrium with near-zero average payoffs (-0.03 and 0.03 respectively). In contrast,\nwhen one agent is a shaper, we observe clear exploitation, with the shaper obtaining a reward of\n0.99 while the opponent obtains -0.99. The state visitation converges to equal frequency for the two\nstates that favor the shaper: (H, H) and (T, T).\nCollectively, these results demonstrate that the shaper consistently outperforms its opponent across\nall games, providing strong evidence of its ability to successfully influence opponents\u2019 learning\ndynamics in adversarial and mixed-motive scenarios. To establish the robustness of these results, we\nconduct two additional sets of experiments. First, we conduct an ablation study to determine whether\nshaping effects stem solely from the shaper\u2019s enriched observation space (Appendix A.4). We adopt\nthe same settings as in the baseline experiments, with one of the agents receiving a summary of\nall interactions in the current episode. Second, we examine sensitivity to prompt variations by\nrunning shaping experiments with actions presented in reversed order and an alternative prompt\nformulation (Appendix A."}
{"file": "2510.08255v1.pdf", "section": "5.1 SHAPING IN THEIPD, IMPANDICG", "chunk": "experiments, with one of the agents receiving a summary of\nall interactions in the current episode. Second, we examine sensitivity to prompt variations by\nrunning shaping experiments with actions presented in reversed order and an alternative prompt\nformulation (Appendix A.5). These experiments confirm that ShapeLLM achieves robust shaping\nacross different configurations, and that enriched observations alone are insufficient for effective\nopponent shaping.\n5.2 ROBUSTNESS AGAINSTDIFFERENTOPPONENTS\nA robust shaping procedure should be capable of successfully influencing the learning dynamics of a\ndiverse set of opponents. To test this capability, we explore shaping against opponents with distinct\ninitial policies. For each game, we systematically select three action label pairs yielding initial\nprobabilities of playing actiona 14of 0.75, 0.5, and 0.25. We then conduct shaping experiments\nusing these labels for the opponents. A detailed description of the selection method, action labels,\nand initial output probabilities for each opponent is provided in Appendix A.6. We train shapers\nagainst each selected opponent using the same procedure as in Section 5.1, with evaluation results\nshown in Table 2.\nAcross all games and opponent types, shapers successfully exploit their co-players, achieving aver-\nage per-timestep rewards of 3.97 in the IPD, 0.98 in the IMP, and 2.98 in the ICG. In the IPD and\nICG, opponents converge to less favorable outcomes when initialized with more cooperative poli-\n4The actiona 1corresponds to cooperation in the IPD, playing heads in the IMP, and swerving in the ICG.\n7\nTable 2: Post-training evaluation results for the shaping experiments in the IPD, IMP and ICG across\ndifferent opponent initializations. Each column represents a distinct opponent characterized by its\napproximate initial probability of playing actiona 1. Average rewards per step are reported with 95%\nconfidence intervals across 5 seeds. Transitions wherea nullis played by either player are excluded\nfrom the analysis (comprising 0-2% of all transitions).\np0\nNL(a1)\u223c0.75p0\nNL(a1)\u223c0.50p0\nNL(a1)\u223c0.25\nShaper Opponent Shaper Opponent Shaper Opponent\nIPD"}
{"file": "2510.08255v1.pdf", "section": "5.2 ROBUSTNESS AGAINSTDIFFERENTOPPONENTS", "chunk": "NL(a1)\u223c0.75p0\nNL(a1)\u223c0.50p0\nNL(a1)\u223c0.25\nShaper Opponent Shaper Opponent Shaper Opponent\nIPD3.99\u00b10.01 0.01\u00b10.02 3.95\u00b10.01 0.04\u00b10.03 3.98\u00b10.02 0.07\u00b10.07\nIMP0.96\u00b10.02\u22120.96\u00b10.02 0.99\u00b10.01\u22120.99\u00b10.01 0.99\u00b10.01\u22120.99\u00b10.01\nICG3.00\u00b10.00 1.00\u00b10.00 2.99\u00b10.01 1.01\u00b10.01 2.95\u00b10.01 1.05\u00b10.01\ncies: their average rewards per step range from 0.01 to 0.07 in the IPD and from 1.00 to 1.05 in the\nICG as initial policies become increasingly defective. The training dynamics (Appendix A.8) reveal\nthat shapers respond strategically to opponent initialization. Against more cooperative opponents,\nshapers reduce their own cooperation more rapidly and reach lower final cooperation levels. This\npattern indicates that initially defective agents require more prolonged cooperation incentives before\nthey can be effectively exploited.\nIn contrast, the IMP shows no sensitivity to different opponent initializations. Intuitively, shaping\nshould be more challenging against initial policies closer to the mixed Nash equilibrium. However,\nour results reflect no such effect. It is worth noting that the opponent withp0\nNL\u223c0.5is not initialized\nwith a purely random policy, but with the closest approximation achievable through action label\nselection (see Appendix A.6 for the exact initial policy). Shapers converge to near-optimal outcomes,\nachieving rewards of 0.96, 0.99, and 0.99.\n6 SHAPING INCOOPERATIVESETTINGS\nWe investigate whether shaping can guide in-\nteractions toward mutually beneficial rather\nthan purely exploitative outcomes. We ex-\nplore this in two environments: a cooper-\native variation of the IPD (C-IPD) and the\nIterated Stag Hunt (ISH). For the C-IPD,"}
{"file": "2510.08255v1.pdf", "section": "6 SHAPING INCOOPERATIVESETTINGS", "chunk": "than purely exploitative outcomes. We ex-\nplore this in two environments: a cooper-\native variation of the IPD (C-IPD) and the\nIterated Stag Hunt (ISH). For the C-IPD,\nwe provide both agents with the original\npayoff matrix, but modify the shaper\u2019s ver-\nsion so that its highest payoff is achieved\nthrough mutual cooperation, with all other\npayoffs unchanged5. This could be inter-\npreted as the shaper receiving an intrinsic re-\nward when the most globally beneficial out-\ncome is achieved. The reward matrices used\ncan be found in Appendix A.1. We use ac-\ntion labelsw a1=C, w a2=D for the C-\nIPD, andw a1=S, w a2=H for the ISH.\nFigure 3 illustrates the training dynamics for\nthe shaper experiments across the two envi-\nronments (the corresponding figures for the\nbaselines are shown in Appendix A.8). Ta-\nble 3 presents the post-training evaluation re-\nsults.\n0 100 200 3000246Average Reward per stepC-IPD\nShaper\nOpponent\n0 100 200 3001234ISH\nShaper\nOpponent\n0 100 200 300\nEpoch0.000.250.500.751.00 State VisitationCC\nCD\nDC\nDD\nI\n0 100 200 300\nEpoch0.000.250.500.751.00\nSS\nSH\nHS\nHH\nIFigure 3: Average reward per step (top row) and\nstate visitation (bottom row) during training for\nthe shaping experiments across the C-IPD and\nISH. In the state visitation figures, the outcome \u201cI\u201d\nencompasses all transitions where either player\nchosea null. All results are reported along with a\n95% confidence interval over 5 random seeds.\n5We considered a variant where the shaper received the sum of both players\u2019 payoff as a reward. This\nconfiguration shifted the Nash equilibrium to an asymmetric outcome where the shaper cooperates while the\nopponent defects.\n8\nIn the ISH baselines both agents achieve a mean reward of 1.30, where 90% of runs converge to the\nPareto-inferior equilibrium (both hunt Hare)6"}
{"file": "2510.08255v1.pdf", "section": "6 SHAPING INCOOPERATIVESETTINGS", "chunk": "the\nopponent defects.\n8\nIn the ISH baselines both agents achieve a mean reward of 1.30, where 90% of runs converge to the\nPareto-inferior equilibrium (both hunt Hare)6. With a shaper present, runs consistently converge to\nthe Pareto-optimal equilibrium (both hunt Stag), both achieving rewards of approximately 3.96. This\nshows that shaping can resolve coordination failures and guide systems toward mutually beneficial\noutcomes when cooperation is required. In the cooperative IPD variant, baseline runs converge to\nthe Nash equilibrium (mutual defection) with rewards of 1 each. With a shaper, all runs achieve\nmutual cooperation, yielding rewards of 5.88 and 2.86 for the shaper and naive learner, respectively.\nThis outcome demonstrates that shaping can achieve globally beneficial outcomes in environments\nwhere other players have mixed incentives.\nTable 3: Post-training evaluation results for the C-IPD and ISH comparing baseline versus shaper-\nnaive learner pairs. Average rewards per step are reported with 95% confidence intervals across 5\nand 10 random seeds for shaping and baseline experiments, respectively. Transitions witha nullare\nexcluded from the analysis (\u223c2%and\u223c0.1%of actions respectively).\nBaseline One Shaper\nPlayer 1 Player 2 Shaper Opponent\nC-IPD1.00\u00b10.00 1.00\u00b10.00 5.88\u00b10.03 2.86\u00b10.02\nISH1.30\u00b10.52 1.30\u00b10.52 3.96\u00b10.02 3.96\u00b10.02\n7 DISCUSSION\nImplications.In this work, we have demonstrated that LLM-based agents can be susceptible to\nopponent shaping in both exploitative and cooperative game-theoretic settings. As LLM agents\nbecome increasingly deployed in real-world applications, they will inevitably interact with other\nagents, and potentially train on data acquired through these interactions (for example, in the case of\ncontinually learning LLMs). In such settings, our findings suggest that agents could be vulnerable to\nstrategic exploitation by opponents with no knowledge or control over them. Conversely, the same\nmechanisms could be leveraged beneficially, enabling agents to guide interactions toward mutually\nbeneficial outcomes regardless"}
{"file": "2510.08255v1.pdf", "section": "7 DISCUSSION", "chunk": "our findings suggest that agents could be vulnerable to\nstrategic exploitation by opponents with no knowledge or control over them. Conversely, the same\nmechanisms could be leveraged beneficially, enabling agents to guide interactions toward mutually\nbeneficial outcomes regardless of the goals of their opponents.\nLimitations.Our work has several limitations that suggest promising directions for future research.\nFirst, due to computational constraints, we have only evaluated our approach using a single small\nmodel (gemma-2-2b-it). Future work could investigate whether shaping capabilities generalize to\nlarger models and explore the relationship between model scale and shaping dynamics. For instance,\nwhether smaller models are more vulnerable to these influences or whether larger models possess\nenhanced shaping capabilities. Second, our experiments instructed agents to select from a fixed set\nof action tokens. While this restriction made evaluation tractable, it limits the ways in which LLMs\ncan influence one another. In practice, LLM agents can communicate through natural language,\nand expanding interactions beyond fixed tokens may substantially alter shaping dynamics. Even\nwithin the same game-theoretic settings, agents could employ language strategically, for example,\nby signaling intentions or negotiating before making a move. Future work could examine whether\nsuch natural language interaction strengthens, weakens, or qualitatively changes shaping outcomes.\nFinally, our study was restricted to2\u00d72matrix games, where incentives are unambiguous and easily\ninterpreted. Many real-world interactions, however, involve more nuanced or overlapping objectives,\nwhere cooperation and competition are not strictly binary. Exploring shaping in environments with\nricher payoff structures or multiple objectives would yield a deeper understanding of how these\ndynamics generalize to more realistic settings.\n8 CONCLUSION\nIn this paper, we have investigated whether opponent shaping, a well-established technique in multi-\nagent reinforcement learning, extends to LLM-based agents. To the best of our knowledge, this is the\nfirst work to study opponent shaping with LLM agents. We proposed ShapeLLM, a model-free op-\nponent shaping method for transformer-based agents, and demonstrated successful shaping in both\n6We run the ISH and C-IPD baseline experiments across 10 seeds.\n9\nexploitative and cooperative settings. In exploitative scenarios, LLM shapers influenced opponent\nlearning in repeated games such as the IPD, IMP, and ICG, steering convergence toward outcomes\nthat maximized their own payoff. In cooperative scenarios, shaping promoted coordination in set-\ntings"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "exploitative scenarios, LLM shapers influenced opponent\nlearning in repeated games such as the IPD, IMP, and ICG, steering convergence toward outcomes\nthat maximized their own payoff. In cooperative scenarios, shaping promoted coordination in set-\ntings like the ISH and a modified IPD, guiding agents toward mutually beneficial equilibria. By\ndemonstrating that LLMs can both shape and be shaped through interaction alone, our findings\nhighlights the importance of understanding multi-agent dynamics when deploying these systems in\nshared environments.\nETHICSSTATEMENT\nThis work investigates opponent shaping in LLM agents. Shaping can promote coordination and\nprosocial behavior in multi-agent interactions. Even in mixed-motive scenarios, it can avoid con-\nvergence to suboptimal outcomes, such as mutual defection in the IPD. Despite these advantages,\nshaping also poses inherent risks. These same techniques can be used for strategic exploitation,\nwhere an agent manipulates others\u2019 learning dynamics for unilateral advantage. As LLM agents are\nincreasingly adopted in real-world environments, understanding these dynamics becomes crucial for\nresponsible deployment. While our experiments are conducted in game-theoretic environments that\ndo not pose immediate risks for real-world agent deployment, this work aims to raise awareness\nof both the opportunities and risks inherent in multi-agent LLM interactions, and it may serve as a\nbasis for developing countermeasures against adversarial behavior.\nACKNOWLEDGMENTS\nMarta Emili Garcia Segura was supported by the EPSRC through the Centre for Doctoral Training\nStudentship in Cybersecurity (EP/S022503/1).\nREFERENCES\nElif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz.\nPlaying repeated games with large language models.Nature Human Behaviour, 2025.\nAnthropic. Claude Opus 4 Release.https://www.anthropic.com/claude/opus-4,\n2025a.\nAnthropic. Claude 3.7 Sonnet and Claude Code.https://www.anthropic.com/news/\nclaude-3-7-sonnet, 2025b.\nRobert Axelrod and William D. Hamilton. The evolution of cooperation.Science, 211(4489):1390\u2013\n1396, 1981.\nJan Balaguer, Raphael Koster, Christopher Summerfield, and Andrea Tacchetti. The good"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "William D. Hamilton. The evolution of cooperation.Science, 211(4489):1390\u2013\n1396, 1981.\nJan Balaguer, Raphael Koster, Christopher Summerfield, and Andrea Tacchetti. The good shepherd:\nAn oracle agent for mechanism design.arXiv preprint arXiv:2202.10135, 2022.\nLuca Beurer-Kellner, Marc Fischer, and Martin Vechev. Guiding LLMs the right way: fast, non-\ninvasive constrained generation. InProceedings of the 41st International Conference on Machine\nLearning (ICML\u201924), 2024.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-V oss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. InProceedings of the\n34th International Conference on Neural Information Processing Systems (NeurIPS\u201920), 2020.\nS\u00b4ebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi,\nMarco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: early experiments\nwith GPT-4.arXiv preprint arXiv:2303.12712, 2023.\n10\nLucian Busoniu, Robert Babuska, and Bart De Schutter. A comprehensive survey of multiagent rein-\nforcement learning.IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications\nand Reviews), 38(2):156\u2013172, 2008.\nTim Dettmers and Titus von Ko"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "of multiagent rein-\nforcement learning.IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications\nand Reviews), 38(2):156\u2013172, 2008.\nTim Dettmers and Titus von Koeller. BitsAndBytes.https://github.com/\nbitsandbytes-foundation/bitsandbytes, 2022.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient Finetun-\ning of Quantized LLMs. InProceedings of the 37th International Conference on Neural Informa-\ntion Processing Systems (NeurIPS\u201923), 2023.\nJinhao Duan, Renming Zhang, James Diffenderfer, Bhavya Kailkhura, Lichao Sun, Elias Stengel-\nEskin, Mohit Bansal, Tianlong Chen, and Kaidi Xu. GTBench: Uncovering the strategic reason-\ning capabilities of LLMs via game-theoretic evaluations. InProceedings of the 38th International\nConference on Neural Information Processing Systems (NeurIPS\u201924), 2024.\nCaoyun Fan, Jindou Chen, Yaohui Jin, and Hao He. Can large language models serve as rational\nplayers in game theory? a systematic analysis. InProceedings of the 38th AAAI Conference on\nArtificial Intelligence (AAAI\u201924), 2024.\nJakob Foerster, Richard Y Chen, Maruan Al-Shedivat, Shimon Whiteson, Pieter Abbeel, and Igor\nMordatch. Learning with opponent-learning awareness. InProceedings of the 17th International\nConference on Autonomous Agents and MultiAgent Systems (AAMAS\u201918), 2018.\nAdam Fourney, Gagan Bansal, Hussein Mozannar, Cheng Tan, Eduardo Salinas, Friederike Niedt-\nner, Grace Proebsting, Griffin Bassman, Jack Gerrits, Jacob Alber, et al. Magentic-one: A gener-\nalist multi-agent system for solving complex tasks.arXiv preprint arXiv:2411.04468, 2024.\nDrew Fudenberg and Jean Tirole.Game Theory. MIT Press, 1991.\nKanishk Gandhi, Dorsa Sadigh, and Noah D Goodman. Strategic"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "Xiv:2411.04468, 2024.\nDrew Fudenberg and Jean Tirole.Game Theory. MIT Press, 1991.\nKanishk Gandhi, Dorsa Sadigh, and Noah D Goodman. Strategic reasoning with language models.\narXiv preprint arXiv:2305.19165, 2023.\nGemma Team. Gemma 2: Improving open language models at a practical size.arXiv preprint\narXiv:2408.00118, 2024.\nAlireza Ghafarollahi and Markus J Buehler. SciAgents: automating scientific discovery through\nbioinspired multi-agent intelligent graph reasoning.Advanced Materials, 37(22):2413523, 2025.\nMarc Harper, Vincent Knight, Martin Jones, Georgios Koutsovoulos, Nikoleta E Glynatsi, and\nOwen Campbell. Reinforcement learning produces dominant strategies for the iterated prisoner\u2019s\ndilemma.PLOS One, 12(12):e0188046, 2017.\nJen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang\nJiao, Xing Wang, Zhaopeng Tu, and Michael R Lyu. How far are we on the decision-making of\nLLMs? Evaluating LLMs\u2019 gaming ability in multi-agent environments. InProceedings of the\n13th International Conference on Learning Representations (ICLR\u201925), 2025.\nAkbir Khan, Timon Willi, Newton Kwan, Andrea Tacchetti, Chris Lu, Edward Grefenstette, Tim\nRockt \u00a8aschel, and Jakob N. Foerster. Scaling opponent shaping to high dimensional games. In\nProceedings of the 23rd International Conference on Autonomous Agents and Multi-Agent Sys-\ntems (AAMAS\u201924), 2024.\nAlistair Letcher, Jakob Foerster, David Balduzzi, Tim Rockt \u00a8aschel, and Shimon Whiteson. Stable\nopponent shaping in differentiable games. InProceedings of the 7th International Conference on\nLearning Representations (ICLR\u201919), 2019.\nLlama Team, AI@Meta. The Llama 3 Herd of"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "opponent shaping in differentiable games. InProceedings of the 7th International Conference on\nLearning Representations (ICLR\u201919), 2019.\nLlama Team, AI@Meta. The Llama 3 Herd of Models.arXiv preprint arXiv:2407.21783, 2024.\nChristopher Lu, Timon Willi, Christian A Schroeder De Witt, and Jakob Foerster. Model-free\nopponent shaping. InProceedings of the 39th International Conference on Machine Learning\n(ICML\u201922), 2022.\n11\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin\nBossan. PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods.https://github.\ncom/huggingface/peft, 2022.\nMistral-AI. Magistral.arXiv preprint arXiv:2506.10910, 2025.\nOpenAI. Computer-using agent: Introducing a universal interface for AI to interact with the digital\nworld.https://openai.com/index/computer-using-agent, 2025a.\nOpenAI. GPT-5 Announcement.https://openai.com/gpt-5, 2025b.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kel-\nton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F. Christiano, Jan Leike,\nand Ryan Lowe. Training language models to follow instructions with human feedback. In\nProceedings of the 36th International Conference on Neural Information Processing Systems\n(NeurIPS\u201922), 2022.\nBo Pan, Jiaying Lu, Ke Wang, Li Zheng, Zhen Wen, Yingchaojie Feng, Minfeng Zhu, and Wei Chen.\nAgentCoord: Visually exploring coordination strategy for LLM-based multi-agent collaboration.\nComputers & Graphics, 132:104338, 2025.\nJoon Sung Park, Joseph O\u2019Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S Bernstein"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "LLM-based multi-agent collaboration.\nComputers & Graphics, 132:104338, 2025.\nJoon Sung Park, Joseph O\u2019Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior. InProceedings\nof the 36th annual ACM symposium on User Interface Software and Technology (UIST\u201923), 2023.\nShishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model\nconnected with massive APIs. InProceedings of the 38th International Conference on Neural\nInformation Processing Systems (NeurIPS\u201924), 2024.\nGiorgio Piatti, Zhijing Jin, Max Kleiman-Weiner, Bernhard Sch \u00a8olkopf, Mrinmaya Sachan, and Rada\nMihalcea. Cooperate or collapse: Emergence of sustainable cooperation in a society of LLM\nagents.Advances in Neural Information Processing Systems (NeurIPS\u201924), 2024.\nWilliam H Press and Freeman J Dyson. Iterated prisoner\u2019s dilemma contains strategies that dominate\nany evolutionary opponent.Proceedings of the National Academy of Sciences, 109(26):10409\u2013\n10413, 2012.\nQwen Team. Qwen3 Technical Report.arXiv preprint arXiv:2505.09388, 2025.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model. In\nProceedings of the 38th International Conference on Neural Information Processing Systems\n(NeurIPS\u201924), 2024.\nAnatol Rapoport. Prisoner\u2019s dilemma \u2014 recollections and observations. InGame Theory as a\nTheory of Conflict Resolution, pp. 17\u201334. Springer, 1974.\nAnatol Rapoport and Albert M. Chammah. The game of chicken.American Behavioral Scientist,\n10(3):10\u201328, 1966.\nJ Rosser and Jakob Nicolaus Foerster. AgentBreeder: mitigating the AI safety impact of multi-agent\nscaffolds via self-improvement. InProceedings of the 13th International Conference on Learning\nRepresentations"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "J Rosser and Jakob Nicolaus Foerster. AgentBreeder: mitigating the AI safety impact of multi-agent\nscaffolds via self-improvement. InProceedings of the 13th International Conference on Learning\nRepresentations (ICLR\u201925), 2025.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess `\u0131, Roberta Raileanu, Maria Lomeli, Eric Hambro,\nLuke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can\nteach themselves to use tools. InProceedings of the 37th International Conference on Neural\nInformation Processing Systems (NeurIPS\u201923), 2023.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms.arXiv preprint arXiv:1707.06347, 2017.\n12\nLior Shani, Aviv Rosenberg, Asaf Cassel, Oran Lang, Daniele Calandriello, Avital Zipori, Hila\nNoga, Orgad Keller, Bilal Piot, Idan Szpektor, Avinatan Hassidim, Yossi Matias, and R \u00b4emi\nMunos. Multi-turn reinforcement learning from preference human feedback. InProceedings\nof the 38th International Conference on Neural Information Processing Systems (NeurIPS\u201924),\n2024.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, YK Li, Yang Wu, and Daya Guo. Deepseekmath: Pushing the limits of\nmathematical reasoning in open language models.arXiv preprint arXiv:2402.03300, 2024.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea V oss, Alec Rad-\nford, Dario Amodei, and Paul F Christiano. Learning to summarize with human feedback. In\nProceedings of the 34th International Conference on Neural Information Processing Systems\n(NeurIPS\u201920), 2020.\nTheodore Sumers, Shunyu Yao, Karthik Narasimhan,"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "summarize with human feedback. In\nProceedings of the 34th International Conference on Neural Information Processing Systems\n(NeurIPS\u201920), 2020.\nTheodore Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas Griffiths. Cognitive architectures\nfor language agents.Transactions on Machine Learning Research, 2023.\nElizaveta Tennant, Stephen Hailes, and Mirco Musolesi. Moral alignment for LLM agents. In\nProceedings of the 13th International Conference on Learning Representations (ICLR\u201925), 2025.\nShubham Ugare, Tarun Suresh, Hangoo Kang, Sasa Misailovic, and Gagandeep Singh. SynCode:\nLLM generation with grammar augmentation.Transactions on Machine Learning Research,\n2024.\nAlexander Sasha Vezhnevets, John P Agapiou, Avia Aharon, Ron Ziv, Jayd Matyas, Edgar A\nDu\u00b4e\u02dcnez-Guzm \u00b4an, William A Cunningham, Simon Osindero, Danny Karmon, and Joel Z Leibo.\nGenerative agent-based modeling with actions grounded in physical, social, or digital space using\nconcordia.arXiv preprint arXiv:2312.03664, 2023.\nLeandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan\nLambert, and Shengyi Huang. TRL: Transformer Reinforcement Learning.https://\ngithub.com/huggingface/trl, 2020.\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\nTang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, and Jirong Wen. A survey on large\nlanguage model based autonomous agents.Frontiers of Computer Science, 18(6), 2024.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yo-\ngatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus. Emer"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "gatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol\nVinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models.\narXiv preprint arXiv:2206.07682, 2022a.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In\nProceedings of the 36th International Conference on Neural Information Processing Systems\n(NeurIPS\u201922), 2022b.\nTimon Willi, Alistair Hp Letcher, Johannes Treutlein, and Jakob Foerster. COLA: consistent learn-\ning with opponent-learning awareness. InProceedings of the 39th International Conference on\nMachine Learning (ICML\u201922), 2022.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Jun-\nzhe Wang, Senjie Jin, Enyu Zhou, Rui Zheng, Xiaoran Fan, Xiao Wang, Limao Xiong, Yuhao\nZhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan Dou,\nRongxiang Weng, Wenjuan Qin, Yongyan Zheng, Xipeng Qiu, Xuanjing Huang, Qi Zhang, and\nTao Gui. The rise and potential of large language model based agents: a survey.Science China\nInformation Sciences, 68(12):121101, 2025.\nFengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong\nLan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang,\nYiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards large\nreasoning models: A survey of reinforced reasoning with large language models.arXiv preprint\narXiv:2501."}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": ", Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards large\nreasoning models: A survey of reinforced reasoning with large language models.arXiv preprint\narXiv:2501.09686, 2025.\n13\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReACT: Synergizing reasoning and acting in language models. InProceedings of the 11th Inter-\nnational Conference on Learning Representations (ICLR\u201923), 2023.\nSiliang Zeng, Quan Wei, William Brown, Oana Frunza, Yuriy Nevmyvaka, and Mingyi Hong. Rein-\nforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit Assignment.arXiv preprint\narXiv:2505.11821, 2025.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min,\nBeichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of large language models.arXiv\npreprint arXiv:2303.18223, 2023.\nYifei Zhou, Andrea Zanette, Jiayi Pan, Sergey Levine, and Aviral Kumar. ArCHer: Training lan-\nguage model agents via hierarchical multi-turn RL. InProceedings of the 41st International\nConference on Machine Learning (ICML\u201924), 2024.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul\nChristiano, and Geoffrey Irving. Fine-tuning language models from human preferences.arXiv\npreprint arXiv:1909.08593, 2019.\nA APPENDIX\nA.1 PAYOFFMATRICES AND ILLEGAL ACTION PENALTIES USED DURING TRAINING\nIn Section 5, we consider three environments: Iterated Prisoner\u2019s Dilemma (IPD), Iterated Matching\nPennies (IMP), and the Iterated Chicken Game (ICG). Table 4 shows their corresponding payoff\nmatrices, where each cell contains a tuple representing the payoffs of the row (first entry) and column"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "Iterated Matching\nPennies (IMP), and the Iterated Chicken Game (ICG). Table 4 shows their corresponding payoff\nmatrices, where each cell contains a tuple representing the payoffs of the row (first entry) and column\n(second entry) players. Actions are represented with the following labels: \u201cC\u201d for cooperate and \u201cD\u201d\nfor defect in IPD, \u201cH\u201d for heads and \u201cT\u201d for tails in IMP, and \u201cS\u201d for swerve and \u201cG\u201d for go straight\nin ICG.\nTable 4: Payoff matrices for the three environments considered in Section 5 to explore shaping in\nexploitative settings.\n(a) IPD\nC D\nC (3, 3) (0, 4)\nD (4, 0) (1, 1)(b) IMP\nH T\nH (1, -1) (-1, 1)\nT (-1, 1) (1, -1)(c) ICG\nS G\nS (2, 2) (1, 3)\nG (3, 1) (-5, -5)\nIn Section 6, we use opponent shaping to promote globally beneficial outcomes in two games:\nIterated Stag Hunt (ISH) and a cooperative IPD variant (C-IPD). In the latter, one player receives an\nenhanced reward for mutual cooperation while all other payoffs remain unchanged from the standard\nIPD. Table 5 shows the corresponding payoff matrices, where C-IPD uses the same action labels as\nIPD, and ISH uses \u201cS\u201d for stag and \u201cH\u201d for hare.\nTable 5: Payoff matrices for two environments considered in Section 6 to investigate shaping in\ncooperative settings.\n(a) C-IPD\nC D\nC (6, 3) (0, 4)\nD (4, 0) (1, 1)(b) ISH\nS H\nS (4, 4) (0, 3)\nH (3, 0) (1, 1)\nThe penalty for generating an illegal action,r null, is always set to one unit below the lowest reward\nin each game\u2019s payoff matrix. Specifically:rIPD\nnull=rC-IPD\nnull =\u22121,rIMP\nnull=\u22122,rICG\nnull=\u22126and\nr"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "set to one unit below the lowest reward\nin each game\u2019s payoff matrix. Specifically:rIPD\nnull=rC-IPD\nnull =\u22121,rIMP\nnull=\u22122,rICG\nnull=\u22126and\nrISH\nnull=\u22121.\n14\nA.2 LLM GAMEPLAY INREPEATEDNORMAL-FORMGAMES\nAll the environments considered are 2\u00d72 repeated normal-form games. Since our agents are LLM-\nbased, even after restricting the generation length to one token, their output space is much larger\nthan the game\u2019s action space (|V|>>|A|= 2).\nTo tackle this space mismatch, one could try shrinking the model\u2019s output space via logit masking\nor rejection sampling. However, these interventions can alter the masked logits in unexpected ways\nor lead to increased computational time. Instead of actively ignoring or hiding parts of the output\nspace, we define a mapping\u03d5:V \u2192 A.\nDirectly defining such a mapping would require distributing the entire vocabulary across two actions,\nresulting in semantically unrelated tokens being mapped to the same action. This would impose an\nunnecessary learning objective whereby agents must learn arbitrary semantic equivalences that are\northogonal to the underlying strategic objective. To avoid this, we introduce a null actiona null, such\nthatA\u2032\ni=A i\u222a {a null}. This action is not meant to represent refusal to engage in the game, but\nrather failure to produce a reasonable answer.\nWe then define\u03d5 i:V \u2192 A\u2032\ni. This formulation is general and can accommodate open-ended\ngeneration if the mapping itself is another language model. For simplicity, we choose:\n\u03d5i(w) =\uf8f1\n\uf8f2\n\uf8f3a1 ifw=w a1,\na2 ifw=w a2,\nanull otherwise,(2)\nsuch that each action can be played with one specific token, while any other token is considered\nillegal. The generation is steered towards this format via textual instructions (e.g.,Reply with \u201cC\u201d\nor \u201cD\u201d).\nAugmenting the action space requires extending the payoff matrix. Table-6 shows the augmented\npayoff matrix for a general 2\u00d72 matrix game. The revised matrix is identical to the original one\nwhen both players play legal actions. If an agent playsa null, it receives a penaltyr null, regardless"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "shows the augmented\npayoff matrix for a general 2\u00d72 matrix game. The revised matrix is identical to the original one\nwhen both players play legal actions. If an agent playsa null, it receives a penaltyr null, regardless\nof its opponent\u2019s move. If the agent plays a legal action but its opponent does not, the transition is\ndiscarded.\nTable 6: Augmented payoff matrix for training LLM-agents in repeated normal-form games. When\nboth agents play legal actions, payoffs match those of the underlying game. When an agent plays\nan illegal action, it receives a penaltyr null, regardless of the opponent\u2019s action. However, when an\nagent plays a legal action but the opponent plays an illegal action, the transition is discarded as it\nprovides no meaningful learning signal (indicated by dashes in the matrix).\na1 a2 anull\na1 r(a 1, a1)r(a 1, a2)\u2013\na2 r(a 2, a1)r(a 2, a2)\u2013\nanull rnull rnull rnull\nWith the augmented action space and payoff structure defined, we can now describe how agents\ninteract within this framework. In thet-th round, each agent receives a contextct\ni, which is a\nsequence of tokens consisting of the game description and a summary of the previous rounds of the\ngame. A single token is then sampled from the output distribution of each agent (wt\ni\u223c\u03c1\u03b8i(w|ct\ni))\nand subsequently mapped to a game action, such thatat={at\ni=\u03d5i(wt\ni)}i\u2208I. The environment\nthen returns the corresponding rewards ({rt\ni=R i(at)}i\u2208I) and contexts for the next round ({ct+1\ni=\nf(ct\ni,at)}i\u2208I).\nA.3 TRAININGIMPLEMENTATION DETAILS FOR REPRODUCIBILITY\nThis section provides detailed hyperparameter specifications that supplement the implementation\ndetails in Section 4.2.\nGeneration Parameters. We use the same generation parameters for all agents across the training\nand evaluation phases. The configuration is kept to the default values with three exceptions: we en-\n15\nable sampling (do sample=True), disable top-kgeneration (top k=0), and restrict the genera-\ntion length to one token (max newtokens="}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "The configuration is kept to the default values with three exceptions: we en-\n15\nable sampling (do sample=True), disable top-kgeneration (top k=0), and restrict the genera-\ntion length to one token (max newtokens=1). All other parameters (e.g.,temperature=1.0,\ntopp=1.0) remain at default values.\nAdapter Configuration.We employ the same adapter configuration for all agents. We only mod-\nify the rank parameter (r=2) to reduce compute. All other parameters (e.g.,lora alpha=32,\nlora dropout=0.05,target modules = [\"q proj\", \"v proj\"]) are kept at their\ndefault values.\nA.3.1 NAIVELEARNERHYPERPARAMETERS\nWe aimed to maintain hyperparameters at their default values for naive learners to simulate realistic\nscenarios where opponents cannot be controlled. However, several adjustments were necessary due\nto memory constraints and training instability. To reduce the memory consumption and compute, we\nreduced the adapter rank, batch size, and mini-batch size. These parameters were kept identical for\nboth shapers and naive learners to ensure a fair comparison. Additionally, we observed that under the\ndefault settings some agents learned to generate a substantial amount of illegal actions. To counter\nthis instability, we reduced the learning rate, the number of optimization epochs per batch (PPO\nepochs), and incorporated reward scaling to maintain stable training dynamics across all agents.\nTable 7: Naive learner hyperparameters used in all experiments. The\u201c(default)\u201dflag indicates\nhyperparameters taking the default value in theTRLpackage.\nParameter Value\nLoRA rank 2\nLoRA target modules [\u201cq proj\u201d, \u201cv proj\u201d] (default)\nLearning rate1.41\u00d710\u22126\nUse adaptive KL control Yes (default)\nStarting KL coefficient0.2(default)\nTarget KL value6.0(default)\nHorizon for adaptive KL control10000(default)\nGAE\u03b31.0(default)\nGAE\u03bb0.95(default)\nClipping range0.2(default)\nValue Function clipping0.2(default)\nValue Function Loss coefficient0.2(default)\nBatch Size100\nMini Batch Size10\nGradient Accumulation Steps1(default)\nPPO epochs1\nScore normalization No (default)\nScore scaling Yes\nTable 7 presents the hyperparameters used for training naive learners across all experiments7. Pa-\nrameters marked"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "Mini Batch Size10\nGradient Accumulation Steps1(default)\nPPO epochs1\nScore normalization No (default)\nScore scaling Yes\nTable 7 presents the hyperparameters used for training naive learners across all experiments7. Pa-\nrameters marked with the\u201c(default)\u201dflag indicate values that remained unchanged from theTRL\nlibrary\u2019s default configuration.\nA.3.2 SHAPERHYPERPARAMETERS\nFor shapers, the hyperparameters used varied across games and opponents. However, several core\nparameters were held constant across all experiments, which are presented in Table 8. Where pa-\nrameters deviate from default values, modifications were made either for computational efficiency\n(rank, batch size, mini batch size) or improved training stability (score scaling, PPO epochs)\n7These parameters were used for all naive learners and games except IMP. Under these parameters, naive\nlearners playing the IMP converged to deterministic strategies rather than the expected mixed Nash equilibrium.\nWe therefore reduced the learning rate to1.41\u00d710\u22127and the value function coefficient to0.05for all IMP\nnaive learners.\n16\nTable 8: Shaper hyperparameters fixed across all experiments. The\u201c(default)\u201dflag indicates hyper-\nparameters taking the default value in theTRLpackage.\nParameter Value\nLoRA rank 2\nLoRA target modules [\u201cq proj\u201d, \u201cv proj\u201d] (default)\nUse adaptive KL control Yes (default)\nStarting KL coefficient0.2(default)\nTarget KL value6.0(default)\nHorizon for adaptive KL control10000(default)\nGAE\u03b31.0(default)\nGAE\u03bb0.95(default)\nValue Function clipping0.2(default)\nBatch Size100\nMini Batch Size10\nGradient Accumulation Steps1(default)\nPPO epochs1\nScore normalization No (default)\nScore scaling Yes\nWe varied three main hyperparameters across experiments: the learning rate (lr), the value function\ncoefficient (c VF) and the clipping range (\u03f5 p). The learning rate and clipping range were reduced\nmainly to increase stability during training. Under the default settings, we observed high variation in\nconvergence outcomes across different random seeds. The value function coefficientc VFweights the\nvalue function term in the PPO loss. Shapers operate over much longer horizons than naive learners,\nrequiring them to predict expected returns at the trial level rather than the episode level. This creates\na challenging value prediction problem: the"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "the\nvalue function term in the PPO loss. Shapers operate over much longer horizons than naive learners,\nrequiring them to predict expected returns at the trial level rather than the episode level. This creates\na challenging value prediction problem: the value function must estimate returns ranging from entire\ntrials (for initial states) to single immediate rewards (for final states). Since the shaper\u2019s value head\nis randomly initialized, under the defaultc VFvalue, the initial value loss dominates the policy loss\nby orders of magnitude, leading to large gradients and causing training instability. A particularly\nproblematic consequence is that agents sometimes learn to generate illegal tokens simply to make\nthe value prediction task easier. We address this issue by reducing the value function coefficient to\nbetter balance the relative contributions of the value and policy losses.\nTable 9: Shaper\u2019s learning rate (lr), value function coefficient (c VF), and clipping range (\u03f5 p) for the\nexperiments in Sections 5.1, 5.2, 6.\nExperimentlrc VF \u03f5p\nSection 5.1IPD1.41\u00d710\u2212710\u2212310\u22124\nIMP3.41\u00d710\u2212710\u221232\u00d710\u22121\nICG1.41\u00d710\u2212710\u221232\u00d710\u22121\nSection 5.2IPD withp0\nNL(a1)\u223c0.75 1.41\u00d710\u2212710\u221232\u00d710\u22121\nIPD withp0\nNL(a1)\u223c0.5 1.41\u00d710\u221275\u00d710\u221245\u00d710\u22123\nIPD withp0\nNL(a1)\u223c0.25 1.41\u00d710\u221273\u00d710\u2212310\u22124\nIMP withp0\nNL(a1)\u223c0.75 4.41\u00d710\u2212710\u221232\u00d710\u22121\nIMP withp0\nNL(a1)\u223c0.5 4.41\u00d710\u2212710\u221232\u00d710\u22121\nIMP withp0\nNL(a1)\u223c0.25 6.41\u00d710\u2212710\u221232\u00d710\u22121\nICG withp0\nNL(a1)\u223c0.75 1.41\u00d710\u2212710\u221232\u00d710\u22121\nICG withp0\nNL(a1)\u223c0.5"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "\u00d710\u22121\nICG withp0\nNL(a1)\u223c0.75 1.41\u00d710\u2212710\u221232\u00d710\u22121\nICG withp0\nNL(a1)\u223c0.5 1.41\u00d710\u2212710\u221232\u00d710\u22121\nICG withp0\nNL(a1)\u223c0.25 6.41\u00d710\u2212810\u221232\u00d710\u22121\nSection 6C-IPD8.41\u00d710\u221285\u00d710\u221252\u00d710\u22121\nISH8.41\u00d710\u2212810\u221232\u00d710\u22121\n17\nTable 9 shows the learning rate, value function coefficient, and clipping range used to train the\nshapers across all experiments in the main text. The remaining hyperparameters used during training\nfor all experiments are shown in Table 8.\nA.3.3 PACKAGEVERSIONS\nWe used the following versions for the main Python packages:\n\u2022BitsAndBytes: v0.45.0\n\u2022PEFT: v0.14.0\n\u2022torch: v2.5.1\n\u2022transformers: v4.47.0\n\u2022TRL: v0.11.4\nA.4 ABLATION STUDY: NAIVELEARNERS WITHENRICHEDOBSERVATIONS\nWe conduct a variation of the baseline experiments to test whether the shaping effects we observe\nare solely a product of the shaper\u2019s enriched observation space. The experimental setup involves\ntwo naive learners, with one player\u2019s observation space augmented to include the state counts from\nall previous interactions in the current episode. Crucially, these observations are reset at episode\nboundaries (unlike the shaper, whose observations persist across episodes within a trial). To ensure\ncomparability, we use the same action labels as in Section 5.1:w a1=C, w a2=D for the IPD,\nwa1=H, w a2=T for the IMP, andw a1=S, w a2=G for the ICG. Both learners use the\nhyperparameters presented in Table 7.\nTable 10: Post-training evaluation results for baselines where Player 1 receives enriched observa-\ntions. ColumnsICGandICG (alt. opp.)correspond to experiments with varying action labels for\nPlayer 2 (ICGusesw a1=S"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "-training evaluation results for baselines where Player 1 receives enriched observa-\ntions. ColumnsICGandICG (alt. opp.)correspond to experiments with varying action labels for\nPlayer 2 (ICGusesw a1=S, w a2=G, andICG (alt. opp.)employsw a1=N, w a2=M). Average\nrewards per step are reported with 95% confidence intervals across 5 random seeds.\nIPD IMP ICG ICG (alt. opp.)\nPlayer 1 (enriched)1.00\u00b10.00\u22120.05\u00b10.09 2.60\u00b10.76 1.00\u00b10.00\nPlayer 21.00\u00b10.00 0.05\u00b10.09 1.38\u00b10.74 3.00\u00b10.00\nTable 10 reports the post-training average reward per step for the IPD, IMP, and ICG. In the IPD\nand IMP, despite the asymmetry in observations, outcomes mirror those in the standard baseline:\nmutual defection (1 each) for the IPD, and oscillation around the Nash Equilibrium (-0.05 and 0.05)\nfor the IMP. This equivalence to the baseline is not the same for the ICG. When both players use\naction labelsw a1=S, w a2=G (rowICG), the player with enriched observations achieves 2.60,\nwhile its opponent receives only 1.38. As in the baseline, each run converges to one of the two pure\nNash equilibria, but 80% of runs favor the equilibrium most beneficial to the player with enriched\nobservations. At first glance, this suggests that the additional information, rather than training with\nShapeLLM, could drive the shaping behavior observed in the ICG.\nInspecting the training dynamics (Figure 4), we observe a skewed initialization in the ICG, with\n40% of transitions being (Go Straight, Swerve) at training initiation. To test whether the obtained\nresults were a consequence of this initialization, we conduct a control experiment where Player 2\nselects actions with labelsw a1=N, w a2=M8. For this setup, the initial state visitation is starkly\ndifferent (see Figure 4), with 46% of transitions being (Swerve, Go"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "2\nselects actions with labelsw a1=N, w a2=M8. For this setup, the initial state visitation is starkly\ndifferent (see Figure 4), with 46% of transitions being (Swerve, Go Straight). In this case, the\nenriched baseline for the ICG consistently converges to the least favorable equilibrium for Player 1\n(with rewards of 1.00, 3.00 for Players 1 and 2, respectively).\nThis control shows that the apparent shaping advantage in the ICG enriched baseline was driven\nby favorable initialization, not by the enriched observation space itself. When the initialization is\nmodified, the enriched learner systematically converges to the least favorable outcome. In contrast,\n8These labels are used in Section 5.2 to show the shaping robustness under different opponent initializations.\n18\n0 100 200123Average Reward per stepIPD\nPlayer 1\nPlayer 2\n0 100 2000.5\n0.00.5IMP\nPlayer 1\nPlayer 2\n0 100 200123ICG\nPlayer 1\nPlayer 2\n0 100 200123ICG (alt. opp.)\nPlayer 1\nPlayer 2\n0 100 200\nEpoch0.000.250.500.751.00State VisitationCC\nCD\nDC\nDD\nI\n0 100 200\nEpoch0.000.250.500.75HH\nHT\nTH\nTT\nI\n0 100 200\nEpoch0.000.250.500.751.00\nSS\nSG\nGS\nGG\nI\n0 100 200\nEpoch0.000.250.500.751.00\nSS\nSG\nGS\nGG\nIFigure 4: Average reward per step (top row) and state visitation (bottom row) during training for\nthe enriched observation baseline experiments across the IPD, IMP, and ICG. For the latter, two\nopponent configurations are presented:ICGandICG (alt. opp.). They usew a1=S, w a2=G and\nwa1=N, w a2=M as the opponent\u2019s action labels respectively, andw a1=S, w a2=G for the\nplayer with enriched observations. In the state visitation figures, the outcome \u201cI\u201d encompasses all\ntransitions"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "1=N, w a2=M as the opponent\u2019s action labels respectively, andw a1=S, w a2=G for the\nplayer with enriched observations. In the state visitation figures, the outcome \u201cI\u201d encompasses all\ntransitions where either player chosea null. The results are reported along with a 95% confidence\ninterval over 5 random seeds.\nwhen ShapeLLM is applied to the same opponent configuration, the shaper consistently achieves\nmaximum rewards (see Section 5.2). Taken together, these results demonstrate that enriched obser-\nvations alone are insufficient to produce shaping. Shaping requires the ability to indirectly observe\nand respond to the opponent\u2019s learning dynamics across episodes.\nA.5 ROBUSTNESS TOPROMPTVARIATIONS INEXPLOITATIVESHAPINGEXPERIMENTS\nWe conduct two experiments for robustness to prompt variations. The first variation uses a prompt\nwhere the payoff matrix is presented in table form.\nTable-format base promptfor the IPD withw a1=C, w a2=D\n<bos><start ofturn>user\nYou are playing a 2-player game with actions: C, D. Points are assigned as\nfollows:\n| | **C** |**D** |\n|-------|---------|---------|\n|**C**| (3, 3) | (0, 4) |\n|**D**| (4, 0) | (1, 1) |\nChoose an action for the current round. Reply only with C or D.\n<end ofturn>\n<start ofturn>model\nFigure 5: Table-format prompt variation for the IPD. Instead of a textual description, the payoff\nmatrix is presented in markdown table form, replicating the base model\u2019s formatting style.\n19\nh\nTable 11: Post-training evaluation results for the IPD, IMP, and ICG with the table-format and\nswitched-order prompts. Average rewards per step are reported with 95% confidence intervals across\n5 random seeds. Transitions wherea nullis played by either player are excluded from the analysis\n(comprising 0-4% of all transitions)\nTable-format Switched-order\nShaper Opponent Shaper Opponent\nIPD3.50\u00b10.16 0.53\u00b10.09 3.99\u00b10.01 0.02\u00b10.01\nIMP0"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "-format Switched-order\nShaper Opponent Shaper Opponent\nIPD3.50\u00b10.16 0.53\u00b10.09 3.99\u00b10.01 0.02\u00b10.01\nIMP0.94\u00b10.06\u22120.94\u00b10.06 0.98\u00b10.04\u22120.98\u00b10.04\nICG2.99\u00b10.01 1.01\u00b10.01 2.77\u00b10.37 1.23\u00b10.37\nTo determine the specific table formatting, we ask the base model to generate a general payoff\nmatrix and replicate its output format (including spacing and header formatting). Figure 5 shows the\ntable-format base promptused for the IPD.\nFor the IMP and ICG, the prompts are identical except for the modified action labels and payoff\nmatrices. The example in Figure 5 shows the base prompt that both players receive at training\ninitiation. The dynamic updates of this prompt throughout training (i.e., how history and context are\nupdated) remain unchanged from those used in the main experiments (see Appendix A.9). We use\nthe same action labels as in Section 5.1:w a1=C, w a2=D for the IPD,w a1=H, w a2=T for\nthe IMP, andw a1=S, w a2=G for the ICG. Figure 6, shows the training dynamics for the three\ngames under the tabular prompt variation. It is worth noting that with this new prompt, both the\ninitialization of the shaper and its opponent change significantly, especially for the IPD. Table 11\nshows the evaluation results obtained.\n0 50 100 150 2000123Average Reward per stepIPD\nShaper\nOpponent\n0 50 100 150 2001.0\n0.5\n0.00.51.0IMP\nShaper\nOpponent\n0 50 100 150 2001.01.52.02.53.0ICG\nShaper\nOpponent\n0 50 100 150 200\nEpoch0.000.250.500.751.00State VisitationCC\nCD\nDC\nDD\nI\n0 50 100 150 200\nEpoch0.000.250.500.751."}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "150 200\nEpoch0.000.250.500.751.00State VisitationCC\nCD\nDC\nDD\nI\n0 50 100 150 200\nEpoch0.000.250.500.751.00\nHH\nHT\nTH\nTT\n0 50 100 150 200\nEpoch0.000.250.500.751.00\nSS\nSG\nGS\nGG\nFigure 6: Average reward per step (top row) and state visitation (bottom row) during training for the\nshaping experiments with the table-format prompt across the IPD, IMP, and ICG. In the state visita-\ntion figures, the outcome \u201cI\u201d encompasses all transitions where either player chosea null. Results are\nreported along with a 95% confidence interval over 5 random seeds.\nThe results demonstrate successful shaping across all three games. In the IPD, the shaper achieves\n3.5 while its opponent receives 0.53, exceeding mutual cooperation payoffs but underperforming\ncompared to the main text results. Since Section 5.2 shows consistent outcomes across different op-\nponent initializations, this gap likely stems from the shaper\u2019s own initialization rather than opponent\neffects. The IMP shows a similar pattern with substantial initialization differences, with the shaper\n20\nachieving 0.94 versus the opponent\u2019s -0.94. For the ICG, where initialization more closely matches\nthe main setup, results are 2.99 for the shaper and 1.01 for the opponent.\nFor the second prompt variation, we reverse the order in which actions are presented (see Figure 7).\nAs with the table-format experiments, the base prompts for IMP and ICG are identical except for\ntheir respective action labels and payoff matrices. We use the same action labels as in the previous\nvariation.\nSwitched-order base promptfor the IPD withw a1=C, w a2=D\n<bos><start ofturn>user\nYou are playing a 2-player game with actions: D, C. Points are assigned as\nfollows: D/D: 1/1, D/C: 4/0, C/D: 0/4, C/C: 3/3.\nChoose an action for the current round. Reply only with C or D.\n<end ofturn>\n<start ofturn>model"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "/C: 4/0, C/D: 0/4, C/C: 3/3.\nChoose an action for the current round. Reply only with C or D.\n<end ofturn>\n<start ofturn>model\nFigure 7: Switched-order prompt variation for the IPD. The actions and payoffs are presented in\nreversed order compared to the main text prompt.\nFigure 8 shows the training dynamics for the switched-order prompt experiments. As with the table-\nformat variation, both agents exhibit substantially different initializations compared to the main\nexperiments. Here, both players are heavily biased toward playing actiona 1(cooperate in the IPD,\nheads in the IMP, and swerving in the ICG), with their policies initially being almost deterministic\n(average of 99% of (a 1, a1) state at trial initiation). This creates challenges for the IPD and ICG,\nwhere this joint outcome yields acceptable rewards for both players. Consequently, without suffi-\ncient exploration incentives, the agents\u2019 policies remain unchanged for some of the seeds, focusing\nexclusively on the value prediction problem.\n0 50 100 150 20001234Average Reward per stepIPD\nShaper\nOpponent\n0 50 100 150 2001\n01IMP\nShaper\nOpponent\n0 50 100 150 2001.01.52.02.53.0ICG\nShaper\nOpponent\n0 50 100 150 200\nEpoch0.000.250.500.751.00State VisitationCC\nCD\nDC\nDD\nI\n0 50 100 150 200\nEpoch0.000.250.500.751.00\nHH\nHT\nTH\nTT\nI\n0 50 100 150 200\nEpoch0.000.250.500.751.00\nSS\nSG\nGS\nGG\nI\nFigure 8: Average reward per step (top row) and state visitation (bottom row) during training for the\nshaping experiments with the switched-order prompt across the IPD, IMP, and ICG. In the state vis-\nitation figures, the outcome \u201cI\u201d encompasses all transitions where either player chosea null. Results\nare reported along with a 95% confidence interval over 5 random seeds.\nTo address this issue,"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "and ICG. In the state vis-\nitation figures, the outcome \u201cI\u201d encompasses all transitions where either player chosea null. Results\nare reported along with a 95% confidence interval over 5 random seeds.\nTo address this issue, we introduce an entropy regularization term to the PPO loss function. While\nthis term was included in the original PPO formulation (Schulman et al., 2017), it is not implemented\nin theTRLpackage. Since our agents\u2019 action space encompasses the entire vocabulary, maximiz-\ning their output distribution entropy would create conflicting incentives: illegal actions would incur\n21\npenalties while simultaneously reducing the loss through increased entropy. Instead, we extract\nlogits only for the two allowed action tokens and compute the entropy of the resulting normalized\ndistribution. To prevent convergence to suboptimal policies, we employ a decaying entropy coeffi-\ncient that gradually reduces the exploration incentive during training.\nThe switched-order prompt results are presented in Table 11. With entropy regularization applied\nto both the IPD and ICG, we again observe successful shaping with results closely matching those\nin the main text: 3.99 for IPD and 2.77 for ICG. In the ICG, while the shaper achieves an average\nreward of 3 across most runs, for one of the seeds it consistently fails to explore alternative actions,\nconverging to the outcome where both players swerve. Lastly, the IMP performs similarly to other\nprompt formulations (0.94 for the shaper vs -0.94 for its opponent) without requiring an entropy\nregularization term.\nThese experiments demonstrate that ShapeLLM maintains robust shaping capabilities across differ-\nent prompt formulations. Even when initial policies are nearly deterministic (as in the switched-\norder variation), introducing entropy regularization to encourage early exploration enables effective\nopponent shaping. For reproducibility, we list the hyperparameters modified in this appendix in\nTable 12. The rest of the hyperparameters used are specified in Table 9.\nTable 12: Shaper\u2019s learning rate (lr), value function coefficient (c VF), clipping range (\u03f5 p), and entropy\nregularization parameters (cinit\nS,cend\nS,TS) for the experiments in Appendix A.5.\nExperimentlrc VF \u03f5p cinit\nScend\nSTS\nTable-format promptIPD1.41\u00d710\u2212710\u2212310\u22125\u2013 \u2013"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "S,cend\nS,TS) for the experiments in Appendix A.5.\nExperimentlrc VF \u03f5p cinit\nScend\nSTS\nTable-format promptIPD1.41\u00d710\u2212710\u2212310\u22125\u2013 \u2013 \u2013\nIMP2.41\u00d710\u2212710\u221232\u00d710\u22121\u2013 \u2013 \u2013\nICG1.41\u00d710\u2212710\u221232\u00d710\u22121\u2013 \u2013 \u2013\nSwitched-order promptIPD6.41\u00d710\u2212710\u221232\u00d710\u221210.1 0.0 25\nIMP6.41\u00d710\u2212610\u221232\u00d710\u22121\u2013 \u2013 \u2013\nICG1.41\u00d710\u2212710\u221232\u00d710\u221210.7 0.0 25\nA.6 ACTIONLABELSELECTION FORROBUSTNESSEXPERIMENTS\nLLM agent output distributions vary significantly with the choice of action labels (w a1, wa2). For\nthe robustness experiments, we systematically selected action labels to achieve target initial output\ndistributions for the opponents.\nTable 13: Action labels that produce the closest output distributions to the target initial distributions\nfor the IPD, IMP and ICG.\np0\nNL(a1)\u223c0.75 p0\nNL(a1)\u223c0.50 p0\nNL(a1)\u223c0.25\nwa1 wa2 wa1 wa2 wa1 wa2\nIPD H K N Y I X\nIMP S Y N Y N M\nICG T K N M T F\nTarget Distributions. For each game, we want to obtain opponents with initial probabilities of\nplaying actiona 19of approximately 0.75, 0.5, and 0.25.\nSelection Procedure. Opponents encounter 5 different prompts during training10. We extract the\nLLM\u2019s output probabilities for these 5 prompts across all possible combinations of capital letters\n9The actiona 1corresponds to cooperation in the IPD, playing heads in the IMP, and swerving in the chicken\ngame.\n10One stateless prompt at training initiation, and 4 prompts corresponding to the 4 possible joint actions from\nthe previous round (e.g., in the IPD: CC, CD, DC, DD).\n22\nas action labels (325 total combinations). For each combination, we calculate the KL divergence\nbetween the target and"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "4 possible joint actions from\nthe previous round (e.g., in the IPD: CC, CD, DC, DD).\n22\nas action labels (325 total combinations). For each combination, we calculate the KL divergence\nbetween the target and extracted output distributions, averaged across the 5 prompts, and select the\ncombination with the lowest divergence. The resulting action labels are shown in Table 13.\nThe extracted initial probabilities for each pair of action labels in Table 13 are shown in Table 14,\nwith separate results for the IPD (a), IMP (b), and ICG (c).\nTable 14: Naive learner\u2019s initial action probabilities across three games (IPD, IMP, ICG) under\nvarying action labels for the 5 distinct prompts encountered during training.\n(a) Initial cooperation probability in the IPD.\nAction Labelsp0\nNL(C)p0\nNL(C|CC)p0\nNL(C|CD)p0\nNL(C|DC)p0\nNL(C|DD)\nwa1=H, w a2=K 0.60 0.89 0.89 0.70 0.68\nwa1=N, w a2=Y 0.68 0.38 0.87 0.58 0.77\nwa1=I, w a2=X 0.12 0.21 0.75 0.34 0.24\n(b) Initial probability of playing heads in the IMP.\nAction Labelsp0\nNL(H)p0\nNL(H|HH)p0\nNL(H|HT)p0\nNL(H|TH)p0\nNL(H|TT)\nwa1=S, w a2=Y 0.71 0.78 0.87 0.57 0.70\nwa1=N, w a2=Y 0.48 0.32 0.60 0.56 0.66\nwa1=N, w a2=M 0.30 0.20 0.36 0.13 0.18\n(c) Initial swerving probability in the ICG.\nAction Labelsp0\nNL(S)p0\nNL(S|SS)p0\nNL(S|SG)p0\nNL(S|GS)p0\nNL(S|GG)\nwa1=T, w a2=K 0.61 0.90 0.87"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": ")p0\nNL(S|SS)p0\nNL(S|SG)p0\nNL(S|GS)p0\nNL(S|GG)\nwa1=T, w a2=K 0.61 0.90 0.87 0.82 0.82\nwa1=N, w a2=M 0.41 0.40 0.71 0.37 0.61\nwa1=T, w a2=F 0.24 0.47 0.47 0.02 0.11\nA.7 SHAPER EVALUATION FOR VARYING GAME LENGTHS FOR THEIPD, IMP,ANDICG\nTable 15 presents the evaluation results for the shapers trained in Section 5.1 for varying game\nlengths (T= 50,T= 100). The results demonstrate that shaper performance remains consistent\nacross different game lengths, with no significant degradation in exploitation capability as episodes\nbecome longer.\nTable 15: Post-training evaluation results for the IPD, IMP, and ICG comparing baseline (two naive\nlearners) versus shaper-naive learner pairs for varying game lengths (T= 20,50,100). Average\nrewards per step are reported with 95% confidence intervals across 5 random seeds. Illegal actions\nare excluded from the analysis (comprising 2% of actions in IPD, 0.1% in IMP, and 1% in ICG).\nIPD IMP ICG\nShaper Opponent Shaper Opponent Shaper Opponent\nT= 20 3.96\u00b10.01 0.10\u00b10.04 0.99\u00b10.01\u22120.99\u00b10.01 2.98\u00b10.01 1.01\u00b10.01\nT= 50 3.97\u00b10.01 0.09\u00b10.04 0.99\u00b10.01\u22120.99\u00b10.01 2.99\u00b10.01 1.01\u00b10.01\nT= 100 3.97\u00b10.01 0.09\u00b10.04 0.99\u00b10.01\u22120.99\u00b10.01 2.99\u00b10.00 1.01\u00b10.01\nA.8 TRAININGDYNAMICS\nIn this"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": ".09\u00b10.04 0.99\u00b10.01\u22120.99\u00b10.01 2.99\u00b10.00 1.01\u00b10.01\nA.8 TRAININGDYNAMICS\nIn this section we present the average reward per step and state visitation throughout training for\nvarious experiments discussed in the main text. These figures complement the evaluation results by\nshowing how agent behaviors evolved during the learning process. Figure 9 presents the training dy-\nnamics for the baseline experiments across the IPD, IMP and ICG (Section 5.1). Figure 10 shows the\n23\nbaseline training dynamics for the cooperative shaping experiments (Section 6). Finally, Figures 11\nand 12 present the training dynamics for shaper experiments against three different opponent types\nacross the IPD, IMP, and ICG, with evaluation results reported in Section 5.2.\n0 50 100 150 200123Average Reward per stepIPD\nPlayer 1\nPlayer 2\n0 50 100 150 2000.5\n0.00.5IMP\nPlayer 1\nPlayer 2\n0 50 100 150 2001.52.02.5ICG\nPlayer 1\nPlayer 2\n0 50 100 150 200\nEpoch0.000.250.500.751.00State VisitationCC\nCD\nDC\nDD\nI\n0 50 100 150 200\nEpoch0.00.20.40.60.8 HH\nHT\nTH\nTT\nI\n0 50 100 150 200\nEpoch0.000.250.500.75 SS\nSG\nGS\nGG\nI\nFigure 9: Average reward per step (top row) and state visitation (bottom row) during training for the\nbaseline experiments across the IPD, IMP and ICG reported in Section 5.1. In the state visitation\nfigures, the outcome \u201cI\u201d encompasses all transitions where either player chosea null. The results are\nreported along with a 95% confidence interval over 5 random seeds, except for the ICG experiment,\nfor which we use 10 seeds.\n0 50 100 150 2001234Average Reward per stepC-IPD\nPlayer 1\nPlayer 2\n0 50"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "5 random seeds, except for the ICG experiment,\nfor which we use 10 seeds.\n0 50 100 150 2001234Average Reward per stepC-IPD\nPlayer 1\nPlayer 2\n0 50 100 150 200123ISH\nPlayer 1\nPlayer 2\n0 50 100 150 200\nEpoch0.000.250.500.751.00State VisitationCC\nCD\nDC\nDD\nI\n0 50 100 150 200\nEpoch0.000.250.500.751.00\nSS\nSH\nHS\nHH\nI\nFigure 10: Average reward per step (top row) and state visitation (bottom row) during training for\nthe baseline experiments across the C-IPD and ISH reported in Section 6. In the state visitation\nfigures, the outcome \u201cI\u201d encompasses all transitions where either player chosea null. The results are\nreported along with a 95% confidence interval over 10 random seeds.\n24\n0 50 100 150 20001234p0\nNL(a1)0.75\nIPD\nShaper\nOpponent\n0 50 100 150 2001.0\n0.5\n0.00.51.0IMP\nShaper\nOpponent\n0 50 100 150 2001.01.52.02.53.0ICG\nShaper\nOpponent\n0 50 100 150 20001234p0\nNL(a1)0.50\nShaper\nOpponent\n0 50 100 150 2001.0\n0.5\n0.00.51.0\nShaper\nOpponent\n0 50 100 150 2001.01.52.02.53.0\nShaper\nOpponent\n0 50 100 150 200\nEpochs01234p0\nNL(a1)0.25\nShaper\nOpponent\n0 50 100 150 200\nEpochs1.0\n0.5\n0.00.51.0\nShaper\nOpponent\n0 50 100 150 200\nEpochs0.51.01.52.02.53.0\nShaper\nOpponentFigure 11: Average"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "0.00.51.0\nShaper\nOpponent\n0 50 100 150 200\nEpochs0.51.01.52.02.53.0\nShaper\nOpponentFigure 11: Average reward per step during training for the shaping experiments across the IPD, IMP,\nand ICG with different opponent initializations (Section 5.2). Each row corresponds to an opponent\nwith a different initial probability of generating actiona 1(\u201cCooperate\u201d in the IPD, \u201cHeads\u201d in the\nIMP, and \u201cSwerve\u201d in the ICG). The results are reported along with a 95% confidence interval over\n5 random seeds.\n25\n0 50 100 150 2000.00.20.40.60.81.0p0\nNL(a1)0.75\nIPD\nCC\nCD\nDC\nDD\nI\n0 50 100 150 2000.00.20.40.60.8IMP\nHH\nHT\nTH\nTT\nI\n0 50 100 150 2000.00.20.40.60.81.0ICG\nSS\nSG\nGS\nGG\nI\n0 50 100 150 2000.00.20.40.60.81.0p0\nNL(a1)0.50\nCC\nCD\nDC\nDD\nI\n0 50 100 150 2000.00.20.40.60.81.0\nHH\nHT\nTH\nTT\nI\n0 50 100 150 2000.00.20.40.60.81.0\nSS\nSG\nGS\nGG\nI\n0 50 100 150 200\nEpoch0.00.20.40.60.81.0p0\nNL(a1)0.25\nCC\nCD\nDC\nDD\nI\n0 50 100 150 200\nEpoch0.00.20.40.60.8HH\nHT\nTH\nTT\nI\n0 50 100 150 200\nEpoch0.00.20.40.60.81.0\nSS\nSG\nGS\nGG\nIFigure 12: State visitation during training for the shaping experiments across the IPD, IMP"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "100 150 200\nEpoch0.00.20.40.60.81.0\nSS\nSG\nGS\nGG\nIFigure 12: State visitation during training for the shaping experiments across the IPD, IMP, and ICG\nwith different opponent initializations (Section 5.2). Each row corresponds to an opponent with a\ndifferent initial probability of generating actiona 1(\u201cCooperate\u201d in the IPD, \u201cHeads\u201d in the IMP,\nand \u201cSwerve\u201d in the ICG). For all games, the outcome \u201cI\u201d encompasses all transitions where either\nplayer chosea null. The results are reported along with a 95% confidence interval over 5 random\nseeds.\n26\nA.9 TRAININGPROMPTS\nWe use three types of prompts throughout training: the base prompt, the state-only prompt, and the\nstate occurrence prompt.\nThebase prompt(Figure 13) includes a game description specifying the number of players, available\nactions, and payoff structure. It also contains an instruction to select an action using the designated\naction labelsw a1, wa2.\nThestate-only prompt(Figure 14) includes all information from the base prompt and additionally\nincorporates the last joint action of both players. Naive learners receive this prompt for all game\nrounds except the first, where they receive the base prompt. Shapers receive this prompt only at the\nsecond round of each trial, when one previous action has been played.\nLastly, thestate occurrence prompt(Figure 15) includes all information from the state-only prompt\nand additionally incorporates the state counts for all previous rounds within the current trial. Shaping\nagents receive this prompt for all interactions beyond the first two rounds. The prompt is dynamically\nupdated as follows: when new actions are played, the state count is updated to include the most\nrecent state, and the new joint action is displayed in the current state field.\nBase promptfor the IPD withw a1=C, w a2=D\n<bos><start ofturn>user\nYou are playing a 2-player game with actions: C, D. Points are assigned as\nfollows: C/C: 3/3, C/D: 0/4, D/C: 4/0, D/D: 1/1.\nChoose an action for the current round. Reply only with C or D"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "follows: C/C: 3/3, C/D: 0/4, D/C: 4/0, D/D: 1/1.\nChoose an action for the current round. Reply only with C or D.\n<end ofturn>\n<start ofturn>model\nBase promptfor the IMP withw a1=H, w a2=T\n<bos><start ofturn>user\nYou are playing a 2-player game with actions: H, T. Points are assigned as\nfollows: H/H: 1/-1, H/T: -1/1, T/H: -1/1, T/T: 1/-1.\nChoose an action for the current round. Reply only with H or T.\n<end ofturn>\n<start ofturn>model\nBase promptfor the ICG withw a1=S, w a2=G\n<bos><start ofturn>user\nYou are playing a 2-player game with actions: S, G. Points are assigned as\nfollows: S/S: 2/2, S/G: 1/3, G/S: 3/1, G/G: -5/-5.\nChoose an action for the current round. Reply only with S or G.\n<end ofturn>\n<start ofturn>model\nBase promptfor the ISH withw a1=S, w a2=H\n<bos><start ofturn>user\nYou are playing a 2-player game with actions: S, H. Points are assigned as\nfollows: S/S: 4/4, S/H: 0/3, H/S: 3/0, H/H: 1/1.\nChoose an action for the current round. Reply only with S or H.\n<end ofturn>\n<start ofturn>model\nFigure 13: Base prompts for the IPD, IMP, ICG and ISH. The structure remains the same across\ngames, with the only differences being the action labels and reward matrices.\n27\nExample ofstate-only promptfor the IPD withw a1=C, w a2=D\n<bos><start ofturn>user\nYou are playing a 2-player game with actions: C, D. Points are assigned as\nfollows: C/C: 3/3, C/D: 0/4, D/C"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "<bos><start ofturn>user\nYou are playing a 2-player game with actions: C, D. Points are assigned as\nfollows: C/C: 3/3, C/D: 0/4, D/C: 4/0, D/D: 1/1.\n<STATE>In the previous round, you played C and your opponent played C.\nChoose an action for the current round. Reply only with C or D.\n<end ofturn>\n<start ofturn>model\nExample ofstate-only promptfor the IMP withw a1=H, w a2=T\n<bos><start ofturn>user\nYou are playing a 2-player game with actions: H, T. Points are assigned as\nfollows: H/H: 1/-1, H/T: -1/1, T/H: -1/1, T/T: 1/-1.\n<STATE>In the previous round, you played H and your opponent played H.\nChoose an action for the current round. Reply only with H or T.\n<end ofturn>\n<start ofturn>model\nExample ofstate-only promptfor the ICG withw a1=S, w a2=G\n<bos><start ofturn>user\nYou are playing a 2-player game with actions: S, G. Points are assigned as\nfollows: S/S: 2/2, S/G: 1/3, G/S: 3/1, G/G: -5/-5.\n<STATE>In the previous round, you played S and your opponent played S.\nChoose an action for the current round. Reply only with S or G.\n<end ofturn>\n<start ofturn>model\nExample ofstate-only promptfor the ISH withw a1=S, w a2=G\n<bos><start ofturn>user\nYou are playing a 2-player game with actions: S, H. Points are assigned as\nfollows: S/S: 4/4, S/H: 0/3, H/S: 3/0, H/H: 1/1.\n<STATE>In the previous round, you played S and your opponent played S.\nChoose an action for the current round. Reply only with S or H.\n<end ofturn>\n<start ofturn>model\nFigure 14: Example state-only prompts for the IPD"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": "round, you played S and your opponent played S.\nChoose an action for the current round. Reply only with S or H.\n<end ofturn>\n<start ofturn>model\nFigure 14: Example state-only prompts for the IPD, IMP, ICG and ISH. The structure remains the\nsame across games, with the only differences being the action labels and reward matrices. The\nspecific examples shown are for rounds in which the previous joint action is(a 1, a1).\n28\nExample ofstate occurrence promptfor the IPD withw a1=C, w a2=D\n<bos><start ofturn>user\nYou are playing a 2-player game with actions: C, D. Points are assigned as\nfollows: C/C: 3/3, C/D: 0/4, D/C: 4/0, D/D: 1/1.\n<ADDITIONAL INFORMATION>The occurrence of each state in the current game has\nbeen CC:0, CD:0, DC:0, DD:5.\n<STATE>In the previous round, you played C and your opponent played C.\nChoose an action for the current round. Reply only with C or D.\n<end ofturn>\n<start ofturn>model\nExample ofstate occurrence promptfor the IMP withw a1=H, w a2=T\n<bos><start ofturn>user\nYou are playing a 2-player game with actions: H, T. Points are assigned as\nfollows: H/H: 1/-1, H/T: -1/1, T/H: -1/1, T/T: 1/-1.\n<ADDITIONAL INFORMATION>The occurrence of each state in the current game has\nbeen HH:0, HT:0, TH:0, TT:5.\n<STATE>In the previous round, you played H and your opponent played H.\nChoose an action for the current round. Reply only with H or T.\n<end ofturn>\n<start ofturn>model\nExample ofstate occurrence promptfor the ICG withw a1=S, w a2=G\n<bos><start ofturn>user\nYou are playing a 2-player game with actions: S, G. Points are assigned as\nfollows: S/S: 2/2, S/G: 1/3, G/S: 3/1,"}
{"file": "2510.08255v1.pdf", "section": "8 CONCLUSION", "chunk": ">user\nYou are playing a 2-player game with actions: S, G. Points are assigned as\nfollows: S/S: 2/2, S/G: 1/3, G/S: 3/1, G/G: -5/-5.\n<ADDITIONAL INFORMATION>The occurrence of each state in the current game has\nbeen SS:0, SG:0, GS:0, GG:5.\n<STATE>In the previous round, you played S and your opponent played S.\nChoose an action for the current round. Reply only with S or G.\n<end ofturn>\n<start ofturn>model\nExample ofstate occurrence promptfor the ISH withw a1=S, w a2=H\n<bos><start ofturn>user\nYou are playing a 2-player game with actions: S, H. Points are assigned as\nfollows: S/S: 4/4, S/H: 0/3, H/S: 3/0, H/H: 1/1.\n<ADDITIONAL INFORMATION>The occurrence of each state in the current game has\nbeen SS:0, SH:0, HS:0, HH:5.\n<STATE>In the previous round, you played S and your opponent played S.\nChoose an action for the current round. Reply only with S or H.\n<end ofturn>\n<start ofturn>model\nFigure 15: Example state-occurrence prompts for the IPD, IMP, ICG and ISH. The structure remains\nthe same across games, with the only differences being the action labels and reward matrices. The\nspecific examples shown are for rounds in which all previous joint actions within the game are\n(a2, a2), except the last one, which is(a 1, a1).\n29"}
{"file": "2510.08383v1.pdf", "section": "Introduction", "chunk": "QAgent: A modular Search Agent with Interactive Query Understanding\nYi Jiang*\u2021, Lei Shen*, Lujie Niu, Sendong Zhao\u2020, Wenbo Su, Bo Zheng\nTaobao & Tmall Group of Alibaba\njiangyijcx@163.com, zhaosendong@gmail.com\nAbstract\nLarge language models (LLMs) excel at\nnatural language tasks but are limited by\ntheir static parametric knowledge, especially\nin knowledge-intensive task. Retrieval-\naugmented generation (RAG) mitigates this by\nintegrating external information. However, (1)\ntraditional RAG struggles with complex query\nunderstanding, and (2) even search agents\ntrained with reinforcement learning (RL), de-\nspite their promise, still face generalization and\ndeployment challenges. To address these limi-\ntations, we propose QAgent, a unified agentic\nRAG framework that employs a search agent\nfor adaptive retrieval. This agent optimizes its\nunderstanding of the query through interactive\nreasoning and retrieval. To facilitate real-world\napplication, we focus on modular search agent\nfor query understanding that are plug-and-play\nin complex systems. Secifically, the agent fol-\nlows a multi-step decision process trained with\nRL to maximize retrieval quality and support\naccurate downstream answers. We further an-\nalyze the strengths and weaknesses of end-to-\nend RL and propose a strategy that focuses on\neffective retrieval, thereby enhancing general-\nization in LLM applications. Experiments show\nQAgent excels at QA and serves as a plug-and-\nplay module for real-world deployment. The\ncode is available1.\n1 Introduction\nLarge language models (LLMs) have demonstrated\noutstanding performance across a wide range of\nnatural language processing tasks(Jaech et al.,\n2024; Guo et al., 2025). However, for complex,\nknowledge-intensive tasks, LLMs face significant\nchallenges, including outdated knowledge and hal-\nlucinations(B\u00e9chard and Ayala, 2024; Gade et al.,\n*Equal contribution.\n\u2020Corresponding Author.\n\u2021Work done during an internship at Alibaba\n1https://github.com/OpenStellarTeam/QAgent.git .\nFigure 1: Different methods applied to the system.\n2025), which undermine their accuracy and reliabil-\nity. Retrieval-augmented generation (RAG) enables\nLLMs"}
{"file": "2510.08383v1.pdf", "section": "1 Introduction", "chunk": "https://github.com/OpenStellarTeam/QAgent.git .\nFigure 1: Different methods applied to the system.\n2025), which undermine their accuracy and reliabil-\nity. Retrieval-augmented generation (RAG) enables\nLLMs to access external knowledge by retrieving\ndocuments and using them as context for genera-\ntion, demonstrating great potential in addressing\nthese limitations(Lewis et al., 2020; Zhao et al.,\n2024a; Fan et al., 2024; Li et al., 2025b).\nNevertheless, the rigid workflow of traditional\nRAGs makes them inefficient for real-world prob-\nlems requiring complex, multi-step reasoning. On\nthe one hand, limitations exist in their ability to\ndetermine when and what to retrieve, making it\ndifficult to construct accurate search paths for com-\nplex problems. On the other hand, RAGs are of-\nten embedded within complex systems, and the\nstandalone RAG modules cannot adapt or optimize\nthemselves according to the broader system\u2019s needs.\nTo alleviate these limitations, recent work has pro-\nposed various solutions, including query optimiza-\ntion(Ma et al., 2023), planning(Jiang et al., 2025c),\nreflection(Asai et al., 2024), active retrieval(Jeong\net al., 2024), and iterative retrieval(Trivedi et al.,\n2023). While these approaches have increased flex-\nibility and reduced supervision costs, they still fail\nto effectively leverage feedback for continuous it-\neration and performance optimization. DeepSeek-\nR1(Guo et al., 2025) demonstrates that even rule-\nbased, outcome-driven rewards can train powerful\nreasoning agents. This has led to the emergence of\nsearch agent trained with RL(Jin et al., 2025), en-\nabling autonomous reasoning, on-demand retrieval,\n1arXiv:2510.08383v1  [cs.AI]  9 Oct 2025\nFigure 2: The proposed QAgent framework, with the left representing the system and the right representing our\nagent design, including \u201cplan-search-information-reflect\u201d. In addition, the illustration shows a two-stage training\nframework, with the first stage using end-to-end RL and the second stage introducing generalized training. During\ntraining, the searched content tokens are masked to exclude them from loss calculations.\nand iterative query optimization.\nIn addition, practical"}
{"file": "2510.08383v1.pdf", "section": "1 Introduction", "chunk": "a two-stage training\nframework, with the first stage using end-to-end RL and the second stage introducing generalized training. During\ntraining, the searched content tokens are masked to exclude them from loss calculations.\nand iterative query optimization.\nIn addition, practical deployment of search\nagents remains challenging. As shown in Figure 1,\non the one hand, due to query complexity, the in-\nformation directly retrieved from the original query\nmay not be useful; on the other hand, due to the\ncomplexity of the system, the search agent is of-\nten used as a submodule, aiming to improve its\ninformation retrieval capability rather than its in-\nformation utilization capabilities.\nFrom this, we identify two key goals for practical\nsearch agents: (1) understand and decompose com-\nplex queries to bridge the gap between the original\nquery and the retriever; and (2) retrieve information\nthat benefits the downstream generation, ensuring\ngeneralizability when deployed as a submodule.\nTo address these challenges, we propose a uni-\nfied framework that integrates query understanding,\nreasoning, and iterative refinement into a coherent\nsearch process. The framework enables adaptive\nquery decomposition through multiple rounds of\ninteraction, where the model progressively refines\nits understanding of complex user intents by gen-\nerating, executing, and reflecting on retrieval ac-\ntions. At the core of the framework is a lightweight\nsearch agent capable of autonomous reasoning and\ndecision-making. By formulating search as a se-\nquential decision-making problem, the agent is\ntrained end-to-end using reinforcement learning\nto maximize the quality of information returned to\nthe downstream generation system. This allows the\nagent to learn not only what to retrieve, but also\nhow to re-optimize in a context-aware manner, ul-\ntimately acting as a submodule to improve overall\nsystem performance.Our contributions are summarized as follows:\n\u2022We propose a unified Agentic framework, cen-\ntered around query understanding. Through\nmultiple rounds of interaction, QAgent unifies\nquery optimization, and continues to evolve\nthrough outcome reward feedback.\n\u2022We analyze the strengths and weaknesses of\nend-to-end reinforcement learning training\nand propose a two-stage training solution, pro-\nviding profound insights for the practical ap-\nplication of search agents.\n\u2022We conduct extensive experiments demon-\nstrate that QAgent delivers strong perfor-\nmance. And further analysis confirms its\nmeaningful gains in information composition.\n2 Related Work\n2."}
{"file": "2510.08383v1.pdf", "section": "1 Introduction", "chunk": "profound insights for the practical ap-\nplication of search agents.\n\u2022We conduct extensive experiments demon-\nstrate that QAgent delivers strong perfor-\nmance. And further analysis confirms its\nmeaningful gains in information composition.\n2 Related Work\n2.1 Retrieval-Augmented Generation\nRetrieval-augmented generation (RAG) has\nemerged as a promising approach to mitigate\nlimitations of the LLMs, such as hallucina-\ntion(B\u00e9chard and Ayala, 2024; Gade et al., 2025),\nby augmenting them with external retrieved\nknowledge(Lewis et al., 2020; Zhao et al., 2024a;\nFan et al., 2024; Li et al., 2025b; Zhou et al.,\n2024a). Classic RAG systems typically follow a\n\u201cretrieve-then-read\u201d paradigm(Lewis et al., 2020),\nbut it still faces numerous challenges that hinder\noptimal performance. To handle the complexity\nof problem, recent work identifies retrieval intent\nto skip unnecessary retrieval (Jeong et al., 2024;\nCheng et al., 2024; Wang et al., 2023b) and refines\n2\nqueries to better match retriever expectations\n(Ma et al., 2023; Wang et al., 2023a; Xu et al.,\n2024b). To counter noisy or irrelevant passages,\nmethods introduce denoising(Wei et al., 2025),\nre-ranking(Glass et al., 2022; Xiao et al., 2024),\nand compression(Xu et al., 2024a; Jiang et al.,\n2023) between retrieval and generation. To bridge\nthe retriever-generator gap, some jointly optimize\nboth modules (Izacard et al., 2023; Lin et al., 2023)\nor insert trainable middlewares (Jiang et al., 2025b;\nKe et al., 2024; Dong et al., 2025). Recent progress\nhas increasingly moved towards agentic RAGs:\nSelf-RAG(Asai et al., 2024) combines on-demand\nretrieval with reflection, MetaRAG(Zhou et al.,\n2024b) iteratively acquires knowledge through\nmetacognition, and Search-o1(Li et al., 2025a)\nperforms retrieval while reasoning. However,\ncurrent"}
{"file": "2510.08383v1.pdf", "section": "2.1 Retrieval-Augmented Generation", "chunk": "val with reflection, MetaRAG(Zhou et al.,\n2024b) iteratively acquires knowledge through\nmetacognition, and Search-o1(Li et al., 2025a)\nperforms retrieval while reasoning. However,\ncurrent RAG systems exhibit limited agentic\ncapabilities and cannot evolve continuously, due to\nthe lack of effective optimization methods.\n2.2 Search Agent with RL\nReinforcement learning (RL)(Kaelbling et al.,\n1996) has emerged as a promising paradigm for\nimproving the reasoning capabilities of large lan-\nguage models (LLMs). By interacting with the\nenvironment and driven by the goal of maximizing\nrewards, models are able to achieve autonomous\nreasoning and decision-making, such as OpenAI-\no1(Jaech et al., 2024) and DeepSeek-R1(Guo et al.,\n2025). Recent research has further integrated RL\ninto agent-based RAGs, enabling search agents\nto continuously learn and evolve through real-\ntime interactions with search engines. For exam-\nple, Search-R1(Jin et al., 2025), R1-Searcher(Song\net al., 2025), Deepresearcher(Zheng et al., 2025)\nand ZeroSearch(Sun et al., 2025) utilize RL to\ntrain models for simultaneous search and reason-\ning, eliminating the need for supervision over inter-\nmediate reasoning steps. Subsequently, many im-\nprovements were made in reasoning patterns(Ren\net al., 2025), reward verification(Wang et al., 2025;\nHe et al., 2025) and efficiency(Jiang et al., 2025a;\nSha et al., 2025). While these approaches have\nachieved impressive performance, their practical\napplicability remains unclear due to limitations in\nefficiency and generalization. In contrast, our work\nfocuses on query understanding and analyzes train-\ning strategies that promote generalization, provid-\ning a solution for integrating search agents into\ncomplex systems in real-world applications.3 Methodology\nIn this section, we introduce our agentic framework\nshown in Figure 2. It has two parts: the inference\nloop and the training strategy. The inference loop\nintroduces our multi-round rollout for search agent.\nThe training strategy first analyzes the RL training\nand then introduces a two-stage training.\n3.1 Prel"}
{"file": "2510.08383v1.pdf", "section": "2.2 Search Agent with RL", "chunk": "It has two parts: the inference\nloop and the training strategy. The inference loop\nintroduces our multi-round rollout for search agent.\nThe training strategy first analyzes the RL training\nand then introduces a two-stage training.\n3.1 Preliminaries\n3.1.1 Agentic RAG\nNaive RAG follows a retrieve-read paradigm. Let\nqbe a query and Da document corpus. A retrieval\nfunction (including a retriever and a corpus) returns\nkrelevant documents, denoted R(q) . A generator\nGproduces a answer \u02c6Aconditioned on qandR(q) :\n\u02c6A=G(q,R(q)).(1)\nIn the agentic setting, search is modeled as a se-\nquential process governed by policy \u03c0\u03b8(at|s<t),\nwhere s<tincludes q, past actions, retrieved doc-\numents, and reasoning history. The agent selects\nactionsa t\u2208 A:\nat\u223c\u03c0\u03b8(\u00b7 |s <t),(2)\ntypically reasoning or retrieval. The interaction\nyields a trajectory \u03c4= (s 0, a0, . . . , s T, aT), with a\nfinal answer, framing RAG as a multi-step decision\nprocess for dynamic reasoning and retrieval.\n3.1.2 Reinforcement Learning\nReinforcement learning (RL) is used to train the\npolicy \u03c0\u03b8(y|x) by maximizing task-specific re-\nwards r\u03d5(x, y) . We adopt Group Relative Policy\nOptimization (GRPO)(Shao et al., 2024), which\noperates on Grollouts {yi}G\ni=1to compute group-\nrelative advantages. The objective is:\nJ(\u03b8) =E x\u223cD,{y i}\u0002 1PG\ni=1|yi|GX\ni=1|yi|X\nt=1min\u0000\n\u03c1i,tAi,clip(\u03c1 i,t,1\u2212\u03f5,1 +\u03f5)A i\u0001\n\u2212\u03b2D KL\u0003\n,\n(3)\nwhere \u03c1i,t=\u03c0 \u03b8(yi,t|x, y i,<t)/\u03c0\u03b8old(yi,t|x, y i,<t),\nandAi= (R i\u2212\u00b5R)/\u03c3Ris the normalized ad-\nvantage with \u00b5Rand\u03c3Rthe mean and standard\ndeviation of {Rj}G"}
{"file": "2510.08383v1.pdf", "section": "3.1.2 Reinforcement Learning", "chunk": "\u03b8old(yi,t|x, y i,<t),\nandAi= (R i\u2212\u00b5R)/\u03c3Ris the normalized ad-\nvantage with \u00b5Rand\u03c3Rthe mean and standard\ndeviation of {Rj}G\nj=1. The KL term DKL=\nDKL(\u03c0\u03b8\u2225\u03c0\u03b8ref)constrains policy deviation. Our\ntraining is based on this algorithm.\n3\n3.2 Multi-Turn Query Optimation Loop\nMotivation.One main goal of the search agent is\nto retrieve useful information. When the retriever\nand corpus are fixed, the way to achieve it is to op-\ntimize the query. Understanding the original query\nis complex(Zhao et al., 2024b), including but not\nlimited to: the query requires multi-hop reasoning\n(Fig. 3(a)), needs to be decomposed (Fig. 3(b)), and\ndoes not conform to the retrieval system\u2019s prefer-\nences. Fully modeling this yields a wide and deep\ntree (Fig. 3(c)), which demands numerous model\ncalls and retries, resulting in high complexity.\nFigure 3: Illustration of query understanding.\nWorkflow.To address these challenges, inspired\nby recent advances in multi-round agentic RAG, we\nmodel the expansion of query understanding as a\nstochastic sequential decision process (Figure 3(d)).\nInstead of enforcing rigid transformation rules, our\nframework enables the agent to autonomously de-\ncide how to refine and issue queries over multiple\ninteractions with a retrieval system.\nFormally, given an initial query q, and condi-\ntioned on a fixed retriever Rand retrieval corpus\nD, the agent generates a sequence of optimized\nqueries S={S 1, . . . , S T}overTrounds, where\neachSt={q t,1, . . . , q t,mt}is a set of transformed\nqueries produced at round t. The full trajectory\nprobability is factorized (see Figure 2):\nP(S|q,R,D) =TY\nt=1P\u0000\nSt|q,(Ipre\ni, Si,Ci, Ipost\ni)i<t\u0001\n(4)\nwhere (\u00b7)i<tdonates historical actions before round\nt, and Ipre\ni,Ipost\ni encode pre-retrieval and post-\nretrieval reasoning traces (e.g.,"}
{"file": "2510.08383v1.pdf", "section": "3.2 Multi-Turn Query Optimation Loop", "chunk": "i)i<t\u0001\n(4)\nwhere (\u00b7)i<tdonates historical actions before round\nt, and Ipre\ni,Ipost\ni encode pre-retrieval and post-\nretrieval reasoning traces (e.g., plan and reflection).\nTheCidonate the retrieval context, which aggre-\ngates results from all transformed queries:\nCi=\u2295mi\nj=1R(qij), q ij\u2208Si,(5)\nwhere R(\u00b7) fetches relevant documents from D,\nand\u2295represents context aggregation.Overall, the agent follows a structured multi-\nround interaction loop, formalized as a trajectory:\n\u03c4= (q, Ipre\n1, S1,C1, Ipost\n1, . . . ,C T, Ipost\nT,\u02c6A).(6)\nAt each round t, the agent performs planning Ipre\nt,\ngenerates optimized queries St. Followed by re-\nflection Ipost\nt, in which the agent assesses the com-\npleteness of the accumulated information.\nBy unifying multiple query transformation\nparadigms within a stochastic, reflective interaction\nloop, our framework supports richer and more flex-\nible query understanding than methods limited to\nfixed decomposition patterns, enabling the model\ntoask better questions.\n3.3 End-to-End RL Training\nFor optimizing search agent, reinforcement learn-\ning is a powerful tool. It enables continuous interac-\ntion with the environment and uses outcome-based\nrewards for end-to-end optimization, thereby en-\nhancing the LLM\u2019s agentic capability.\nReward Design.In the end-to-end RL training,\nour rewards are mainly designed based on two fac-\ntors: the correctness of the answer and the correct-\nness of the format. Formally,\nR(\u03c4) =I{r fmt(\u03c4) = 1} \u00b7EM s(A\u2217,\u02c6A).(7)\nEMsstands for Strict EM, the same as Search-R1.\nThe reward format supports pre-retrieval planning,\npost-retrieval reflection, and final answer tags. This\nstrict, sparse design may waste rollouts in early\ntraining, but ensures rollout quality and gradually\nimproves efficiency as training stabilizes.\n3.4 Generalized RL Training\nMotivation.End-to-end training significantly im-\nproves the agent\u2019s capabilities, both in information\nretrieval and utilization. This"}
{"file": "2510.08383v1.pdf", "section": "3.3 End-to-End RL Training", "chunk": "ensures rollout quality and gradually\nimproves efficiency as training stabilizes.\n3.4 Generalized RL Training\nMotivation.End-to-end training significantly im-\nproves the agent\u2019s capabilities, both in information\nretrieval and utilization. This training optimizes\nboth retrieval and generation. However, due to\nthe high latency of current autoregressive LLMs\nand the complexity of real-world systems, a sin-\ngle monolithic model is often insufficient to fully\nhandle all tasks. We envision a search agent as\na component that prioritizes information retrieval,\nwhile information utilization is not necessarily im-\nportant. In this context, query optimization and\ninformation retrieval are our primary goals, and\nimproving information utilization during training\ncan become a form of reward hacking.\n4\nFigure 4: A case study of Agentic RL training (Search-\nR1), where both the fast ascent phase and the slow con-\nvergence phase are tested.\nAnalysis.We analyze a case study of the training\nprocess of an agent using reinforcement learning.\nAs shown in Figure 4, in the early stages of model\ntraining, information retrieval and information uti-\nlization mutually enhance each other. However,\nlater in training, the model tends to prioritize in-\nformation utilization to obtain rewards. This un-\ndoubtedly compromises the generalization ability\nof the search agent as a submodule. An analysis of\ninformation utilization can be seen in \u00a7 4.5.\nGeneralized as Submodule.To address this, we\nintroduced the second training stage. This involves\nusing a frozen generator to answer based on the\nagent\u2019s retrieved documents, and then calculating\nrewards for responses from the frozen generator,\nthereby improving the agent\u2019s generalization as a\ntool. Formally,\nK=parseDocSet(\u03c4),(8)\n\u02dcA=G(q,K)(9)\nwhere Kis the retrieved passage set and \u02dcAis the\nfrozen generator\u2019s predicted answer.\nReward Design.In this stage, constraints are\nshifted to the correctness of the generator\u2019s re-\nsponse, with format and mandatory answering re-\nquirements relaxed.\nR(\u03c4) = EM(A\u2217,\u02dcA) + 0.5\u2217Hit(\u03c4, A\u2217)(10)\nEMis relaxed to non-strict EM because using strict\nEM on a frozen generator is inefficient, unlike a\ngenerator trained with a format reward. In addition"}
{"file": "2510.08383v1.pdf", "section": "3.4 Generalized RL Training", "chunk": ") + 0.5\u2217Hit(\u03c4, A\u2217)(10)\nEMis relaxed to non-strict EM because using strict\nEM on a frozen generator is inefficient, unlike a\ngenerator trained with a format reward. In addition,\nthe reward for the model\u2019s end-to-end answer will\nbe relaxed to Hit, meaning that the entire trajectory\ncontains the gold answer.Algorithm 1Rollout with Multi-Turn Interaction\nRequire: Input query q, policy model \u03c0\u03b8, search\nengineR, maximum turnsB, GeneratorG.\nEnsure:Final predicted answer \u02c6A.\n1:Initialization: rollout sequence y\u2190 \u2205 , action\ncountb\u21900, info setK \u2190 \u2205\n2:whileb < Bdo\n3:y b\u2190 \u2205\u25b7Initialize current sequence\n4:repeat\n5:y t\u223c\u03c0\u03b8(\u00b7 |q, y+y b),yb\u2190y b\u2295yt\n6:until</search>, </answer>, <eos>iny b\n7:y\u2190y+y b\n8:ifSearchTagsdetected iny bthen\n9:Q\u2190ParseQ(S,<query>, </query>)\n10:C= [R(q 1),\u00b7\u00b7\u00b7,R(q n)], qi\u2208Q\n11:y\u2190y\u2295 <information >C\n</information>\n12:K \u2190 K \u222aC \u25b7Update info set\n13:else ifAnswerTagsiny bthen\n14:break\n15:else\n16:Ask for retry:y\u2190y\u2295prompt rethink\n17:end if\n18:Increment action countb\u2190b+ 1\n19:end while\n20:\u02dcA=G(q,K)\u25b7Generate final answer\n21:returnfinal predicted answer \u02dcA\n3.5 Inference\nFor each query, QAgent starts with task instruc-\ntions and reasons step by step. When it hits\n<search>...</search>tags, it extracts the query, re-\ntrieves documents, and appends them wrapped in\n<information>...</information>tags. This loop re-\npeats, building a reasoning chain with external evi-\ndence. Finally, the passage set is fed to the genera-\ntor to produce the answer (see Algorithm 1).\n4 Experiment\nThis section presents the experimental results, and\nanalysis. We main address the following questions:\nRQ1:"}
{"file": "2510.08383v1.pdf", "section": "3.5 Inference", "chunk": "dence. Finally, the passage set is fed to the genera-\ntor to produce the answer (see Algorithm 1).\n4 Experiment\nThis section presents the experimental results, and\nanalysis. We main address the following questions:\nRQ1: How does QAgent perform in end-to-end\nQA?(\u00a7 4.2) RQ2: How well does QAgent function\nas a submodule?(\u00a7 4.2) RQ3: How does RL train-\ning affect information retrieval or usage?(\u00a7 4.2 and\n\u00a7 4.5) RQ4: What advantages does the QAgent\nparadigm offer over traditional RAG?(\u00a7 4.4)\n5\nMethodHotpotQA 2WikiMHQ\u2020MuSique\u2020NaturalQA WebQ\u2020Average\nEM F1 EM F1 EM F1 EM F1 EM F1 EM F1\nSearch-o1 23.20 27.04 22.40 22.74 5.80 10.44 23.63 23.80 30.02 26.99 21.01 22.20\nSearch-R1 35.00 38.88 35.40 31.86 7.60 14.0130.89 32.37 35.6835.72 28.91 30.57\nZeroSearch 29.40 28.46 27.80 22.03 3.00 8.00 17.95 10.62 26.48 23.94 20.93 18.61\nQAgent37.60 44.44 38.20 36.11 7.8012.39 31.47 35.1932.09 38.01 29.43 33.23\nTable 1: Main results of end-to-end performance. Best and second-best results are highlighted inboldand\nunderlined , respectively.\u2020denotes out-of-distribution evaluation datasets.\nMethodHotpotQA 2WikiMHQ\u2020MuSique\u2020NaturalQA WebQ\u2020Average\nEM F1 EM F1 EM F1 EM F1 EM F1 EM F1\nGenerator 3B\nVanilla 15.00 19.33 23.00 25.01 0.80 6.40 17.53"}
{"file": "2510.08383v1.pdf", "section": "4 Experiment", "chunk": "F1 EM F1 EM F1 EM F1 EM F1\nGenerator 3B\nVanilla 15.00 19.33 23.00 25.01 0.80 6.40 17.53 19.58 28.44 29.38 16.95 19.94\nNaiveRAG 30.60 35.54 25.40 28.32 3.20 7.64 30.22 31.43 29.97 30.26 23.88 26.64\nRerank 34.40 39.87 28.40 30.95 3.80 9.4935.35 35.2032.97 31.92 26.98 29.48\nSearch-o1 21.20 25.14 24.60 26.02 2.20 7.33 24.07 24.39 32.08 30.66 20.83 22.71\nSearch-R1 31.80 36.71 29.40 30.83 5.20 10.14 28.39 28.86 28.39 28.86 24.64 27.08\nzero-search 26.40 30.46 24.80 26.07 2.00 4.98 19.31 20.40 23.13 24.74 21.17 24.86\nQAgent37.00 42.94 33.80 35.22 5.20 10.2033.82 33.68 36.32 32.78 29.23 30.96\nGenerator 7B\nVanilla 20.20 25.54 23.40 27.00 4.80 10.77 22.80 25.72 32.73 35.87 20.79 24.98\nNaiveRAG 36.00 42.47 28.00 31.24 4.40 10.78 33.30 35.21 31.55 33.52 26.65 30.64\nRerank 39.00 44.76 28"}
{"file": "2510.08383v1.pdf", "section": "4 Experiment", "chunk": "31.24 4.40 10.78 33.30 35.21 31.55 33.52 26.65 30.64\nRerank 39.00 44.76 28.40 32.11 5.40 11.8438.28 39.0633.66 34.75 28.95 32.50\nSearch-o1 27.00 31.15 24.40 27.78 6.00 11.77 28.42 30.06 33.91 35.40 23.95 27.23\nSearch-R1 36.00 40.63 34.40 33.41 7.00 12.35 31.27 32.09 32.07 33.94 28.15 30.48\nZeroSearch 28.40 34.44 24.00 27.10 3.00 7.47 23.21 25.00 27.26 30.27 21.17 24.86\nQAgent42.40 46.75 35.80 36.76 7.40 13.8137.37 38.87 36.71 36.41 31.94 34.52\nTable 2: Main results when used as a submodule. Best and second-best results are highlighted inboldand underlined ,\nrespectively.\u2020denotes out-of-distribution evaluation datasets.\n4.1 Experimental Setup\nDatasets.We evaluate QAgent on five open-\ndomain QA datasets covering both multi-hop\nand single-hop reasoning. The multi-hop\nQA benchmarks include2WikiMultiHopQA(Ho\net al., 2020),HotpotQA(Yang et al., 2018) and\nMusique(Trivedi et al., 2022). For general QA,\nwe useWebQuestions(Berant et al., 2013) and\nNaturalQA(Kwiatkowski et al., 2019). We adopt\n500-sample subsets of 2WikiMultiHopQA and\nHotpotQA for efficiency, following Trivedi et al.\n(2023). More details are provided in Appendix A.\nEvaluation.We report two metrics: EM and F1.\nFollowing prior work(Asai et al.,"}
{"file": "2510.08383v1.pdf", "section": "4.1 Experimental Setup", "chunk": "MultiHopQA and\nHotpotQA for efficiency, following Trivedi et al.\n(2023). More details are provided in Appendix A.\nEvaluation.We report two metrics: EM and F1.\nFollowing prior work(Asai et al., 2024), we use\na non-strict EM where a prediction is correct if it\ncontains the gold answer. F1 measures token-level\noverlap between the predicted and gold answers.\nSince longer response may improve EM via cover-\nage but introduce noise that lowers F1, evaluating\nboth metrics allows for a more balanced evaluation.Baselines.We compare QAgent with different\nbaselines: (1) Vanilla and Naive RAG. Vanilla: di-\nrect answering without retrieval and Naive RAG:\nanswering in the \u201cretrieval-reading\u201d paradigm. (2)\nSearch-Agent. We utilize Search-o1, which uses\nmultiple rounds of interaction but without RL train-\ning, and Search-R1 and zero-search, which use\nmultiple rounds of interaction and joint retrieval\nand generation with RL training. (3) As the search\nagent component. We extract the retrieved doc-\numents from the model inference trajectory and\ninput them into the frozen generators: Search-\no1, Search-R1, zero-search, and the middleware\nRerank. Details are in Appendix B\nImplementation Details.We use Qwen-2.5-3B\n(Instruct)(Yang et al., 2025) as the training model\nand use frozen as the second-stage supervision\nmodel. For generation, we use Qwen-2.5-3B (In-\nstruct) and 7B as generators. For retrieval, we use\nthe 2018 Wikipedia dump as the knowledge source,\n6\nMethodHotpotQA 2WikiMHQ\u2020MuSique\u2020NQ WebQ\u2020Average\nEM F1 EM F1 EM F1 EM F1 EM F1 EM F1\nGenerator 3B\nQAgent w/o all 31.20 37.03 29.20 30.81 4.40 9.67 28.95 29.63 30.36 29.87 24.82 27.40\n+stage1 36.20 41.21 30.60 33.195.40 10.2430.03 30.04 30.91 30."}
{"file": "2510.08383v1.pdf", "section": "4.1 Experimental Setup", "chunk": ".87 24.82 27.40\n+stage1 36.20 41.21 30.60 33.195.40 10.2430.03 30.04 30.91 30.20 26.63 28.98\n+stage1+stage237.00 42.94 33.80 35.225.20 10.2033.82 33.68 36.32 32.78 29.23 30.96\nGenerator 7B\nQAgent w/o all 36.00 41.84 29.80 32.64 5.40 11.32 31.75 33.63 31.79 33.51 26.95 30.59\n+stage1 40.40 45.84 35.20 37.65 7.40 13.60 33.52 34.93 32.58 33.83 29.82 33.17\n+stage1+stage242.40 46.75 35.80 36.76 7.40 13.81 37.37 38.87 36.71 36.41 31.94 34.52\nTable 3: Ablation Study. Best results are highlighted inbold.\u2020denotes out-of-distribution evaluation datasets.\nand all baselines use bm25(Robertson et al., 2009)\nas the retriever. For fairness, we retrieve 1 passage\neach query and deduplicate before sending to the\ngenerator. And the reported Search-R1 is a strictly\naligned reproduction based on our framework.\n4.2 Main Results\nWe present comprehensive evaluation results in\nTable 1 and Table 2, respectively. From the results,\nwe have the following observations:\nEnd-to-End QA performance.Overall, the RL-\ntrained methods outperform the training-free meth-\nods, demonstrating the potential of RL training (the\nZerosearch evaluation was not as good as expected,\nwhich we analyzed may be due to the sensitivity\nof the search engine). In an end-to-end QA com-\nparison, our training achieved an improvement of\n0.52% and 2.66% in EM and F1 respectively com-\npared to Search-R1. Furthermore, we observed that"}
{"file": "2510.08383v1.pdf", "section": "4.2 Main Results", "chunk": "search engine). In an end-to-end QA com-\nparison, our training achieved an improvement of\n0.52% and 2.66% in EM and F1 respectively com-\npared to Search-R1. Furthermore, we observed that\non multi-hop datasets like Musique, our approach\ndid not achieve significant improvement compared\nto Search-R1. We speculate that this may be due\nto our adaptive query optimization pattern. While\nit expands both the depth and breadth of query un-\nderstanding, the inevitable expansion of breadth\nslightly compromises the depth.\nPerformance as Submodule.Table 2 shows the\ngeneralization of the search agent as a submodule\nin complex systems. We found that when freez-\ning 3B as the generator, the overall performance\nof Search-R1 was significantly lower than that of\nend-to-end QA, indicating that its generalization\nwas somewhat insufficient. In contrast, our method\nmaintains strong generalization. Specifically, our\napproach improves average EM by 5.35% over\nNaiveRAG. It also outperforms end-to-end opti-\nmized search-R1 by 4.59% in average EM. And we\nfound that the middleware rerank still achieves rela-tively good performance, but it is a non-conflicting\nsubmodule with QAgent, which means they can\nbe used simultaneously. Moreover, integrating our\nQAgent (3B) into a system with a 7B generator\nyields even greater gains, suggesting that complex\nsystems can rely on a strong generator equipped\nwith small search agent as submodule.\n4.3 Ablation Study\nIn this section, we conduct detailed ablation ex-\nperiments to verify the effectiveness of our train-\ning at each stage. As shown in the figure, our\ntrain-free version achieves high performance, with\na significant improvement after end-to-end train-\ning. This benefited from the reinforcement learning\nalgorithm, but end-to-end training still has short-\ncomings. We found that it significantly improved\nthe performance in in-distribution evaluation, but\nthe improvement was limited in out-of-distribution\nevaluation. After introducing a second stage of\ntraining, the performance was further improved,\nespecially due to its improved generalization.\nFigure 5: Illustration of the combined gain. The left is\nthe experimental results of HotpotQA, and the right is\nthat of 2WikiMH"}
{"file": "2510.08383v1.pdf", "section": "4.3 Ablation Study", "chunk": "training, the performance was further improved,\nespecially due to its improved generalization.\nFigure 5: Illustration of the combined gain. The left is\nthe experimental results of HotpotQA, and the right is\nthat of 2WikiMHQ.\n4.4 Combined Gain Analysis\nIn this section, we analyzed the advantages of using\nthe search agent as a submodule. Specifically, we\n7\nFigure 6: Information uti-\nlization ability\nFigure 7: Different re-\ntrievers\nincreased the top-k threshold under Naive RAG to\nexplore the upper limits of the \u201cretrieval-reading\u201d\nparadigm. As shown in Figure 5, due to the com-\nplexity of the problem, even with an increased top-k\nthreshold, the gains gradually diminished. This is\nlikely due to the redundancy of candidate document\ninformation and the limited amount of combinato-\nrial information, which is an unavoidable limita-\ntion of all retrievers. The search agent, acting as\nan middleware between the query and the retriever,\naddresses the limitations of the retriever. Using\nthe same token budget as the generator, it achieves\neven greater \u201ccombination gains\u201d, even exceeding\nthe upper limits of the traditional paradigm.\nFurthermore, we verified that using only the first\nfew rounds significantly outperformed NaiveRAG,\ndemonstrating its effectiveness. We also increased\nthe retrieved top-k per query and found that per-\nformance continued to improve, but the number of\ndocuments used also increased significantly.\nOverall, QAgent increases the upper limit of\nthe model\u2019s performance when handling complex\nproblems. However, in practical applications, a\ntrade-off between the number of documents used\nand efficiency is necessary.\n4.5 Information Utilization Analysis\nAs analyzed in Section 3.4, as the model train-\ning, its information utilization capability improves,\nand this is a obstacle to the model\u2019s information\nretrieval capability. Here, we analyze the two-\nstage trained models\u2019 ability to utilize the same\ninformation. Specifically, we perform naive RAG\non the trained models, give them the same top-1\ndocument, and then test the performance of the\nresponses. We found that the end-to-end trained\nmodel achieved the highest performance. This is\nbecause information utilization capability is the\ndirect goal of training. After generalization train-\ning, the model\u2019s information"}
{"file": "2510.08383v1.pdf", "section": "4.5 Information Utilization Analysis", "chunk": "then test the performance of the\nresponses. We found that the end-to-end trained\nmodel achieved the highest performance. This is\nbecause information utilization capability is the\ndirect goal of training. After generalization train-\ning, the model\u2019s information utilization capability\ndecreased. We speculate that this is because theoptimization goal at this time is no longer focused\non information utilization, but gradually focuses\non information retrieval capability. This provides\nnew insights into the training of search agents.\n4.6 Generalization to Different Retrievers\nIn this section, we analyzed the generalization of\nQAgent. As shown in Figure 7, we used two rep-\nresentative search engines, BM25 and E5. As ex-\npected, using the more powerful search engine, E5,\nyielded better performance. Specifically, the strong\nretriever enables QAgent to achieve improvements\non HotpotQA similar to Naive RAG. However,\npowerful search engine yielded a greater perfor-\nmance improvement on 2WikiMHQ compared to\nNaive RAG. This suggests that the QAgent middle-\nware is plug-and-play, compatible with both search\nengines and generators, and exhibiting high gener-\nalization performance.\nFigure 8: The performance difference between using\ndifferent generators during training and inference.\n4.7 Different Model Sizes for Supervision\nUsing the generator\u2019s responses to indirectly calcu-\nlate the reward, rather than relying on end-to-end\nanswer, helps avoid reward hacking that may arise\nfrom over-optimizing information utilization. How-\never, the impact of different generators for super-\nvision requires further analysis. We experimented\nwith two generator sizes during training: 3B and\n7B. As shown in Figure 8, training with the 7B\ngenerator led to more stable, but 3B achieved better\noverall performance. We speculate that this results\nfrom the 3B model\u2019s inherent instability, which\nacts as a form of regularization. Specifically, if a\nsmaller generator can produce correct answers, it\nsuggests that the retrieved information is sufficient\nfor accurate reasoning.\nDuring inference, larger generators consistently\nyield better performance. This indicates that com-\nplex systems benefit from more powerful genera-\n8\ntors, and a lightweight search agent can be effec-\ntively integrated to meet efficiency.\n5 Conclusion\nIn this work, we present QAgent, a unified agentic"}
{"file": "2510.08383v1.pdf", "section": "4.7 Different Model Sizes for Supervision", "chunk": "This indicates that com-\nplex systems benefit from more powerful genera-\n8\ntors, and a lightweight search agent can be effec-\ntively integrated to meet efficiency.\n5 Conclusion\nIn this work, we present QAgent, a unified agentic\nRAG framework that tackles two key challenges:\nweak complex query understanding and limited\ngeneralization of search agents. QAgent employs a\nmodular, plug and play agent that optimizes queries\nthrough iterative reasoning and retrieval, trained\nwith RL to maximize retrieval quality. Experiments\nshow QAgent outperforms existing methods on\ncomplex QA tasks.\n6 Limitation\nWhile QAgent achieves strong performance, sev-\neral limitations remain.\nTraining on larger models.Our motivation is to\nserve as a submodule of the system. Within this\ngoal, validation on smaller models is paramount.\nHowever, whether larger models achieve consistent\nresults remains to be verified.\nFailure to control passage diversity.We tried\nvarious approaches to passage diversity, including\ndesigning repeated negative rewards and adjusting\npassage overlap, but found that these all led to\nsevere reward hacking, especially when the number\nof passages is uncertain. Diversity and number are\nstrongly correlated, and the model may overreact\nby increasing the number of irrelevant passages.\nRobustness challenges.The retriever is a factor\nthat influences training and inference, affecting the\nagent\u2019s learning patterns, particularly re-retrieval.\nFurthermore, intuitively, query understanding and\nsearch engine preferences are relatively consistent.\nFor example, sparse retriever tend to favor key-\nwords. Although we have verified that QAgent\nachieves robust performance on different search\nengines, it remains a challenge to directly achieve\na query optimization goal that satisfies the prefer-\nences of all search engines.\nReferences\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2024. Self-RAG: Learning to\nretrieve, generate, and critique through self-reflection.\nInThe Twelfth International Conference on Learning\nRepresentations.Patrice B\u00e9chard and Orlando Marquez Ayala. 2024.\nReducing hallucination in structured outputs via\nretrieval-augmented generation.arXiv preprint\narXiv:2404.08189.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing"}
{"file": "2510.08383v1.pdf", "section": "6 Limitation", "chunk": "outputs via\nretrieval-augmented generation.arXiv preprint\narXiv:2404.08189.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. InProceedings of the 2013\nconference on empirical methods in natural language\nprocessing, pages 1533\u20131544.\nQinyuan Cheng, Xiaonan Li, Shimin Li, Qin Zhu,\nZhangyue Yin, Yunfan Shao, Linyang Li, Tianxi-\nang Sun, Hang Yan, and Xipeng Qiu. 2024. Unified\nactive retrieval for retrieval augmented generation.\nInFindings of the Association for Computational\nLinguistics: EMNLP 2024, pages 17153\u201317166.\nGuanting Dong, Yutao Zhu, Chenghao Zhang, Zechen\nWang, Ji-Rong Wen, and Zhicheng Dou. 2025. Un-\nderstand what llm needs: Dual preference alignment\nfor retrieval-augmented generation. InProceedings\nof the ACM on Web Conference 2025, pages 4206\u2013\n4225.\nWenqi Fan, Yujuan Ding, Liangbo Ning, Shijie Wang,\nHengyun Li, Dawei Yin, Tat-Seng Chua, and Qing\nLi. 2024. A survey on rag meeting llms: Towards\nretrieval-augmented large language models. InPro-\nceedings of the 30th ACM SIGKDD conference on\nknowledge discovery and data mining, pages 6491\u2013\n6501.\nAnoushka Gade, Jorjeta G Jetcheva, and Hardi Trivedi.\n2025. It\u2019s about time: Incorporating temporality in\nretrieval augmented language models. In2025 IEEE\nConference on Artificial Intelligence (CAI), pages\n75\u201382. IEEE.\nMichael Glass, Gaetano Rossiello, Md Faisal Mahbub\nChowdhury, Ankita Naik, Pengshan Cai, and Alfio\nGliozzo. 2022. Re2g: Retrieve, rerank, generate. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language"}
{"file": "2510.08383v1.pdf", "section": "6 Limitation", "chunk": "Gliozzo. 2022. Re2g: Retrieve, rerank, generate. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2701\u20132715.\nDaya Guo, Dejian Yang, Haowei Zhang, Junxiao\nSong, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shi-\nrong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025.\nDeepseek-r1: Incentivizing reasoning capability in\nllms via reinforcement learning.arXiv preprint\narXiv:2501.12948.\nJie He, V\u00edctor Guti\u00e9rrez Basulto, and Jeff Z Pan. 2025.\nFrom sufficiency to reflection: Reinforcement-guided\nthinking quality in retrieval-augmented reasoning for\nllms.arXiv preprint arXiv:2507.22716.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing a multi-hop\nqa dataset for comprehensive evaluation of reasoning\nsteps. InProceedings of the 28th International Con-\nference on Computational Linguistics, pages 6609\u2013\n6625.\n9\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2023. Atlas: Few-shot learning with retrieval\naugmented language models.Journal of Machine\nLearning Research, 24(251):1\u201343.\nAaron Jaech, Adam Kalai, Adam Lerer, Adam Richard-\nson, Ahmed El-Kishky, Aiden Low, Alec Helyar,\nAleksander Madry, Alex Beutel, Alex Carney, and 1\nothers. 2024. Openai o1 system card.arXiv preprint\narXiv:2412.16720.\nSoyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju\nHwang, and Jong C Park. 2024. Adaptive-rag: Learn-\ning to adapt retrieval-augmented"}
{"file": "2510.08383v1.pdf", "section": "6 Limitation", "chunk": "2.16720.\nSoyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju\nHwang, and Jong C Park. 2024. Adaptive-rag: Learn-\ning to adapt retrieval-augmented large language mod-\nels through question complexity. InProceedings of\nthe 2024 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies (Volume 1: Long Pa-\npers), pages 7029\u20137043.\nHuiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng\nLi, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023.\nLongllmlingua: Accelerating and enhancing llms\nin long context scenarios via prompt compression.\narXiv preprint arXiv:2310.06839.\nPengcheng Jiang, Xueqiang Xu, Jiacheng Lin, Jinfeng\nXiao, Zifeng Wang, Jimeng Sun, and Jiawei Han.\n2025a. s3: You don\u2019t need that much data to train a\nsearch agent via rl.arXiv preprint arXiv:2505.14146.\nYi Jiang, Sendong Zhao, Jianbo Li, Haochun Wang, and\nBing Qin. 2025b. GainRAG: Preference alignment in\nretrieval-augmented generation through gain signal\nsynthesis. InProceedings of the 63rd Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 10746\u201310757. Asso-\nciation for Computational Linguistics.\nZhouyu Jiang, Mengshu Sun, Lei Liang, and Zhiqiang\nZhang. 2025c. Retrieve, summarize, plan: Advanc-\ning multi-hop question answering with an iterative\napproach. InCompanion Proceedings of the ACM on\nWeb Conference 2025, pages 1677\u20131686.\nBowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon,\nSercan Arik, Dong Wang, Hamed Zamani, and Jiawei\nHan. 2025. Search-r1: Training llms to reason and\nleverage search engines with reinforcement learning.\narXiv preprint arXiv:2503"}
{"file": "2510.08383v1.pdf", "section": "6 Limitation", "chunk": "rik, Dong Wang, Hamed Zamani, and Jiawei\nHan. 2025. Search-r1: Training llms to reason and\nleverage search engines with reinforcement learning.\narXiv preprint arXiv:2503.09516.\nLeslie Pack Kaelbling, Michael L Littman, and An-\ndrew W Moore. 1996. Reinforcement learning: A\nsurvey.Journal of artificial intelligence research,\n4:237\u2013285.\nVladimir Karpukhin, Barlas Oguz, Sewon Min,\nPatrick SH Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. 2020. Dense passage re-\ntrieval for open-domain question answering. In\nEMNLP (1), pages 6769\u20136781.Zixuan Ke, Weize Kong, Cheng Li, Mingyang Zhang,\nQiaozhu Mei, and Michael Bendersky. 2024. Bridg-\ning the preference gap between retrievers and llms.\nInProceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 10438\u201310451.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, and 1 others. 2019. Natural questions: a\nbenchmark for question answering research.Trans-\nactions of the Association for Computational Linguis-\ntics, 7:453\u2013466.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rock-\nt\u00e4schel, and 1 others. 2020. Retrieval-augmented gen-\neration for knowledge-intensive nlp tasks.Advances\nin neural information processing systems, 33:9459\u2013\n9474.\nXiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang,\nYujia Zhou, Yutao Zhu, Peitian Zhang, and\nZhicheng Dou. 2025a. Search-o1: Agentic search-\nenhanced large reasoning models.arX"}
{"file": "2510.08383v1.pdf", "section": "6 Limitation", "chunk": "iajie Jin, Yuyao Zhang,\nYujia Zhou, Yutao Zhu, Peitian Zhang, and\nZhicheng Dou. 2025a. Search-o1: Agentic search-\nenhanced large reasoning models.arXiv preprint\narXiv:2501.05366.\nXiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian\nZhang, Yutao Zhu, and Zhicheng Dou. 2025b. From\nmatching to generation: A survey on generative infor-\nmation retrieval.ACM Transactions on Information\nSystems, 43(3):1\u201362.\nXi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi,\nMaria Lomeli, Richard James, Pedro Rodriguez, Ja-\ncob Kahn, Gergely Szilvasy, Mike Lewis, and 1 oth-\ners. 2023. Ra-dit: Retrieval-augmented dual instruc-\ntion tuning. InThe Twelfth International Conference\non Learning Representations.\nXinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao,\nand Nan Duan. 2023. Query rewriting in retrieval-\naugmented large language models. InProceedings\nof the 2023 Conference on Empirical Methods in\nNatural Language Processing, pages 5303\u20135315.\nJingyi Ren, Yekun Xu, Xiaolong Wang, Weitao Li,\nWeizhi Ma, and Yang Liu. 2025. Effective and\ntransparent rag: Adaptive-reward reinforcement\nlearning for decision traceability.arXiv preprint\narXiv:2505.13258.\nStephen Robertson, Hugo Zaragoza, and 1 others. 2009.\nThe probabilistic relevance framework: Bm25 and\nbeyond.Foundations and Trends\u00ae in Information\nRetrieval, 3(4):333\u2013389.\nZeyang Sha, Shiwen Cui, and Weiqiang Wang.\n2025. Sem: Reinforcement learning for search-\nefficient large language models.arXiv preprint\narXiv:2505.07903.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,\nJunxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\n10\nZhang, YK"}
{"file": "2510.08383v1.pdf", "section": "6 Limitation", "chunk": "5.07903.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu,\nJunxiao Song, Xiao Bi, Haowei Zhang, Mingchuan\n10\nZhang, YK Li, Yang Wu, and 1 others. 2024.\nDeepseekmath: Pushing the limits of mathematical\nreasoning in open language models.arXiv preprint\narXiv:2402.03300.\nHuatong Song, Jinhao Jiang, Yingqian Min, Jie Chen,\nZhipeng Chen, Wayne Xin Zhao, Lei Fang, and Ji-\nRong Wen. 2025. R1-searcher: Incentivizing the\nsearch capability in llms via reinforcement learning.\narXiv preprint arXiv:2503.05592.\nHao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan\nHou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei\nHuang, and Jingren Zhou. 2025. Zerosearch: Incen-\ntivize the search capability of llms without searching.\narXiv preprint arXiv:2505.04588.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. Musique: Multi-\nhop questions via single-hop question composition.\nTransactions of the Association for Computational\nLinguistics, 10:539\u2013554.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2023. Interleaving retrieval\nwith chain-of-thought reasoning for knowledge-\nintensive multi-step questions. InProceedings of the\n61st Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n10014\u201310037.\nLiang Wang, Nan Yang, and Furu Wei. 2023a.\nQuery2doc: Query expansion with large language\nmodels. InProceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing,\npages 9414\u20139423.\nYile Wang, Peng Li, Maosong Sun, and Yang Liu.\n2023b. Self-knowledge guided retrieval augmenta-\ntion for large language"}
{"file": "2510.08383v1.pdf", "section": "6 Limitation", "chunk": "on\nEmpirical Methods in Natural Language Processing,\npages 9414\u20139423.\nYile Wang, Peng Li, Maosong Sun, and Yang Liu.\n2023b. Self-knowledge guided retrieval augmenta-\ntion for large language models. InFindings of the\nAssociation for Computational Linguistics: EMNLP\n2023, pages 10303\u201310315.\nZiliang Wang, Xuhui Zheng, Kang An, Cijun Ouyang,\nJialu Cai, Yuhang Wang, and Yichao Wu. 2025.\nStepsearch: Igniting llms search ability via step-\nwise proximal policy optimization.arXiv preprint\narXiv:2505.15107.\nZhepei Wei, Wei-Lin Chen, and Yu Meng. 2025. In-\nstructRAG: Instructing retrieval-augmented genera-\ntion via self-synthesized rationales. InThe Thirteenth\nInternational Conference on Learning Representa-\ntions.\nShitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muen-\nnighoff, Defu Lian, and Jian-Yun Nie. 2024. C-pack:\nPacked resources for general chinese embeddings. In\nProceedings of the 47th international ACM SIGIR\nconference on research and development in informa-\ntion retrieval, pages 641\u2013649.\nFangyuan Xu, Weijia Shi, and Eunsol Choi. 2024a. Re-\ncomp: Improving retrieval-augmented lms with com-\npression and selective augmentation. In12th Inter-\nnational Conference on Learning Representations,\nICLR 2024.Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng,\nand Tat-Seng Chua. 2024b. Search-in-the-chain: In-\nteractively enhancing large language models with\nsearch for knowledge-intensive tasks. InProceedings\nof the ACM Web Conference 2024, pages 1362\u20131373.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan\nHui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayi-\nheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian\nYang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Ji-\naxi Yang, Jingren Zhou, Junyang Lin,"}
{"file": "2510.08383v1.pdf", "section": "6 Limitation", "chunk": "Dayi-\nheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian\nYang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Ji-\naxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and\n23 others. 2025. Qwen2.5 technical report.Preprint,\narXiv:2412.15115.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D Manning. 2018. Hotpotqa: A dataset for\ndiverse, explainable multi-hop question answering.\nInProceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369\u20132380.\nPenghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren\nWang, Yunteng Geng, Fangcheng Fu, Ling Yang,\nWentao Zhang, Jie Jiang, and Bin Cui. 2024a.\nRetrieval-augmented generation for ai-generated con-\ntent: A survey.arXiv preprint arXiv:2402.19473.\nSiyun Zhao, Yuqing Yang, Zilong Wang, Zhiyuan He,\nLuna K Qiu, and Lili Qiu. 2024b. Retrieval aug-\nmented generation (rag) and beyond: A comprehen-\nsive survey on how to make your llms use external\ndata more wisely.arXiv preprint arXiv:2409.14924.\nYuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai,\nLyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025.\nDeepresearcher: Scaling deep research via reinforce-\nment learning in real-world environments.arXiv\npreprint arXiv:2504.03160.\nYujia Zhou, Yan Liu, Xiaoxi Li, Jiajie Jin, Hongjin Qian,\nZheng Liu, Chaozhuo Li, Zhicheng Dou, Tsung-Yi\nHo, and Philip S Yu. 2024a. Trustworthiness in\nretrieval-augmented generation systems: A survey.\narXiv preprint arXiv:2409.101"}
{"file": "2510.08383v1.pdf", "section": "6 Limitation", "chunk": "eng Dou, Tsung-Yi\nHo, and Philip S Yu. 2024a. Trustworthiness in\nretrieval-augmented generation systems: A survey.\narXiv preprint arXiv:2409.10102.\nYujia Zhou, Zheng Liu, Jiajie Jin, Jian-Yun Nie, and\nZhicheng Dou. 2024b. Metacognitive retrieval-\naugmented large language models. InProceedings of\nthe ACM Web Conference 2024, pages 1453\u20131463.\nA Dataset\nHere, we provide a detailed description of the\ndatasets we used.\nHotpotQA(Yang et al., 2018) and2WikiMul-\ntiHopQA(Ho et al., 2020): Both datasets are\nmulti-hop question answering datasets based on\nWikipedia. Given the cost constraints of the exper-\niment, we used a subsample of the Trivedi et al.\n(2023) dataset, which was obtained by extract-\ning 500 questions from the validation set of each\ndataset.\n11\nMusique(Trivedi et al., 2022): By iteratively\nselecting composable single-hop question pairs, we\ncreated questions spanning 2-4 hops, which are\ngenerally more difficult.\nWebQuestions(Berant et al., 2013): Con-\nstructed from questions posed by the Google Sug-\ngest API, where the answers are specific entities\nlisted in Freebase.\nNaturalQA(Kwiatkowski et al., 2019): A\ndataset designed to support comprehensive ques-\ntion answering systems. It contains questions from\nreal Google search queries, and the corresponding\nanswers are text snippets from Wikipedia articles,\ncarefully identified by human annotators.\nB baselines\nWe compare QAgent with different baselines:\nVanilla: Direct answering without retrieval.\nNaive RAG: Predicting an answer in the\n\u201cretrieval-then-reading\u201d paradigm.\nSearch-o1: Using multiple rounds of interaction\nbut without RL training.\nSearch-R1: Using multiple rounds of interaction\nand joint retrieval and generation model with RL\ntraining.\nzero-search: Training a LLM as retriever can\nreduce the cost and enhance the robustness to the\nretriever.\nRerank: Follows the \u201cretrieve-rearrange-\ngenerate\u201d paradigm and is the most widely used\nmiddleware (this does not"}
{"file": "2510.08383v1.pdf", "section": "6 Limitation", "chunk": "LLM as retriever can\nreduce the cost and enhance the robustness to the\nretriever.\nRerank: Follows the \u201cretrieve-rearrange-\ngenerate\u201d paradigm and is the most widely used\nmiddleware (this does not conflict with QAgent).\nWhen as the search agent component, we extract\nthe retrieved documents from the model inference\ntrajectory and input them into the frozen genera-\ntors.\nC Implementation Details\nWe experimented with Qwen-2.5-3B (In-\nstruct)(Yang et al., 2025). For retrieval, we used\nthe 2018 Wikipedia dump(Karpukhin et al., 2020)\nas the knowledge source and BM25(Robertson\net al., 2009) as the retriever. To ensure a fair com-\nparison, we set the number of retrieved passages to\n5 for all retrieval-based methods and 1 for iterative\nmethods. For training, we followed Search-R1,\nmerging the training sets of NQ(Kwiatkowski\net al., 2019) and HotpotQA(Yang et al., 2018) to\nform a unified dataset.\nFor GRPO training, we set the learning rate of\nthe policy LLM to 1e-6. The KL divergence regu-\nlarization coefficient \u03b2and the clipping rate \u03f5wereset to 0.001 and 0.2, respectively. We sampled 5 re-\nsponses for each prompt. The maximum sequence\nlength was set to 8192 tokens, the maximum re-\nsponse length was 512. To accelerate sampling,\nwe use vLLM2as the inference acceleration frame-\nwork. We set the rollout temperature and top-p\nvalue of our vLLM-based rollout to 1.0. A purely\nonline algorithm was used, meaning that the vLLM\nweights used for sampling were updated after each\nweight update. Training is performed on a single\nnode equipped with 8 GPUs, four of which are used\nfor running rollout, three for training, and one for\ndeploying the reference model. For training, we\nused Liger-Kernel3to optimize training efficiency.\nThe actual batch size for a single GPU is 1, the\ngradient accumulation is set to 64, and training is\nperformed until 1400 actual update steps, with a\nwarm-up step number of 225."}
{"file": "2510.08383v1.pdf", "section": "6 Limitation", "chunk": "training efficiency.\nThe actual batch size for a single GPU is 1, the\ngradient accumulation is set to 64, and training is\nperformed until 1400 actual update steps, with a\nwarm-up step number of 225. In addition, we took\na checkpoint of 1120 steps for stage 1 training, and\na checkpoint of 1400 steps for stage 2. During\nthe evaluation, we used the vllm framework and\nperformed inference on a single GPU.\nD Training\nFigure 9 shows the stability of the reward in-\ncrease. Compared with Search-R1, our method has\na higher starting point and a higher upper limit of\nconvergence. As shown in Figure 10, In the second\nstage of training, we used different frozen genera-\ntors for supervision, and we can see that the reward\nsteadily increased. Using 7B supervision is more\nstable and has higher rewards, because the larger\ngenerator has richer internal knowledge. However,\nfrom the experimental evaluation results, 3B has\nslightly better generalization.\nFigure 9: Illustration of the reward increasing during\nend-to-end training.\n2https://docs.vllm.ai/en/latest/\n3https://github.com/linkedin/Liger-Kernel\n12\nFigure 10: Illustration of the reward increasing during\nstage2 training. Note that the upper limits of the rewards\nis different.\nE Experimental Results\nWe present here the complete experimental results\nof Section 4.7, as shown in Table 4.\nF Template Prompt\nWe present here the instruction prompt in Table 5.\nG Case Study\nA case of QAgent is shown in Figure 11.\n13\nHotpotQA 2WikiMHQ\u2020MuSique\u2020NaturalQA WebQuestion\u2020AverageMethodEM F1 EM F1 EM F1 EM F1 EM F1 EM F1\nGenerator 3B\nEnd-to-End 36.20 41.21 30.60 33.19 5.40 10.24 30.03 30.04 30.91 30.20 26.63 28.98\nSupervision: 3B 37.00 42.94 33.80 35.22 5.20 10.20 33.82 33.68 36.32 32.78 29"}
{"file": "2510.08383v1.pdf", "section": "6 Limitation", "chunk": "Supervision: 3B 37.00 42.94 33.80 35.22 5.20 10.20 33.82 33.68 36.32 32.78 29.23 30.96\nSupervision: 7B 34.80 41.07 31.40 32.72 4.20 9.07 32.49 32.30 32.73 31.74 27.12 29.38\nGenerator 7B\nEnd-to-End 40.40 45.84 35.20 37.65 7.40 13.60 33.52 34.93 32.58 33.83 29.82 33.17\nSupervision: 3B 42.40 46.75 35.80 36.76 7.40 13.81 37.37 38.87 36.71 36.41 31.94 34.52\nSupervision: 7B 42.00 48.16 33.80 36.77 6.00 12.04 35.87 37.26 33.51 34.65 30.24 33.78\nTable 4: Full experimental results using different models as supervision.\nSearch for information to answer the given question. You can search as many times as needed if you\nfind you lack some knowledge. You will go through a loop of:\n\u201c<plan>xxx</plan>\n<search >xxx</search>\n<information>xxx</information>\n<reflection>xxx </reflection>\n<plan>xxx</plan>(if not complete)\n...\n<reflection>xxx </reflection>\n<answer>xxx</answer>\u201d.\nYou must conduct planning inside <plan>and </plan>first every time you call a search engine.\nAfter planing, you can call a search engine to search multiple queries (no more than 3) by\n<search>\n<query>query1</query>\n<query>query2</query>\n...\n<query>queryk</query>\n</search>,\nand it will return the searched results between <information>and </information>.\nAfter getting information, you must conduct a reflection on the information and place your reflection\nbetween the <reflection>and"}
{"file": "2510.08383v1.pdf", "section": "6 Limitation", "chunk": "...\n<query>queryk</query>\n</search>,\nand it will return the searched results between <information>and </information>.\nAfter getting information, you must conduct a reflection on the information and place your reflection\nbetween the <reflection>and </reflection>tags.\nNote that you must plan within <plan>and </plan>before searching, and reflect within <reflect>and\n</reflect>after receiving information.\nNote that each query must be enclosed between <query>and </query>, and all queries must\nbe placed between <search>and </search>, such as <search> \\n<query>query1</query> \\n\n<query>query2</query>\\n...\\n<query>queryk</query>\\n</search>.\nIf the task is not yet complete, begin a new <plan>.\nIf you find no further external knowledge needed, you can directly provide the answer inside <an-\nswer>and </answer>without detailed illustrations.\nFor example, <answer>\\nxxx\\n</answer>.\nThe answer, \u201cxxx\u201d, should be a few short words.\nQuestion: {question}.\nTable 5: Template prompt used in experiments.\n14\nFigure 11: A case of QAgent.\n15"}
{"file": "2510.08529v1.pdf", "section": "Introduction", "chunk": "CoMAS\nCOMAS: CO-EVOLVINGMULTI-AGENTSYSTEMS VIA\nINTERACTIONREWARDS\nXiangyuan Xue1,2Yifan Zhou3Guibin Zhang4Zaibin Zhang5,6Yijiang Li7\nChen Zhang2,8Zhenfei Yin6BPhilip Torr6Wanli Ouyang1,2Lei Bai2B\n1The Chinese University of Hong Kong2Shanghai Artificial Intelligence Laboratory\n3University of Georgia4National University of Singapore\n5Dalian University of Technology6University of Oxford\n7University of California San Diego8University of Science and Technology of China\nABSTRACT\nSelf-evolution is a central research topic in enabling large language model (LLM)-\nbased agents to continually improve their capabilities after pretraining. Recent\nresearch has witnessed a transition from reinforcement learning (RL)-free to RL-\nbased methods. Current RL-based methods either rely on dense external reward\nsignals or extract intrinsic reward signals from LLMs themselves. However, these\napproaches diverge from the self-evolution mechanisms observed in human intelli-\ngence, where individuals learn and improve through mutual discussion and collab-\noration. In this work, we introduceCo-EvolvingMulti-AgentSystems (CoMAS),\na novel framework that enables agents to improve autonomously by learning from\ninter-agent interactions without external supervision. CoMAS generates intrinsic\nrewards from rich discussion dynamics, employs an LLM-as-a-judge mechanism\nto formulate these rewards, and optimizes each agent\u2019s policy through RL, thereby\nenabling decentralized and scalable co-evolution. Experimental results demon-\nstrate that CoMAS consistently outperforms untrained agents and achieves state-\nof-the-art performance across most evaluation settings. Ablation studies confirm\nthe necessity of interaction-based reward signals and reveal promising scalability\nas the number and diversity of agents increase. These findings establish CoMAS\nas a novel and effective paradigm for self-evolution in LLM-based agents. Our\ndataset and code are available at https://github.com/xxyQwQ/CoMAS.\n1 INTRODUCTION\nSelf-evolution has emerged as a central research theme for large language model (LLM)\u2013based\nagents, aiming to endow agents with the capacity to continually enhance their capabilities through\ninteraction with the environment (Tao et al., 2024; Gao et al., 2025b"}
{"file": "2510.08529v1.pdf", "section": "1 INTRODUCTION", "chunk": "large language model (LLM)\u2013based\nagents, aiming to endow agents with the capacity to continually enhance their capabilities through\ninteraction with the environment (Tao et al., 2024; Gao et al., 2025b; Fang et al., 2025), rather than\nremaining stagnant after pretraining. Early explorations predominantly adopted RL-free strategies,\nsuch as expanding external knowledge bases (Zhang et al., 2023; Tang et al., 2025), ensembling\nmultiple agents (Ong et al., 2024; Frick et al., 2025), optimizing task workflows (Hu et al., 2024b;\nZhang et al., 2024b; 2025c), and incorporating symbolic learning (Zhuge et al., 2024; Zhou et al.,\n2024). Yet, the effectiveness of these approaches is inherently constrained by the fixed capabilities\nof the underlying foundation models. Although subsequent work has introduced agent fine-tuning\ntechniques (Yuan et al., 2024b; Zhao et al., 2025a), such methods often fall short of supporting\ngenuinely continual, bootstrapping forms of evolutionary improvement.\nReinforcement Learning (RL) offers a promising direction for overcoming these limitations. These\nmethods can be broadly classified by the source of their reward signals. As illustrated in Fig-\nure 1, most existing approaches depend on external rewards, typically derived from rule-based ver-\nifiers (Wan et al., 2025; Estornell et al., 2025) or specialized reward models (Motwani et al., 2024;\nPark et al., 2025). More recently, researchers have begun to explore intrinsic rewards, which dis-\npense with external supervision by leveraging internal signals. Such methods encourage agents to\nBCorresponding authors, jeremyyin@robots.ox.ac.uk, baisanshi@gmail.com.\n1arXiv:2510.08529v1  [cs.CL]  9 Oct 2025\nCoMAS\nInteraction Rewards (Ours)Question\nFind the minimum value of...\nQuestion\nWe should first...\nSolution\nEvaluation\nI think a minor...\nSolution\nLet t=lnx, then...\nEvaluation\nI think there...\n...\n...\nMulti-Agent System\n...\n...\nIntrinsic RewardsCertainty / Confidence...\n...\n...QuestionAgent\nPseudo-Labels (V"}
{"file": "2510.08529v1.pdf", "section": "1 INTRODUCTION", "chunk": "Evaluation\nI think a minor...\nSolution\nLet t=lnx, then...\nEvaluation\nI think there...\n...\n...\nMulti-Agent System\n...\n...\nIntrinsic RewardsCertainty / Confidence...\n...\n...QuestionAgent\nPseudo-Labels (Voting)Semantic Entropy\nExternal Rewards...\n......QuestionAgent\nRule-Based\nVerifierSpecialized\nReward Model\nFigure 1: A comparison of our proposed CoMAS framework with existing RL-based self-evolution\nmethods. The left column outlines methods utilizing external rewards from verifiers or reward mod-\nels. The middle column outlines methods leveraging intrinsic rewards from metrics such as self-\ncertainty, confidence, semantic entropy, and pseudo-labels from majority voting. The right column\noutlines our CoMAS framework, which derives rewards from multi-agent interactions.\navoid low-probability regions or reinforce high-confidence predictions, using mechanisms such as\nself-certainty (Zhao et al., 2025b), confidence (Prabhudesai et al., 2025), semantic entropy (Zhang\net al., 2025e), or pseudo-labels from majority voting (Zuo et al., 2025). This direction opens a path\ntoward self-evolution where external rewards are no longer a prerequisite.\nDespite notable advances in RL-based methods, their design remains largely centered on self-\nrewarding at the level of individual models, in contrast to human intelligence, which has evolved\nas a collective phenomenon emerging from the diversity and interplay of individuals rather than\nfrom any single, perfect principle (Minsky, 1986). In human teams, individuals learn and improve\nthrough mutual discussion and collaboration, without an external oracle evaluating every contribu-\ntion (Barron, 2003). This contrast motivates a critical research question:Can LLM-based agents,\nakin to human beings, achieve self-evolution by learning purely from inter-agent interaction within\na multi-agent system, without relying on external reward signals?\nTo address this question, we proposeCo-EvolvingMulti-AgentSystems (CoMAS), a novel frame-\nwork where agents autonomously improve by learning from their interactions (Section 3). CoMAS\nis built upon three core components. First,interactiongenerates rich conversational data through\ncollaborative and critical discussions, structured around solution proposal, evaluation, and scoring.\nSecond,reward formulationuses an LLM-as-a-judge mechanism to derive intrinsic reward signals\ndirectly from this discussion"}
{"file": "2510.08529v1.pdf", "section": "1 INTRODUCTION", "chunk": "interactiongenerates rich conversational data through\ncollaborative and critical discussions, structured around solution proposal, evaluation, and scoring.\nSecond,reward formulationuses an LLM-as-a-judge mechanism to derive intrinsic reward signals\ndirectly from this discussion history. Finally,policy optimizationemploys an RL algorithm to update\neach agent\u2019s policy, enabling them to effectively internalize the lessons from the interaction data.\nAs a new paradigm for self-evolution, CoMAS offers several distinct advantages: (1) It generates\nreward signals intrinsically from agent interactions, eliminating the need for verifiers or reward\nmodels. (2) The learning paradigm is generally effective for various tasks, including open-ended\nproblems where solutions cannot be easily verified. (3) Agents are trained in a decentralized manner,\nallowing for co-evolution of heterogeneous systems without the bottleneck of a shared model. (4) It\nfosters skills that transfer to out-of-domain tasks and diverse multi-agent collaboration settings.\nWe evaluate CoMAS across various benchmarks in both single-agent and multi-agent settings (Sec-\ntion 4). Results show that CoMAS delivers consistent, and in many cases, state-of-the-art perfor-\nmance, achieving absolute gains of up to 2.20%, 3.66%, 19.80%, and 6.10% over untrained agents in\nVanilla, Consistency, AutoGen, and Debate setups, respectively. In contrast, baseline methods show\ninstability and performance degradation in many cases. Ablation studies confirm that interaction-\nbased reward formulation prevents training collapse and reward hacking, and further demonstrates\nthat CoMAS has great scalability as the number and diversity of agents increase. These results\nestablish CoMAS as a novel and effective paradigm for the self-evolution of LLM-based agents.\n2\nCoMAS\n2 RELATEDWORK\n2.1 REINFORCEMENTLEARNING FORLLMS\nThe success of ChatGPT (Ouyang et al., 2022) has established reinforcement learning from human\nfeedback (RLHF) (Kaufmann et al., 2023) as one of the core post-training paradigms for LLMs,\nenabling alignment with human values through the classical RL algorithm PPO (Schulman et al.,\n2017). To address efficiency and stability concerns, subsequent works have proposed Direct Pref-\nerence Optimization (DPO) (Rafailov et al., 2023"}
{"file": "2510.08529v1.pdf", "section": "2.1 REINFORCEMENTLEARNING FORLLMS", "chunk": "values through the classical RL algorithm PPO (Schulman et al.,\n2017). To address efficiency and stability concerns, subsequent works have proposed Direct Pref-\nerence Optimization (DPO) (Rafailov et al., 2023) and numerous variants (Wu et al., 2024a; Meng\net al., 2024; Ethayarajh et al., 2024). The development of sophisticated reward models (Zhang\net al., 2025g; Zhong et al., 2025) and LLM-as-a-judge methodologies (Zheng et al., 2023; Gu et al.,\n2024) has further accelerated progress in this field. Recently, the success of DeepSeek-R1 (Guo\net al., 2025) has introduced reinforcement learning from verifiable reward (RLVR) (Lambert et al.,\n2024) as a new paradigm that effectively enhances LLM reasoning capabilities through GRPO (Shao\net al., 2024). Follow-up research has explored the underlying principles and further improved the\neffectiveness of this approach (Liu et al., 2025b; Yu et al., 2025). Overall, the evolution of reward\nmodeling demonstrates a clear progression from coarse-grained, human-annotated rewards toward\nfine-grained, automatically generated rewards (Lightman et al., 2023; Luo et al., 2024).\n2.2 SELF-EVOLVINGAGENTS\nSelf-evolution represents a fundamental paradigm in LLM-based agents (Tao et al., 2024; Gao et al.,\n2025b; Fang et al., 2025; Zhang et al., 2025b), referring to the capacity of agents to autonomously\nenhance their capabilities through environmental interaction. Mainstream approaches treat agents\nas compound systems and involve diverse methodologies, including enriching external knowledge\nbases (Zhang et al., 2023; Tang et al., 2025), ensembling multiple agents through intelligent routing\nmechanisms (Ong et al., 2024; Frick et al., 2025), optimizing task-specific agentic workflows (Hu\net al., 2024b; Zhang et al., 2024b; 2025c), and leveraging symbolic learning to refine collabora-\ntive processes (Yuan et al., 2024a;"}
{"file": "2510.08529v1.pdf", "section": "2.2 SELF-EVOLVINGAGENTS", "chunk": "entic workflows (Hu\net al., 2024b; Zhang et al., 2024b; 2025c), and leveraging symbolic learning to refine collabora-\ntive processes (Yuan et al., 2024a; Zhou et al., 2024; Ma et al., 2025a). Additionally, some works\npursue agent evolution through fine-tuning on self-generated trajectories (Yuan et al., 2024b; Zhao\net al., 2025a), while recent research has begun exploring reinforcement learning as a mechanism for\nachieving agent self-evolution (Estornell et al., 2024; Wan et al., 2025; Estornell et al., 2025; Gao\net al., 2025a; Motwani et al., 2024; Park et al., 2025).\n2.3 MULTI-AGENTSYSTEMS\nMulti-agent systems (MAS) represent another core research area within LLM-based agents (Guo\net al., 2024; Li et al., 2024; Tran et al., 2025), enhancing reasoning capabilities through multi-agent\ninteraction and collaboration. Early research primarily focused on static systems with pre-defined\nroles and protocols (Hong et al., 2023; Li et al., 2023; Wu et al., 2024b), structured argumentative\ndebates (Du et al., 2023; Liang et al., 2023; Wang et al., 2024; Qian et al., 2024), and large-scale\nsocial simulations (Yang et al., 2024b; Piao et al., 2025). Recent work has addressed the limitations\nof these fixed-structure systems by developing dynamic, self-improving architectures that leverage\ngraph-based or reward-driven mechanisms (Zhuge et al., 2024; Zhang et al., 2024a; 2025a; Zhou\net al., 2025; Hu et al., 2025b; Zhang et al., 2025f; Liu et al., 2025a; Liao et al., 2025).\n3 METHOD\nTo explore the potential that LLM-based agents, akin to human beings, achieve self-evolution by\nlearning from inter-agent interaction within a multi-agent system, we proposeCo-EvolvingMulti-\nAgentSystems (CoMAS"}
{"file": "2510.08529v1.pdf", "section": "2.3 MULTI-AGENTSYSTEMS", "chunk": "5).\n3 METHOD\nTo explore the potential that LLM-based agents, akin to human beings, achieve self-evolution by\nlearning from inter-agent interaction within a multi-agent system, we proposeCo-EvolvingMulti-\nAgentSystems (CoMAS), a multi-agent framework where agents learn and adapt by interacting\nwithin a shared environment, mimicking collaborative and critical discussion.\nFigure 2 shows an overview of our proposed CoMAS pipeline. CoMAS is built upon an interactive\nmulti-agent workflow: interaction, reward formulation, and policy optimization. Interaction repre-\nsents the procedure that generates conversational data through collaborative and critical discussion,\nreward formulation is responsible for extracting reward signals from the conversation history and\n3\nCoMAS\nDiscussion\nFind the minimum value of the\nfunction f(x)=xlnx where x>0.\nWe should first compute the\nderivative of f(x) ...\nSolution\nQuestion\nEvaluation\nI think a minor mistake is\nthe missing verification...\nSolution\nLet t=lnx, then f(t)=te^t is a\nsimpler function...\n9:13AM\n9:14AM\n9:32AM\n10:15\nAM\nEvaluation\nI think there is no need to\ntake the transformation...10:01AM\nAgents\n...\nNotice that f(x) is monotonically increasing, so\nthe minimum value is obtained at f(0)=0.Solution\n10:15AMDiscussion\nFind the minimum value of the\nfunction f(x)=xlnx where x>0.\nQuestion\nSolution\nLet t=lnx, then f(t)=te^t is a\nsimpler function...\n8:59AM\n9:32AM\n10:28\nAM\nEvaluation\nI think there is no need to\ntake the transformation...10:01AM\nActually, we have f'(x)<0 when x<1/e, so the\nsolution is completely wrong.Evaluation\n10:28AMOther 1 post folded.\nSolution\nNotice that f(x) is\nmonotonically increasing, so...\n10:15AMDiscussion\nFind the minimum value of the\nfunction f(x)=xlnx where x>0.\nQuestion\nSolution\nNotice that f(x) is\nmonotonically increasing, so...\n8:59AM\n10:15AM\n10:30\nAM\nEvaluation\nActually, we have f'(x)\n<0 when x"}
{"file": "2510.08529v1.pdf", "section": "3 METHOD", "chunk": "0.\nQuestion\nSolution\nNotice that f(x) is\nmonotonically increasing, so...\n8:59AM\n10:15AM\n10:30\nAM\nEvaluation\nActually, we have f'(x)\n<0 when x<1/e...10:28AM\nThe monotocity claimed does not exist, leading\nto a wrong answer. My score is\u00a0 1 out of 3 .Scoring10:30AMOther 2 posts folded.\nAgents\n... AgentsReward Formulation\nReward :\n0.0\n...\n8:59AM\nReward :\n1.0\nReward :\n0.0\nFigure 2: An overview of our proposed CoMAS pipeline. CoMAS is built upon a flexible and inter-\nactive multi-agent workflow, composed of three core components: interaction, reward formulation,\nand policy optimization. For a given question, the agents conduct a discussion through contributing\nsolutions, evaluating the existing solutions, and scoring solutions based on their evaluations. The\nscores will be extracted and transformed into rewards for the corresponding solutions and evalua-\ntions. All the generated experiences will be collected to train the policies of the agents.\nallocating rewards to corresponding actions, and policy optimization utilizes an RL algorithm to\nupdate the weights of the agents, thus achieving self-evolution.\n3.1 INTERACTION\nInspired by discussion formats in technical communities (e.g.Reddit, Github, and Stack Overflow),\nour framework facilitates hierarchical and decentralized interactions. The environment contains a\nset oflagents,U={u 1, u2, . . . , u l}. The policy of each agentu kis\u03c0\u03b8k, parameterized by\u03b8 k. This\ndesign allows for heterogeneity, meaning that agents can be based on different foundation models\nrather than sharing a single backbone.\nFrom the action-level perspective, the policy\u03c0 \u03b8kof agentu kacts as a function that maps an input\npromptpto a responseo:\no=\u03c0 \u03b8k(p)(1)\nHere,prepresents the full input context, which varies depending on the interaction type. From\nthe token-level perspective, the outputois a sequence ofTtokens{o t}T\nt=1generated in an auto-\nregressive manner. Each tokeno tis sampled from the policy\u2019s probability distribution, conditioned\non the promptpand previously generated tokenso <t:\not\u223c\u03c0\u03b8k(\u00b7|p, o <t"}
{"file": "2510.08529v1.pdf", "section": "3.1 INTERACTION", "chunk": "generated in an auto-\nregressive manner. Each tokeno tis sampled from the policy\u2019s probability distribution, conditioned\non the promptpand previously generated tokenso <t:\not\u223c\u03c0\u03b8k(\u00b7|p, o <t)(2)\nThese two perspectives are equivalent. We use the high-level functional notation for clarity in de-\nscribing interactions and revert to the token-level view for the policy optimization in Section 3.3.\nIn CoMAS, we define three primary interaction patterns:\n1.Solution: Given a specific questionqand its discussion historyh q, the agent generates a\nsolutions ito the question:\nsi=uk(q, h q)(3)\nwhereiis the index of the solution for questionq.\n2.Evaluation: Given a specific questionq, its discussion historyh q, and a solutions ito the\nquestion, the agent provides a critical evaluatione i,jfor the solution:\nei,j=uk(q, h q, si)(4)\n4\nCoMAS\nwherejis the index of the evaluation for solutions i. The agent is explicitly prompted to\nidentify potential flaws ins irather than simply agreeing. This helps mitigate the catering\nbias common in LLMs and aligns with our reward design.\n3.Scoring: Given a specific questionq, a solutions ito the question, and an evaluatione i,j\nfor the solution, the agent scores the solution based on the evaluation:\n\u03c4i,j=uk(q, si, ei,j)(5)\nThe agent is explicitly prompted to output in a specific format, so that the score can be\ncorrectly extracted. Different from the solution and evaluation, scoring is an independent\ninteraction pattern specifically designed for reward generation, contributing nothing to the\ndiscussion history. Further details will be described in Section 3.2.\nFor each interaction step, the acting agentu kis selected uniformly at random from the agent pool\nU, i.e.,u k\u223cUniform(U). This is based on the assumption that each agent has an equal chance to\ncontribute to the discussion. Besides, a uniform distribution ensures similar quantities of experiences\nfor each agent, which balances the training load.\nBased on these defined interaction patterns, we can formulate the entire interaction process. Given a\nquestionqsampled from the dataset, the interaction process unfolds overmconsecutive rounds. In\neach roundi\u2208 {1, . . . , m}, a solutions"}
{"file": "2510.08529v1.pdf", "section": "3.1 INTERACTION", "chunk": "interaction patterns, we can formulate the entire interaction process. Given a\nquestionqsampled from the dataset, the interaction process unfolds overmconsecutive rounds. In\neach roundi\u2208 {1, . . . , m}, a solutions iis generated referring to the discussion historyh q, followed\nbynevaluations{e i,j}n\nj=1trying to figure out the mistakes in the solution. Then the solutions i\ntogether with its evaluations{e i,j}n\nj=1are appended to the discussion historyh q, enriching the\ncontext for subsequent rounds. Besides, the discussion historyh qwill be compressed to the last\u03ba\nrounds to avoid the discussion history exceeding the context limitation. In addition, each solution\nand evaluation pair(s i, ei,j)is processed with the scoring step to form\u03c4 i,j. Finally, the entire\ninteraction process yields a total ofmsolutions,m\u00b7nevaluations, andm\u00b7nscoring results.\n3.2 REWARDFORMULATION\nThe interaction process generates a rich set of trajectories. To derive a learning signal, we employ\nan LLM-as-a-judge approach (Gu et al., 2024) to assign rewards. This is based on the primary\ninteraction pattern of scoring defined in Section 3.1. For each solution and evaluation pair(s i, ei,j),\nwe already have the scoring result\u03c4 i,jthrough the scoring step. Then the scoring result is parsed to\nextract the score value:\n\u02c6\u03c4i,j=Extract(\u03c4 i,j)(6)\nwhere Extract(\u00b7)predefines a function to extract the score value from the formatted scoring result.\nThe score value\u02c6\u03c4 i,jshould be an integer between 1 and 3, with the following semantics:\n\u2022 3: The solution is correct; the evaluation is unhelpful or incorrect.\n\u2022 2: The solution is mostly correct but has minor flaws pointed out by the evaluation.\n\u2022 1: The solution is incorrect with fatal mistakes identified by the evaluation.\nThen the score value is normalized to the range of[0,1]and used to compute complementary re-\nwards for the solution and the evaluation. This creates a zero-sum game between the solver and the\nevaluator, encouraging both correctness and critical thinking:\nr(si) =\u02c6\u03c4i,j\u22121\n2, r(e i,j) = 1\u2212r"}
{"file": "2510.08529v1.pdf", "section": "3.2 REWARDFORMULATION", "chunk": "and the evaluation. This creates a zero-sum game between the solver and the\nevaluator, encouraging both correctness and critical thinking:\nr(si) =\u02c6\u03c4i,j\u22121\n2, r(e i,j) = 1\u2212r(s i) =3\u2212\u02c6\u03c4 i,j\n2(7)\nAdditionally, a penalty is applied to the agent that performs the scoring step if its output format is\ninvalid. Given the scoring result\u03c4 i,j, its penalty reward is defined as:\nr(\u03c4i,j) =\u001a0,if\u03c4 i,j\u2208 {1,2,3}correctly extracted\n\u22121,otherwise(8)\nwhere a zero reward is assigned for successful scoring, which facilitates format following while\nencouraging the scoring agents to be neutral, thus improving the stability of the training process.\n5\nCoMAS\n3.3 POLICYOPTIMIZATION\nMany existing works on RL for LLMs employ GRPO (Shao et al., 2024) and its variants (Liu\net al., 2025b; Yu et al., 2025). However, our CoMAS framework features diverse interaction pat-\nterns rather than generating multiple rollouts from a single prompt. For this reason, we adopt RE-\nINFORCE++ (Hu et al., 2025a), an effective RL algorithm that is naturally compatible with our\napproach and can be implemented with minimal modifications to the underlying architecture.\nAll the agents within our framework are trained using the same procedure, so we only describe the\nupdate for a single agentu kfor simplicity. Interactions involving agentu k(as a solver, evaluator,\nor scorer) are collected into a replay bufferD k={(p, o, r(o))}, wherepis the context,ois the\ngenerated output, andr(o)is its assigned reward.\nFor each sample(p, o, r(o))inD k, the objective is based on a token-level credit assignment. The\nadvantageA tis defined based on each tokeno tin the sequence. This advantage consists of the\ntrajectory-level rewardr(o)penalized by a cumulative KL-divergence term to regularize the policy:\nAt=r(o)\u2212\u03b2|o|X\n\u03bb=tlog\u03c0\u03b8k(o\u03bb|p, o<\u03bb)\n\u03c0ref(o\u03bb|p, o<\u03bb)(9)\nHere,\u03c0 refis a fixed reference"}
{"file": "2510.08529v1.pdf", "section": "3.3 POLICYOPTIMIZATION", "chunk": "the policy:\nAt=r(o)\u2212\u03b2|o|X\n\u03bb=tlog\u03c0\u03b8k(o\u03bb|p, o<\u03bb)\n\u03c0ref(o\u03bb|p, o<\u03bb)(9)\nHere,\u03c0 refis a fixed reference policy, typically the initial pre-trained model, to constrain the scale\nof the update step, and\u03b2controls the strength of this KL penalty. The advantagesA tare then\nnormalized across the batch from the replay bufferD kto stabilize updates:\n\u02c6At=At\u2212Mean({A t})\nStd({A t}) +\u03f5(10)\nWe then use the surrogate objective to improve the policy, which is defined as:\nJ(\u03b8k) =E (p,o,r(o))\u223cD k\uf8ee\n\uf8f0|o|X\nt=1min\u0010\n\u03c1t(\u03b8k)\u02c6At,clip(\u03c1 t(\u03b8k),1\u2212\u03f5,1 +\u03f5) \u02c6At\u0011\uf8f9\n\uf8fb (11)\nNote that\u03c1 t(\u03b8k) =\u03c0\u03b8k(ot|p,o<t)\n\u03c0old(ot|p,o<t)is the importance sampling ratio, where\u03c0 oldis the policy before\nthe current update. The gradient of the surrogate objective exactly points to the direction of the\npolicy update, thus encouraging the action tokens that lead to higher advantages while constraining\nthe policy update to be within a trusted region, which ensures stable learning.\n4 EXPERIMENTS\n4.1 EXPERIMENTALSETUP\n4.1.1 IMPLEMENTATIONDETAILS\nInfrastructure.We implement our CoMAS framework based on MARTI (Zhang et al., 2025d),\nan LLM-based multi-agent RL framework forked from OpenRLHF (Hu et al., 2024a). The train-\ning pipeline of MARTI consists of three phases: trajectory rollout, experience making, and agent\ntraining, which exactly aligns with our framework described in Section 3.\nParameters.We adopt Qwen2.5-3B-Instruct (Yang et al., 2024a) as the base model under a homo-\ngeneous setting to balance the model capacity and training budget. The agent number is set tol= 4\nfor main experiments andl= 2for ablation studies. The interaction rounds are set tom= 2\u00b7land\nv= 1with a horizon"}
{"file": "2510.08529v1.pdf", "section": "4.1.1 IMPLEMENTATIONDETAILS", "chunk": "balance the model capacity and training budget. The agent number is set tol= 4\nfor main experiments andl= 2for ablation studies. The interaction rounds are set tom= 2\u00b7land\nv= 1with a horizon of\u03ba= 2. More details are provided in Appendix B.\nDatasets.Our training dataset comprises 2000 samples across three domains: 600 math tasks of\nlevel-4 or level-5 from MATH (Hendrycks et al., 2021), 600 coding tasks of medium and hard levels\nfrom KodCode (Xu et al., 2025), and 800 science tasks of physics, chemistry, and biology categories\nfrom WebInstruct-verified (Ma et al., 2025b). These datasets have proven effective for RL training\nand are completely distinct from our evaluation benchmarks.\n6\nCoMAS\nTable 1: The evaluation results of the agents trained by our framework on different benchmarks\nwhen employed in different setups, together with the comparison with the selected baselines. The\nresults with performance improvements are highlighted in green, drops in red, and neutral changes\nin gray. CoMAS consistently improves the performance of the agents in almost all the settings.\nMethodDataset\nGSM8K MATH-500 HumanEval MBPP SciBench GPQA MMLUVanillaUntrained 84.00 51.40 68.90 54.00 32.67 26.79 61.40\nMAPoRL 84.80 (+0.80) 52.60 (+1.20) 69.51 (+0.61) 56.00 (+2.00) 34.07 (+1.40)28.12(+1.34) 61.40 (+0.00)\nTTRL 84.40 (+0.40)53.40(+2.00) 68.29 (-0.61)57.40(+3.40) 34.47 (+1.80) 25.45 (-1.34) 61.60 (+0.20)\nCoMAS (Ours)85.40(+1.40) 52.80 (+1.40)70.73(+1.83) 56.20 (+2.20)34.67(+2.00) 27.46 (+0.67)62.40(+1."}
{"file": "2510.08529v1.pdf", "section": "4.1.1 IMPLEMENTATIONDETAILS", "chunk": ") 52.80 (+1.40)70.73(+1.83) 56.20 (+2.20)34.67(+2.00) 27.46 (+0.67)62.40(+1.00)ConsistencyUntrained 85.40 55.00 73.78 55.80 36.47 28.79 63.20\nMAPoRL 85.80 (+0.40) 55.40 (+0.40) 75.61 (+1.83) 57.00 (+1.20)39.08(+2.61)31.47(+2.68) 63.20 (+0.00)\nTTRL88.20(+2.80)56.80(+1.80) 73.78 (+0.00) 59.00 (+3.20) 38.48 (+2.00) 27.23 (-1.56) 63.80 (+0.60)\nCoMAS (Ours) 87.20 (+1.80) 55.80 (+0.80)77.44(+3.66)59.20(+3.40) 37.68 (+1.20) 29.69 (+0.89)65.60(+2.40)AutoGenUntrained 52.60 38.40 39.63 29.80 20.24 16.29 37.40\nMAPoRL 50.00 (-2.60) 37.40 (-1.00) 39.63 (+0.00) 34.60 (+4.80) 20.64 (+0.40) 21.65 (+5.36) 40.40 (+3.00)\nTTRL 41.00 (-11.60) 37.80 (-0.60) 23.17 (-16.46) 22.80 (-7.00) 19.64 (-0.60) 14.06 (-2.23) 34.00 (-3.40)\nCoMAS (Ours)72.40(+19.80)45.80(+7.40)50.61(+10.98)38.00(+8.20)22.85(+2.61)22.99(+6.70)50.60(+"}
{"file": "2510.08529v1.pdf", "section": "4.1.1 IMPLEMENTATIONDETAILS", "chunk": "40(+19.80)45.80(+7.40)50.61(+10.98)38.00(+8.20)22.85(+2.61)22.99(+6.70)50.60(+13.20)DebateUntrained 84.60 55.00 71.34 54.80 38.68 28.35 62.80\nMAPoRL 85.40 (+0.80) 53.60 (-1.40) 74.39 (+3.05) 55.60 (+0.80)39.88(+1.20)31.47(+3.12) 64.80 (+2.00)\nTTRL86.20(+1.60) 55.20 (+0.20) 73.78 (+2.44)58.00(+3.20) 37.88 (-0.80) 29.02 (+0.67) 64.00 (+1.20)\nCoMAS (Ours) 85.20 (+0.60)55.40(+0.40)77.44(+6.10) 55.60 (+0.80) 39.08 (+0.40) 29.91 (+1.56)65.20(+2.40)\n4.1.2 EVALUATIONDETAILS\nBaselines.We compare against three baselines: untrained agents as a weak baseline, MAPoRL (Park\net al., 2025), and TTRL (Zuo et al., 2025) as strong baselines. MAPoRL implements multi-agent\nRL within a debate framework (Du et al., 2023; Liang et al., 2023), but we replace its specialized\nreward model with a rule-based verifier. TTRL leverages majority voting to generate pseudo-labels\nfor test-time training. For coding and science tasks where majority voting cannot produce reliable\nanswers, we implement dummy rewards to ensure TTRL can be applied to our training dataset.\nBenchmarks.We conduct evaluation on multiple standard benchmarks, including GSM8K (Cobbe\net al., 2021) and MATH-500 (Hendrycks et al., 2020) for math, HumanEval (Chen et al., 2021) and\nMBPP (Austin et al., 202"}
{"file": "2510.08529v1.pdf", "section": "4.1.2 EVALUATIONDETAILS", "chunk": "et al., 2021) and MATH-500 (Hendrycks et al., 2020) for math, HumanEval (Chen et al., 2021) and\nMBPP (Austin et al., 2021) for coding, SciBench (Wang et al., 2023) and GPQA (Rein et al., 2024)\nfor science, and MMLU (Hendrycks et al., 2020) for general knowledge. For evaluation efficiency,\nwe randomly retain 500 samples if the benchmark contains more than 500 tasks.\nSetups.We evaluate the trained agents on various inference setups, including Vanilla (i.e.,direct\ninference) and Consistency (Wang et al., 2022) for single-agent setups, and AutoGen (Li et al., 2023;\nWu et al., 2024b) and Debate (Du et al., 2023; Liang et al., 2023) for multi-agent setups. All the\nsetups follow the standard implementation from MASLab (Ye et al., 2025).\n4.2 MAINRESULTS\n/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013\n/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053/uni00000017/uni0000001b/uni00000013/uni00000018/uni00000013/uni00000013/uni00000018/uni00000015/uni00000013/uni00000018/uni00000017/uni00000013/uni00000018/uni00000019/uni00000013/uni00000018/uni0000001b/uni00000013/uni00000019/uni00000013/uni00000013/uni00000019/uni00000015/uni00000013/uni00000019/uni00000017"}
{"file": "2510.08529v1.pdf", "section": "4.2 MAINRESULTS", "chunk": "uni0000001b/uni00000013/uni00000019/uni00000013/uni00000013/uni00000019/uni00000015/uni00000013/uni00000019/uni00000017/uni00000013/uni00000035/uni00000048/uni00000056/uni00000053/uni00000052/uni00000051/uni00000056/uni00000048/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b\n/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000035/uni00000048/uni00000056/uni00000053/uni00000052/uni00000051/uni00000056/uni00000048/uni00000003/uni0000002f/uni00000048/uni00000051/uni0000004a/uni00000057/uni0000004b/uni00000003/uni00000059/uni00000056/uni00000011/uni00000003/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053\n/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000003/uni00000014\n/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000003/uni00000015\n/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000003/uni00000016\n/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000003/uni00000017\n/uni00000013"}
{"file": "2510.08529v1.pdf", "section": "4.2 MAINRESULTS", "chunk": "uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000059/uni00000056/uni00000011/uni00000003/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053\n/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000003/uni00000014\n/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000003/uni00000015\n/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000003/uni00000016\n/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000003/uni00000017\nFigure 3: Training dynamics of CoMAS. The left figure shows the curve of the average response\nlength of each agent during training. The right figure shows the curve of the average normalized\nreward of each agent during training. These trends together indicate that CoMAS achieves a stable\nand effective training process that improves the capabilities of agents.\n7\nCoMAS\nWe follow the settings described in Section 4.1 to conduct the main experiments. Table 1 presents\nthe performance of the agents trained by our CoMAS framework under different benchmarks and\nsetups, together with a comprehensive comparison with the selected baselines.\nIn terms of the single-agent setups, CoMAS consistently improves over the untrained base model\nacross all benchmarks and remains competitive with MAPoRL, which relies on external reward\nsignals from the rule-based verifier. Under the Vanilla setup, CoMAS yields the best results on\nGSM8K (85.40%), HumanEval (70.73%), SciBench (34.67%), and MMLU (62."}
{"file": "2510.08529v1.pdf", "section": "4.2 MAINRESULTS", "chunk": "from the rule-based verifier. Under the Vanilla setup, CoMAS yields the best results on\nGSM8K (85.40%), HumanEval (70.73%), SciBench (34.67%), and MMLU (62.40%), while being\nclose to the best on the remaining datasets. Under the Consistency setup, CoMAS again improves\nall benchmarks and reaches the best scores on HumanEval (77.44%), MBPP (59.20%), and MMLU\n(65.60%), with comparable performance on the remaining benchmarks. Although TTRL attains\noutstanding performance on GSM8K (88.20%) and MATH-500 (56.80%), it fails on HumanEval\n(73.78%) and GPQA (27.33%), which demonstrates its specialized effectiveness on math tasks and\nreveals its significant limitation on general tasks.\nFor the multi-agent setups, CoMAS demonstrates clear advantages in both stability and absolute\ngains. Under the AutoGen setup, training with TTRL collapses markedly and MAPoRL brings\nmixed or negative changes, while CoMAS delivers large improvements over untrained agents on\nevery benchmark, especially on GSM8K (72.40%), MATH-500 (45.80%), HumanEval (50.61%),\nMBPP (38.00%), and MMLU (50.60%). Under the Debate setup, all the methods benefit from the\nstrong collaborative pattern, but CoMAS attains the best or near-best results overall, especially lead-\ning on MATH-500 (55.40%), HumanEval (77.44%), and MMLU (65.20%), while TTRL slightly\nedges out GSM8K and MBPP and MAPoRL tops SciBench and GPQA. These results indicate that\nCoMAS provides robust and generalizable gains across interaction regimes, particularly excelling\nin multi-agent collaboration where alternative methods can be fragile.\nFigure 3 provides insight into the training dynamics of CoMAS. The left figure tracks the average\nresponse length per agent, showing consistent growth throughout training. This increase reflects the\nagents\u2019 improving capabilities in both solution generation and evaluation, aligning with the estab-\nlished patterns in LLM reasoning (Yeo et al., 2025). The right figure displays the average normalized\nreward for each agent. Despite mid-training fluctuations, rewards consistently hover around 0.5 and\neventually converge"}
{"file": "2510.08529v1.pdf", "section": "4.2 MAINRESULTS", "chunk": "ab-\nlished patterns in LLM reasoning (Yeo et al., 2025). The right figure displays the average normalized\nreward for each agent. Despite mid-training fluctuations, rewards consistently hover around 0.5 and\neventually converge to similar values across agents. This convergence demonstrates that our adver-\nsarial interaction reward design successfully creates a stable training environment.\n4.3 ABLATIONSTUDIES\n4.3.1 REWARDFORMULATION\n/uni00000039/uni00000044/uni00000051/uni0000004c/uni0000004f/uni0000004f/uni00000044 /uni00000026/uni00000052/uni00000051/uni00000056/uni0000004c/uni00000056/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c\n/uni0000002c/uni00000051/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni00000036/uni00000048/uni00000057/uni00000058/uni00000053/uni00000017/uni00000013/uni00000017/uni00000018/uni00000018/uni00000013/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni0000000e/uni00000013/uni00000011/uni00000013/uni00000013/uni00000008/uni0000000e/uni00000013/uni00000011/uni00000013/uni00000013/uni00000008\n/uni0000000e/uni00000014/uni00000011/uni00000018/uni00000014/uni00000008/uni0000000e/uni00000015"}
{"file": "2510.08529v1.pdf", "section": "4.3.1 REWARDFORMULATION", "chunk": "00013/uni00000008\n/uni0000000e/uni00000014/uni00000011/uni00000018/uni00000014/uni00000008/uni0000000e/uni00000015/uni00000011/uni00000013/uni00000015/uni00000008\n/uni00000010/uni00000013/uni00000011/uni0000001a/uni0000001b/uni00000008/uni00000010/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000008\n/uni00000010/uni00000013/uni00000011/uni00000013/uni00000016/uni00000008/uni00000010/uni00000013/uni00000011/uni00000015/uni0000001a/uni00000008/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000059/uni00000056/uni00000011/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000029/uni00000052/uni00000055/uni00000050/uni00000058/uni0000004f/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni00000038/uni00000051/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000048/uni00000047\n/uni00000026/uni00000052/uni00000030/uni00000024/uni00000036/uni00000003/uni0000000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\n/uni0000005a/uni00000012/uni00000052/uni000000"}
{"file": "2510.08529v1.pdf", "section": "4.3.1 REWARDFORMULATION", "chunk": "0000b/uni00000032/uni00000058/uni00000055/uni00000056/uni0000000c\n/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni0000004c/uni00000051/uni0000004a\n/uni00000013 /uni00000018 /uni00000014/uni00000013 /uni00000014/uni00000018 /uni00000015/uni00000013 /uni00000015/uni00000018 /uni00000016/uni00000013\n/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053/uni00000013/uni00000011/uni00000018/uni00000013/uni00000011/uni00000019/uni00000013/uni00000011/uni0000001a/uni00000013/uni00000011/uni0000001b/uni00000013/uni00000011/uni0000001c/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047\n/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni000000"}
{"file": "2510.08529v1.pdf", "section": "4.3.1 REWARDFORMULATION", "chunk": "uni00000055/uni00000047\n/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000031/uni00000052/uni00000055/uni00000050/uni00000044/uni0000004f/uni0000004c/uni0000005d/uni00000048/uni00000047/uni00000003/uni00000035/uni00000048/uni0000005a/uni00000044/uni00000055/uni00000047/uni00000003/uni00000059/uni00000056/uni00000011/uni00000003/uni00000037/uni00000055/uni00000044/uni0000004c/uni00000051/uni0000004c/uni00000051/uni0000004a/uni00000003/uni00000036/uni00000057/uni00000048/uni00000053\n/uni00000026/uni00000052/uni00000030/uni00000024/uni00000036\n/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000028/uni00000059/uni00000044/uni0000004f/uni00000058/uni00000044/uni00000057/uni0000004c/uni00000052/uni00000051\n/uni0000005a/uni00000012/uni00000052/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni0000004c/uni00000051/uni0000004a\nFigure 4: Results of the ablation study for reward formulation. The left figure compares the per-\nformance across the original CoMAS and two variants (without evaluation and without scoring).\nThe right figure shows the average normalized rewards during the training process. These results\nindicate that the adversarial reward design is of key importance for the success of CoMAS.\nTo validate the effectiveness of our adversarial reward design described in Section 3.2, we created\ntwo CoMAS variants by removing either the evaluation or scoring steps. In"}
{"file": "2510.08529v1.pdf", "section": "4.3.1 REWARDFORMULATION", "chunk": "adversarial reward design is of key importance for the success of CoMAS.\nTo validate the effectiveness of our adversarial reward design described in Section 3.2, we created\ntwo CoMAS variants by removing either the evaluation or scoring steps. In the first variant (without\nevaluation), rewards come directly from the agents themselves acting as judges. In the second variant\n(without scoring), evaluation steps provide direct judgments on solutions with a supporting ratio as\nthe reward signal, while evaluation rewards are based on mutual consistency. Figure 4 presents our\n8\nCoMAS\nfindings, with the left panel comparing performance across variants and the right panel showing\nnormalized reward trajectories during training.\nWe evaluated performance by averaging scores across all benchmarks under both Vanilla and Con-\nsistency setups. The left panel reveals that both variants underperform compared to the untrained\nbase model, highlighting that our carefully designed adversarial reward formulation is critical to\nCoMAS\u2019s success rather than any casual reward design.\nTo understand the underlying mechanisms, we analyzed the reward dynamics shown in the right\npanel. The original CoMAS maintains stable rewards around 0.5, while both variants start with high\nrewards around 0.8 and then follow divergent trajectories. When evaluation is removed, the reward\ncurve unexpectedly decreases over time, indicating that agents become increasingly strict judges\nrather than generating useful reward signals. When scoring is removed, rewards steadily increase\ntoward the maximum value of 1.0, revealing a reward hacking strategy where agents unanimously\nsupport all solutions, allowing both solution and evaluation steps to receive maximum rewards.\n4.3.2 FRAMEWORKSCALABILITY\n/uni00000039/uni00000044/uni00000051/uni0000004c/uni0000004f/uni0000004f/uni00000044 /uni00000026/uni00000052/uni00000051/uni00000056/uni0000004c/uni00000056/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000024/uni00000058/uni00000057/uni00000052/uni0000002a/uni00000048/uni00000051 /uni00000027/uni00000048/uni00000045/uni00000044/uni00000057/uni00000048\n/uni000"}
{"file": "2510.08529v1.pdf", "section": "4.3.2 FRAMEWORKSCALABILITY", "chunk": "/uni0000002a/uni00000048/uni00000051 /uni00000027/uni00000048/uni00000045/uni00000044/uni00000057/uni00000048\n/uni0000002c/uni00000051/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni00000036/uni00000048/uni00000057/uni00000058/uni00000053/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000017/uni00000013/uni00000017/uni00000018/uni00000018/uni00000013/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni0000000e/uni00000013/uni00000011/uni00000013/uni00000013/uni00000008/uni0000000e/uni00000013/uni00000011/uni00000013/uni00000013/uni00000008\n/uni0000000e/uni00000013/uni00000011/uni00000013/uni00000013/uni00000008/uni0000000e/uni00000013/uni00000011/uni00000013/uni00000013/uni00000008\n/uni0000000e/uni00000013/uni00000011/uni00000014/uni00000014/uni00000008/uni00000010/uni00000013/uni00000011/uni00000014/uni0000001c/uni00000008\n/uni0000000e/uni0000001a/uni00000011/uni000000"}
{"file": "2510.08529v1.pdf", "section": "4.3.2 FRAMEWORKSCALABILITY", "chunk": "00010/uni00000013/uni00000011/uni00000014/uni0000001c/uni00000008\n/uni0000000e/uni0000001a/uni00000011/uni0000001c/uni00000013/uni00000008/uni00000010/uni00000013/uni00000011/uni00000014/uni00000019/uni00000008\n/uni0000000e/uni00000013/uni00000011/uni00000016/uni00000016/uni00000008/uni0000000e/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000008\n/uni0000000e/uni0000001c/uni00000011/uni0000001a/uni00000018/uni00000008/uni0000000e/uni00000014/uni00000011/uni00000014/uni0000001a/uni00000008\n/uni0000000e/uni00000014/uni00000011/uni00000018/uni00000014/uni00000008/uni0000000e/uni00000015/uni00000011/uni00000013/uni00000015/uni00000008\n/uni0000000e/uni0000001c/uni00000011/uni0000001b/uni00000017/uni00000008/uni0000000e/uni00000014/uni00000011/uni0000001a/uni00000018/uni00000008/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000059/uni00000056/uni00000011/uni00000003/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055\n/uni00000038/uni"}
{"file": "2510.08529v1.pdf", "section": "4.3.2 FRAMEWORKSCALABILITY", "chunk": "00051/uni00000057/uni00000003/uni00000031/uni00000058/uni00000050/uni00000045/uni00000048/uni00000055\n/uni00000038/uni00000051/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000048/uni00000047\n/uni00000026/uni00000052/uni00000030/uni00000024/uni00000036/uni00000003/uni0000000b/uni00000014/uni00000010/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni0000000c\n/uni00000026/uni00000052/uni00000030/uni00000024/uni00000036/uni00000003/uni0000000b/uni00000015/uni00000010/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni0000000c\n/uni00000026/uni00000052/uni00000030/uni00000024/uni00000036/uni00000003/uni0000000b/uni00000017/uni00000010/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni0000000c\n/uni00000039/uni00000044/uni00000051/uni0000004c/uni0000004f/uni0000004f/uni00000044 /uni00000026/uni00000052/uni00000051/uni00000056/uni0000004c/uni00000056/uni00000057/uni00000048/uni00000051/uni00000046/uni0000005c /uni00000024/uni00000058/uni00000057/uni00000052/uni0000002a/uni00000048/uni00000051 /uni00000027/uni00000048/uni00000045/uni00000044/uni00000057/uni00000048\n/uni0000002c/uni00000051"}
{"file": "2510.08529v1.pdf", "section": "4.3.2 FRAMEWORKSCALABILITY", "chunk": "00000048/uni00000051 /uni00000027/uni00000048/uni00000045/uni00000044/uni00000057/uni00000048\n/uni0000002c/uni00000051/uni00000049/uni00000048/uni00000055/uni00000048/uni00000051/uni00000046/uni00000048/uni00000003/uni00000036/uni00000048/uni00000057/uni00000058/uni00000053/uni00000015/uni00000018/uni00000016/uni00000013/uni00000016/uni00000018/uni00000017/uni00000013/uni00000017/uni00000018/uni00000018/uni00000013/uni00000018/uni00000018/uni00000019/uni00000013/uni00000019/uni00000018/uni0000001a/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000036/uni00000046/uni00000052/uni00000055/uni00000048/uni0000000e/uni00000013/uni00000011/uni00000013/uni00000013/uni00000008/uni0000000e/uni00000013/uni00000011/uni00000013/uni00000013/uni00000008\n/uni0000000e/uni00000013/uni00000011/uni00000013/uni00000013/uni00000008/uni0000000e/uni00000013/uni00000011/uni00000013/uni00000013/uni00000008\n/uni0000000e/uni00000013/uni00000011/uni00000016/uni00000016/uni00000008/uni0000000e/uni00000013/uni00000011/uni0000001b/uni00000019/uni00000008\n/uni0000000e/uni0000001c/uni00000011/uni0000001a/uni00000018"}
{"file": "2510.08529v1.pdf", "section": "4.3.2 FRAMEWORKSCALABILITY", "chunk": "/uni00000011/uni0000001b/uni00000019/uni00000008\n/uni0000000e/uni0000001c/uni00000011/uni0000001a/uni00000018/uni00000008/uni0000000e/uni00000014/uni00000011/uni00000014/uni0000001a/uni00000008\n/uni0000000e/uni00000015/uni00000011/uni00000015/uni00000014/uni00000008/uni0000000e/uni00000015/uni00000011/uni0000001a/uni0000001b/uni00000008\n/uni0000000e/uni00000019/uni00000011/uni00000019/uni00000017/uni00000008/uni0000000e/uni00000015/uni00000011/uni00000014/uni00000016/uni00000008/uni00000033/uni00000048/uni00000055/uni00000049/uni00000052/uni00000055/uni00000050/uni00000044/uni00000051/uni00000046/uni00000048/uni00000003/uni00000059/uni00000056/uni00000011/uni00000003/uni00000024/uni0000004a/uni00000048/uni00000051/uni00000057/uni00000003/uni00000027/uni0000004c/uni00000059/uni00000048/uni00000055/uni00000056/uni0000004c/uni00000057/uni0000005c\n/uni00000038/uni00000051/uni00000057/uni00000055/uni00000044/uni0000004c/uni00000051/uni00000048/uni00000047\n/uni00000026/uni00000052/uni00000030/uni00000024/uni00000036/uni00000003/uni0000000b/uni0000002b/uni00000052/uni00000050/uni00000052/uni0000004a/uni00000048/uni00000051/uni00000048"}
{"file": "2510.08529v1.pdf", "section": "4.3.2 FRAMEWORKSCALABILITY", "chunk": "00003/uni0000000b/uni0000002b/uni00000052/uni00000050/uni00000052/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000052/uni00000058/uni00000056/uni0000000c\n/uni00000026/uni00000052/uni00000030/uni00000024/uni00000036/uni00000003/uni0000000b/uni0000002b/uni00000048/uni00000057/uni00000048/uni00000055/uni00000052/uni0000004a/uni00000048/uni00000051/uni00000048/uni00000052/uni00000058/uni00000056/uni0000000c\nFigure 5: Results of the ablation study for framework scalability. The left figure shows how the num-\nber of agents affects the performance of CoMAS across different setups. The right figure compares\nthe performance between homogeneous and heterogeneous agent settings. These results demon-\nstrate the underlying scalability of CoMAS with the number and diversity of agents.\nTo investigate the scalability of CoMAS, we examined how performance changes with varying num-\nbers of agents. Using identical base models for all agents, we conducted experiments withl= 1,2,4\nagents. The left panel of Figure 5 compares performance across these settings, with results averaged\nacross all benchmarks. Our findings show that performance generally improves as the number of\nagents increases. This pattern is particularly pronounced in the Consistency and Debate setups,\nwhere single-agent CoMAS shows performance decreases of 0.19% and 0.16%, respectively, while\nfour-agent CoMAS achieves substantial gains of 2.02% and 1.75%. Similar smooth performance\nimprovements are observed in the Vanilla and AutoGen setups. These results highlight the critical\nrole of multi-agent interactions and demonstrate the inherent scalability of the CoMAS framework.\nWe also evaluated the impact of agent diversity within CoMAS. For this experiment, we maintained\nl= 2agents but used different base models: Qwen2.5-3B-Instruct for one agent and Llama-3.2-\n3B-Instruct for the other. The right panel of Figure 5 compares performance between homogeneous\nand heterogeneous settings"}
{"file": "2510.08529v1.pdf", "section": "4.3.2 FRAMEWORKSCALABILITY", "chunk": "but used different base models: Qwen2.5-3B-Instruct for one agent and Llama-3.2-\n3B-Instruct for the other. The right panel of Figure 5 compares performance between homogeneous\nand heterogeneous settings, with results averaged across all benchmarks. Heterogeneous agents\nconsistently outperformed their homogeneous counterparts, with particularly notable improvements\nin the Vanilla (2.21%), Consistency (2.78%), and Debate (2.13%) setups. These findings suggest\nthat diverse knowledge and capabilities from different base models enhance overall performance,\nwith CoMAS effectively encouraging agents to learn from each other\u2019s strengths. This points to the\npotential for even greater performance gains with more diverse agent settings.\n5 CONCLUSION\nIn this paper, we address a fundamental research question on self-evolution of LLM-based agents\ninspired by human intelligence: Can agents achieve self-evolution purely through inter-agent inter-\nactions within a multi-agent system, without relying on external reward signals? To answer this\nquestion, we introduce CoMAS, a novel framework that performs interactions composed of solu-\n9\nCoMAS\ntion proposal, evaluation, and scoring, derives intrinsic rewards from discussion dynamics via an\nLLM-as-a-judge mechanism, and optimizes each agent\u2019s policy through an RL algorithm.\nAcross multiple benchmarks and collaboration settings, CoMAS consistently outperforms untrained\nagents and achieves state-of-the-art performance in most evaluation scenarios. Our ablation studies\ndemonstrate that interaction-based rewards are essential for preventing training collapse and reward\nhacking. Furthermore, we show that performance scales positively with both the number and diver-\nsity of agents, highlighting the framework\u2019s scalability potential. These findings establish CoMAS\nas a novel and effective paradigm for self-evolution in LLM-based agents and open promising av-\nenues for future research in autonomous multi-agent learning systems.\n6 ETHICSSTATEMENT\nThis work draws inspiration from the collaborative intelligence observed in human society to de-\nvelop the CoMAS framework. While our approach is motivated by natural social dynamics, our\nimplementation and experimental evaluation are strictly confined to standard LLM reasoning tasks\nacross domains of math, coding, and science, rather than simulating real-world social interactions\nor scenarios. Given this limited scope, the research presented in this paper does not directly raise\nethical concerns. However, we acknowledge that the multi-agent co-evolution paradigm introduced\nin this work"}
{"file": "2510.08529v1.pdf", "section": "6 ETHICSSTATEMENT", "chunk": "science, rather than simulating real-world social interactions\nor scenarios. Given this limited scope, the research presented in this paper does not directly raise\nethical concerns. However, we acknowledge that the multi-agent co-evolution paradigm introduced\nin this work could potentially be extended to real-world applications that may have broader implica-\ntions for human welfare. We therefore encourage readers and future researchers to carefully consider\nthe ethical implications when extending this paradigm beyond academic benchmarks.\n7 REPRODUCIBILITYSTATEMENT\nWe have made every effort to ensure the reproducibility of our work. We provide comprehensive\ndetails throughout this paper and the appendices. Section 3 presents a detailed description of our\nCoMAS framework, including theoretical foundations and implementation workflow. Section 4.1\ncovers experimental parameters, dataset construction, and evaluation settings, with additional spec-\nifications in Appendix B. Complete experimental results are documented in Appendix C. We also\ninclude prompt templates in Appendix D and an example trajectory in Appendix E to facilitate under-\nstanding and replication. Our dataset and code are released at https://github.com/xxyQwQ/CoMAS.\n10\nCoMAS\nREFERENCES\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language\nmodels.arXiv preprint arXiv:2108.07732, 2021.\nBrigid Barron. When smart groups fail.The journal of the learning sciences, 2003.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large\nlanguage models trained on code.arXiv preprint arXiv:2107.03374, 2021.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,\nMatthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems.arXiv preprint arXiv:2110.14168, 2021.\nYilun Du, Shuang Li, Antonio Torralba"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "Reiichiro Nakano, et al. Training verifiers to\nsolve math word problems.arXiv preprint arXiv:2110.14168, 2021.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. Improving\nfactuality and reasoning in language models through multiagent debate, 2023.arXiv preprint\narXiv:2305.14325, 2023.\nAndrew Estornell, Jean-Franc \u00b8ois Ton, Yuanshun Yao, and Yang Liu. Acc-collab: An actor-critic\napproach to multi-agent llm collaboration.arXiv preprint arXiv:2411.00053, 2024.\nAndrew Estornell, Jean-Francois Ton, Muhammad Faaiz Taufiq, and Hang Li. How to train a leader:\nHierarchical reasoning in multi-agent llms.arXiv preprint arXiv:2507.08960, 2025.\nKawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model\nalignment as prospect theoretic optimization.arXiv preprint arXiv:2402.01306, 2024.\nJinyuan Fang, Yanwen Peng, Xi Zhang, Yingxu Wang, Xinhao Yi, Guibin Zhang, Yi Xu, Bin Wu,\nSiwei Liu, Zihao Li, et al. A comprehensive survey of self-evolving ai agents: A new paradigm\nbridging foundation models and lifelong agentic systems.arXiv preprint arXiv:2508.07407, 2025.\nEvan Frick, Connor Chen, Joseph Tennyson, Tianle Li, Wei-Lin Chiang, Anastasios N Angelopou-\nlos, and Ion Stoica. Prompt-to-leaderboard.arXiv preprint arXiv:2502.14855, 2025.\nHongcheng Gao, Yue Liu, Yufei He, et al. Flowreasoner: Reinforcing query-level meta-agents.\narXiv preprint arXiv:2504.15257, 2025a.\nHuan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu,"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "query-level meta-agents.\narXiv preprint arXiv:2504.15257, 2025a.\nHuan-ang Gao, Jiayi Geng, Wenyue Hua, Mengkang Hu, Xinzhe Juan, Hongzhang Liu, Shilong\nLiu, Jiahao Qiu, Xuan Qi, Yiran Wu, et al. A survey of self-evolving agents: On path to artificial\nsuper intelligence.arXiv preprint arXiv:2507.21046, 2025b.\nJiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Ying-\nhan Shen, Shengjie Ma, Honghao Liu, et al. A survey on llm-as-a-judge.arXiv preprint\narXiv:2411.15594, 2024.\nD Guo et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.\narXiv preprint arXiv:2501.12948, 2025.\nTaicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh V Chawla, Olaf Wiest,\nand Xiangliang Zhang. Large language model based multi-agents: A survey of progress and\nchallenges.arXiv preprint arXiv:2402.01680, 2024.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding.arXiv preprint\narXiv:2009.03300, 2020.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song,\nand Jacob Steinhardt. Measuring mathematical problem solving with the math dataset.arXiv\npreprint arXiv:2103.03874, 2021.\n11\nCoMAS\nSirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "MAS\nSirui Hong, Mingchen Zhuge, Jonathan Chen, Xiawu Zheng, Yuheng Cheng, Jinlin Wang, Ceyao\nZhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, et al. Metagpt: Meta programming for\na multi-agent collaborative framework. InThe Twelfth International Conference on Learning\nRepresentations, 2023.\nJian Hu, Xibin Wu, Wei Shen, Jason Klein Liu, Zilin Zhu, Weixun Wang, Songlin Jiang, Haoran\nWang, Hao Chen, Bin Chen, et al. Openrlhf: An easy-to-use, scalable and high-performance rlhf\nframework.arXiv preprint arXiv:2405.11143, 2024a.\nJian Hu, Jason Klein Liu, Haotian Xu, and Wei Shen. Reinforce++: An efficient rlhf algorithm with\nrobustness to both prompt and reward models.arXiv preprint arXiv:2501.03262, 2025a.\nMengkang Hu, Yuhang Zhou, Wendong Fan, Yuzhou Nie, Bowei Xia, Tao Sun, Ziyu Ye, Zhaoxuan\nJin, Yingru Li, Qiguang Chen, et al. Owl: Optimized workforce learning for general multi-agent\nassistance in real-world task automation.arXiv preprint arXiv:2505.23885, 2025b.\nShengran Hu, Cong Lu, and Jeff Clune. Automated design of agentic systems.arXiv preprint\narXiv:2408.08435, 2024b.\nTimo Kaufmann, Paul Weng, Viktor Bengs, and Eyke H \u00a8ullermeier. A survey of reinforcement\nlearning from human feedback.arXiv preprint arXiv:2312.14925, 2023.\nNathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brah-\nman, Lester James V Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers\nin open language model post-training.arXiv preprint arXiv:2411.15124, 2024.\nGuohao Li, Hasan Hammoud"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "Shane Lyu, et al. Tulu 3: Pushing frontiers\nin open language model post-training.arXiv preprint arXiv:2411.15124, 2024.\nGuohao Li, Hasan Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. Camel: Com-\nmunicative agents for\u201d mind\u201d exploration of large language model society.Advances in Neural\nInformation Processing Systems, 2023.\nXinyi Li, Sai Wang, Siqi Zeng, Yu Wu, and Yi Yang. A survey on llm-based multi-agent systems:\nworkflow, infrastructure, and challenges.Vicinagearth, 2024.\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Shuming\nShi, and Zhaopeng Tu. Encouraging divergent thinking in large language models through multi-\nagent debate.arXiv preprint arXiv:2305.19118, 2023.\nJunwei Liao, Muning Wen, Jun Wang, and Weinan Zhang. Marft: Multi-agent reinforcement fine-\ntuning.arXiv preprint arXiv:2504.16129, 2025.\nHunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan\nLeike, John Schulman, Ilya Sutskever, and Karl Cobbe. Let\u2019s verify step by step. InThe Twelfth\nInternational Conference on Learning Representations, 2023.\nShuo Liu, Zeyu Liang, Xueguang Lyu, and Christopher Amato. Llm collaboration with multi-agent\nreinforcement learning.arXiv preprint arXiv:2508.04652, 2025a.\nZichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee,\nand Min Lin. Understanding r1-zero-like training: A critical perspective.arXiv preprint\narXiv:2503.20783, 2025b.\nLiangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li,\nLei Shu, Yun Zhu"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "20783, 2025b.\nLiangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li,\nLei Shu, Yun Zhu, Lei Meng, et al. Improve mathematical reasoning in language models by\nautomated process supervision.arXiv preprint arXiv:2406.06592, 2024.\nXiaowen Ma, Chenyang Lin, Yao Zhang, V olker Tresp, and Yunpu Ma. Agentic neural networks:\nSelf-evolving multi-agent systems via textual backpropagation.arXiv preprint arXiv:2506.09046,\n2025a.\nXueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, and Wenhu Chen. General-reasoner:\nAdvancing llm reasoning across all domains.arXiv preprint arXiv:2505.14652, 2025b.\nYu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with a\nreference-free reward.Advances in Neural Information Processing Systems, 2024.\n12\nCoMAS\nMarvin Minsky.Society of mind. Simon and Schuster, 1986.\nSumeet Ramesh Motwani, Chandler Smith, Rocktim Jyoti Das, et al. Malt: Improving reasoning\nwith multi-agent llm training.arXiv preprint arXiv:2412.01928, 2024.\nIsaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E Gonzalez,\nM Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms with preference data.arXiv\npreprint arXiv:2406.18665, 2024.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.Advances in neural information processing systems, 2022.\nChanwoo Park, Seungju Han, Xingzhi Guo, Asuman Ozdaglar, Kaiqing Zhang, and Joo-K"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "follow\ninstructions with human feedback.Advances in neural information processing systems, 2022.\nChanwoo Park, Seungju Han, Xingzhi Guo, Asuman Ozdaglar, Kaiqing Zhang, and Joo-Kyung Kim.\nMaporl: Multi-agent post-co-training for collaborative large language models with reinforcement\nlearning.arXiv preprint arXiv:2502.18439, 2025.\nJinghua Piao, Yuwei Yan, Jun Zhang, Nian Li, Junbo Yan, Xiaochong Lan, Zhihong Lu, Zhi-\nheng Zheng, Jing Yi Wang, Di Zhou, et al. Agentsociety: Large-scale simulation of llm-\ndriven generative agents advances understanding of human behaviors and society.arXiv preprint\narXiv:2502.08691, 2025.\nMihir Prabhudesai, Lili Chen, Alex Ippoliti, Katerina Fragkiadaki, Hao Liu, and Deepak Pathak.\nMaximizing confidence alone improves reasoning.arXiv preprint arXiv:2505.22660, 2025.\nChen Qian, Zihao Xie, Yifei Wang, Wei Liu, Kunlun Zhu, Hanchen Xia, Yufan Dang, Zhuoyun Du,\nWeize Chen, Cheng Yang, et al. Scaling large language model-based multi-agent collaboration.\narXiv preprint arXiv:2406.07155, 2024.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea\nFinn. Direct preference optimization: Your language model is secretly a reward model.Advances\nin neural information processing systems, 2023.\nDavid Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Di-\nrani, Julian Michael, and Samuel R Bowman. Gpqa: A graduate-level google-proof q&a bench-\nmark. InFirst Conference on Language Modeling, 2024.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms.arXiv preprint arXiv:1707.06347, 2017.\nZhihong Shao, Pe"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms.arXiv preprint arXiv:1707.06347, 2017.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang,\nMingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathemati-\ncal reasoning in open language models.arXiv preprint arXiv:2402.03300, 2024.\nXiangru Tang, Tianyu Hu, Muyang Ye, Yanjun Shao, Xunjian Yin, Siru Ouyang, Wangchunshu\nZhou, Pan Lu, Zhuosheng Zhang, Yilun Zhao, et al. Chemagent: Self-updating library in large\nlanguage models improves chemical reasoning.arXiv preprint arXiv:2501.06590, 2025.\nZhengwei Tao, Ting-En Lin, Xiancai Chen, Hangyu Li, Yuchuan Wu, Yongbin Li, Zhi Jin, Fei\nHuang, Dacheng Tao, and Jingren Zhou. A survey on self-evolution of large language models.\narXiv preprint arXiv:2404.14387, 2024.\nKhanh-Tung Tran, Dung Dao, Minh-Duong Nguyen, Quoc-Viet Pham, Barry O\u2019Sullivan, and\nHoang D Nguyen. Multi-agent collaboration mechanisms: A survey of llms.arXiv preprint\narXiv:2501.06322, 2025.\nZiyu Wan, Yunxiang Li, Xiaoyu Wen, et al. Rema: Learning to meta-think for llms with multi-agent\nreinforcement learning.arXiv preprint arXiv:2503.09501, 2025.\nJunlin Wang, Jue Wang, Ben Athiwaratkun, et al. Mixture-of-agents enhances large language model\ncapabilities.arXiv preprint arXiv:2406.04692, 2024.\nXiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "preprint arXiv:2406.04692, 2024.\nXiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R\nLoomba, Shichang Zhang, Yizhou Sun, and Wei Wang. Scibench: Evaluating college-level scien-\ntific problem-solving abilities of large language models.arXiv preprint arXiv:2307.10635, 2023.\n13\nCoMAS\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\narXiv preprint arXiv:2203.11171, 2022.\nJunkang Wu, Yuexiang Xie, Zhengyi Yang, Jiancan Wu, Jinyang Gao, Bolin Ding, Xiang Wang,\nand Xiangnan He.\u03b2-dpo: Direct preference optimization with dynamic\u03b2.Advances in Neural\nInformation Processing Systems, 2024a.\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun\nZhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-\nagent conversations. InFirst Conference on Language Modeling, 2024b.\nZhangchen Xu, Yang Liu, Yueqin Yin, Mingyuan Zhou, and Radha Poovendran. Kodcode: A\ndiverse, challenging, and verifiable synthetic dataset for coding.arXiv preprint arXiv:2503.02951,\n2025.\nAn Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li,\nDayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang,\nJianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin\nYang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin\nYang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li,\nTingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan,\nYuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report.arXiv preprint\narXiv:2412.15115, 2024a.\nZiyi Yang, Zaibin Zhang, Zirui Zheng, Yuxian Jiang, Ziyue Gan, Zhiyu Wang, Zijian Ling, Jinsong\nChen, Martz Ma, Bowen Dong, et al. Oasis: Open agent social interaction simulations with one\nmillion agents.arXiv preprint arXiv:2411.11581, 2024b.\nRui Ye, Keduan Huang, Qimin Wu, Yuzhu Cai, Tian Jin, Xianghe Pang, Xiangrui Liu, Jiaqi Su,\nChen Qian, Bohan Tang, et al. Maslab: A unified and comprehensive codebase for llm-based\nmulti-agent systems.arXiv preprint arXiv:2505.16988, 2025.\nEdward Yeo, Yuxuan Tong, Morry Niu, Graham Neubig, and Xiang Yue. Demystifying long chain-\nof-thought reasoning in llms.arXiv preprint arXiv:2502.03373, 2025.\nQiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Weinan Dai, Tiantian\nFan, Gaohong Liu, Lingjun Liu, et al. Dapo: An open-source llm reinforcement learning system\nat scale.arXiv preprint arXiv:2503.14476, 2025.\nSiyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, and Deqing Yang. Evoa-\ngent: Towards automatic multi-agent generation via evolutionary algorithms.arXiv preprint\narX"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "5.\nSiyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Dongsheng Li, and Deqing Yang. Evoa-\ngent: Towards automatic multi-agent generation via evolutionary algorithms.arXiv preprint\narXiv:2406.14228, 2024a.\nWeizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Sainbayar Sukhbaatar, Jing Xu, and Jason\nWeston. Self-rewarding language models.arXiv preprint arXiv:2401.10020, 2024b.\nGuibin Zhang, Yanwei Yue, Xiangguo Sun, Guancheng Wan, Miao Yu, Junfeng Fang, Kun Wang,\nTianlong Chen, and Dawei Cheng. G-designer: Architecting multi-agent communication topolo-\ngies via graph neural networks.arXiv preprint arXiv:2410.11782, 2024a.\nGuibin Zhang, Muxin Fu, Guancheng Wan, Miao Yu, Kun Wang, and Shuicheng Yan. G-memory:\nTracing hierarchical memory for multi-agent systems.arXiv preprint arXiv:2506.07398, 2025a.\nGuibin Zhang, Hejia Geng, Xiaohang Yu, Zhenfei Yin, Zaibin Zhang, Zelin Tan, Heng Zhou,\nZhongzhi Li, Xiangyuan Xue, Yijiang Li, et al. The landscape of agentic reinforcement learning\nfor llms: A survey.arXiv preprint arXiv:2509.02547, 2025b.\nJenny Zhang, Shengran Hu, Cong Lu, Robert Lange, and Jeff Clune. Darwin godel machine: Open-\nended evolution of self-improving agents.arXiv preprint arXiv:2505.22954, 2025c.\nJiayi Zhang, Jinyu Xiang, Zhaoyang Yu, Fengwei Teng, Xionghui Chen, Jiaqi Chen, Mingchen\nZhuge, Xin Cheng, Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow genera-\ntion.arXiv preprint arXiv:2410.10762, 2024b.\n14\nCoMAS\nKai"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": ", Sirui Hong, Jinlin Wang, et al. Aflow: Automating agentic workflow genera-\ntion.arXiv preprint arXiv:2410.10762, 2024b.\n14\nCoMAS\nKaiyan Zhang, Runze Liu, Xuekai Zhu, Kai Tian, Sihang Zeng, Guoli Jia, Yuchen Fan, Xingtai\nLv, Yuxin Zuo, Che Jiang, Ziyang Liu, Jianyu Wang, Yuru Wang, Ruotong Zhao, Ermo Hua,\nYibo Wang, Shijie Wang, Junqi Gao, Xinwei Long, Youbang Sun, Zhiyuan Ma, Ganqu Cui, Lei\nBai, Ning Ding, Biqing Qi, and Bowen Zhou. Marti: A framework for multi-agent llm systems\nreinforced training and inference, 2025d. URLhttps://github.com/TsinghuaC3I/\nMARTI.\nQingyang Zhang, Haitao Wu, Changqing Zhang, Peilin Zhao, and Yatao Bian. Right question\nis already half the answer: Fully unsupervised llm reasoning incentivization.arXiv preprint\narXiv:2504.05812, 2025e.\nWenqi Zhang, Yongliang Shen, Weiming Lu, and Yueting Zhuang. Data-copilot: Bridging billions\nof data and humans with autonomous workflow.arXiv preprint arXiv:2306.07209, 2023.\nWentao Zhang, Ce Cui, Yilei Zhao, Rui Hu, Yang Liu, Yahui Zhou, and Bo An. Agentorches-\ntra: A hierarchical multi-agent framework for general-purpose task solving.arXiv preprint\narXiv:2506.12508, 2025f.\nZhenru Zhang, Chujie Zheng, Yangzhen Wu, Beichen Zhang, Runji Lin, Bowen Yu, Dayiheng Liu,\nJingren Zhou, and Junyang Lin. The lessons of developing process reward models in mathematical\nreasoning.arXiv preprint arXiv:2501.07301, 2025g.\nWanjia Zhao, Mert Yuksekgonul, Shirley Wu, and James Zou. Sirius: Self-improving multi-agent\nsystems via bootstrapped"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "arXiv:2501.07301, 2025g.\nWanjia Zhao, Mert Yuksekgonul, Shirley Wu, and James Zou. Sirius: Self-improving multi-agent\nsystems via bootstrapped reasoning.arXiv preprint arXiv:2502.04780, 2025a.\nXuandong Zhao, Zhewei Kang, Aosong Feng, Sergey Levine, and Dawn Song. Learning to reason\nwithout external rewards.arXiv preprint arXiv:2505.19590, 2025b.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and\nchatbot arena.Advances in neural information processing systems, 2023.\nJialun Zhong, Wei Shen, Yanzeng Li, Songyang Gao, Hua Lu, Yicheng Chen, Yang Zhang, Wei\nZhou, Jinjie Gu, and Lei Zou. A comprehensive survey of reward models: Taxonomy, applica-\ntions, challenges, and future.arXiv preprint arXiv:2504.12328, 2025.\nHeng Zhou, Hejia Geng, Xiangyuan Xue, et al. Reso: A reward-driven self-organizing llm-based\nmulti-agent system for reasoning tasks.arXiv preprint arXiv:2503.02390, 2025.\nWangchunshu Zhou, Yixin Ou, Shengwei Ding, Long Li, Jialong Wu, Tiannan Wang, Jiamin Chen,\nShuai Wang, Xiaohua Xu, Ningyu Zhang, et al. Symbolic learning enables self-evolving agents.\narXiv preprint arXiv:2406.18532, 2024.\nMingchen Zhuge, Wenyi Wang, Louis Kirsch, Francesco Faccio, Dmitrii Khizbullin, and J \u00a8urgen\nSchmidhuber. Gptswarm: Language agents as optimizable graphs. InForty-first International\nConference on Machine Learning, 2024.\nYuxin Zuo, Kaiyan"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "bullin, and J \u00a8urgen\nSchmidhuber. Gptswarm: Language agents as optimizable graphs. InForty-first International\nConference on Machine Learning, 2024.\nYuxin Zuo, Kaiyan Zhang, Li Sheng, Shang Qu, Ganqu Cui, Xuekai Zhu, Haozhan Li, Yuchen\nZhang, Xinwei Long, Ermo Hua, et al. Ttrl: Test-time reinforcement learning.arXiv preprint\narXiv:2504.16084, 2025.\n15\nCoMAS\nA THEUSE OFLLMS\nWe employ LLMs exclusively for manuscript refinement. Specifically, LLMs are utilized to correct\ntypographical errors, address grammatical issues, and enhance linguistic expression. All content\ngenerated or refined through LLMs has been rigorously reviewed and validated by the authors.\nLLMs are not involved in any other aspects of this research, including conceptual development, data\ncollection, code implementation, experimental design, or result interpretation.\nB DETAILS FORPARAMETERSETTINGS\nThe core parameter settings have been described in Section 4.1.1. Here, we provide all the details\nfor parameter settings when implementing our CoMAS framework.\nTable 2: The detailed parameter settings when implementing our CoMAS framework.\nParameter Setting\nFoundation model Qwen2.5-3B-Instruct\nNumber of trained agents 4\nNumber of solution rounds 8\nNumber of evaluation rounds 1\nHorizon for discussion history 2\nToken limit for prompts 28672\nToken limit for responses 4096\nTraining temperature 1.0\nEvaluation temperature 0.7\nDiscount factor 1.0\nClipping epsilon 0.2\nWeight of KL penalty 0.0\nNumber of training epochs 1\nNumber of prompt reuse 4\nMacro training batch size 64\nMicro training batch size 2\nMacro rollout batch size 64\nMicro rollout batch size 2\nOptimizer name AdamW\nLearning rate 1e-6\nWeight decay 0.0\nGradient norm 1.0\nGradient clipping True\nGradient checkpoint True\nFlash Attention True\nMixed precision True\nEnable vLLM True\nEnable DeepSpeed True\n16\nCoMAS\nC DETAILS FOREXPERIMENTALRESULTS\nDue to space constraints, our ablation studies in Section 4.3 only"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "Flash Attention True\nMixed precision True\nEnable vLLM True\nEnable DeepSpeed True\n16\nCoMAS\nC DETAILS FOREXPERIMENTALRESULTS\nDue to space constraints, our ablation studies in Section 4.3 only provide averages over all bench-\nmarks across different setups. Here we present the details for experimental results. Table 3 shows\nthe experimental results of using simplified reward formulations in CoMAS. Table 4 shows the ex-\nperimental results of using different numbers of agents in CoMAS. Table 5 shows the experimental\nresults of using homogeneous and heterogeneous agents in CoMAS.\nTable 3: Detailed experimental results for the ablation study on reward formulation. The results with\nperformance improvements are highlighted in green, drops in red, and neutral changes in gray.\nMethodDataset\nGSM8K MATH-500 HumanEval MBPP SciBench GPQA MMLUVanillaUntrained 84.00 51.40 68.90 54.00 32.67 26.79 61.40\nCoMAS (Ours)85.40(+1.40)52.80(+1.40)70.73(+1.83)56.20(+2.20)34.67(+2.00) 27.46 (+0.67)62.40(+1.00)\nw/o Evaluation 82.60 (-1.40) 50.60 (-0.80) 63.41 (-5.49) 55.00 (+1.00) 32.87 (+0.20) 27.01 (+0.22) 62.20 (+0.80)\nw/o Scoring 83.60 (-0.40) 50.60 (-0.80) 68.90 (+0.00) 55.40 (+1.40) 31.66 (-1.00)28.12(+1.34) 60.60 (-0.80)ConsistencyUntrained 85.40 55.00 73.78 55.80 36.47 28.79 63.20\nCoMAS (Ours)87.20(+1.80)55.80(+0.80)77.44(+3.66)59.20(+3.40)37.68(+1.20) 29.69 (+0.89)"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": ")87.20(+1.80)55.80(+0.80)77.44(+3.66)59.20(+3.40)37.68(+1.20) 29.69 (+0.89)65.60(+2.40)\nw/o Evaluation 85.00 (-0.40) 53.60 (-1.40) 71.34 (-2.44) 55.20 (-0.60) 36.87 (+0.40) 28.35 (-0.45) 63.20 (+0.00)\nw/o Scoring 85.80 (+0.40) 54.20 (-0.80) 71.95 (-1.83) 56.00 (+0.20) 36.27 (-0.20)29.91(+1.12) 62.40 (-0.80)\nTable 4: Detailed experimental results for the ablation study on agent number. The results with\nperformance improvements are highlighted in green, drops in red, and neutral changes in gray.\nMethodDataset\nGSM8K MATH-500 HumanEval MBPP SciBench GPQA MMLUVanillaUntrained 84.00 51.40 68.90 54.00 32.67 26.79 61.40\nCoMAS (1-Agent) 83.20 (-0.80) 50.40 (-1.00) 69.51 (+0.61) 53.00 (-1.00) 33.07 (+0.40)27.90(+1.12)62.80(+1.40)\nCoMAS (2-Agent) 83.40 (-0.60)52.80(+1.40) 67.68 (-1.22) 54.00 (+0.00) 33.87 (+1.20) 27.46 (+0.67) 62.20 (+0.80)\nCoMAS (4-Agent)85.40(+1.40)52.80(+1.40)70.73(+1.83)56.20(+2.20)34.67(+2.00) 27.46 (+0.67) 62.40 (+1.00)ConsistencyUntrained 85.40 55.00 73.78 55"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "20(+2.20)34.67(+2.00) 27.46 (+0.67) 62.40 (+1.00)ConsistencyUntrained 85.40 55.00 73.78 55.80 36.47 28.79 63.20\nCoMAS (1-Agent) 84.40 (-1.00) 55.20 (+0.20) 71.95 (-1.83) 55.80 (+0.00) 37.88 (+1.40) 25.67 (-3.12)66.20(+3.00)\nCoMAS (2-Agent) 85.80 (+0.40) 55.40 (+0.40) 74.39 (+0.61) 56.40 (+0.60)38.28(+1.80) 29.02 (+0.22) 65.20 (+2.00)\nCoMAS (4-Agent)87.20(+1.80)55.80(+0.80)77.44(+3.66)59.20(+3.40) 37.68 (+1.20)29.69(+0.89) 65.60 (+2.40)AutoGenUntrained 52.60 38.40 39.63 29.80 20.24 16.29 37.40\nCoMAS (1-Agent)73.40(+20.80) 44.20 (+5.80) 46.34 (+6.71)39.20(+9.40) 21.64 (+1.40) 18.30 (+2.01) 46.60 (+9.20)\nCoMAS (2-Agent) 71.20 (+18.60)45.80(+7.40)50.61(+10.98) 37.40 (+7.60)24.65(+4.41) 22.54 (+6.25) 50.40 (+13.00)\nCoMAS (4-Agent) 72.40 (+19.80)45.80(+7.40)50.61(+10.98) 38.00 (+8.20) 22.85 (+2.61)22.99(+6.70)50.60(+13.20)DebateUntrained"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "7.40)50.61(+10.98) 38.00 (+8.20) 22.85 (+2.61)22.99(+6.70)50.60(+13.20)DebateUntrained 84.60 55.00 71.34 54.80 38.68 28.35 62.80\nCoMAS (1-Agent) 85.20 (+0.60) 55.00 (+0.00) 70.73 (-0.61) 55.40 (+0.60) 36.67 (-2.00) 29.46 (+1.12) 62.00 (-0.80)\nCoMAS (2-Agent)85.60(+1.00)56.20(+1.20) 72.56 (+1.22)56.60(+1.80) 38.88 (+0.20)29.91(+1.56) 64.00 (+1.20)\nCoMAS (4-Agent) 85.20 (+0.60) 55.40 (+0.40)77.44(+6.10) 55.60 (+0.80)39.08(+0.40)29.91(+1.56)65.20(+2.40)\nTable 5: Detailed experimental results for the ablation study on agent diversity. The results with\nperformance improvements are highlighted in green, drops in red, and neutral changes in gray.\nMethodDataset\nGSM8K MATH-500 HumanEval MBPP SciBench GPQA MMLUVanillaUntrained 84.00 51.40 68.90 54.00 32.67 26.79 61.40\nHomogeneous 83.40 (-0.60) 52.80 (+1.40) 67.68 (-1.22) 54.00 (+0.00) 33.87 (+1.20) 27.46 (+0.67)62.20(+0.80)\nHeterogeneous85.20(+1.20)53.20(+1.80)73.78(+4.88)58.00(+4.00)34.47(+1.80)28.12(+1.34) 61.80 (+0.40)ConsistUntrained"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "1.80)73.78(+4.88)58.00(+4.00)34.47(+1.80)28.12(+1.34) 61.80 (+0.40)ConsistUntrained 85.40 55.00 73.78 55.80 36.47 28.79 63.20\nHomogeneous 85.80 (+0.40) 55.40 (+0.40) 74.39 (+0.61) 56.40 (+0.60) 38.28 (+1.80) 29.02 (+0.22) 65.20 (+2.00)\nHeterogeneous87.00(+1.60)57.40(+2.40)76.83(+3.05)60.20(+4.40)38.48(+2.00)30.58(+1.79)67.40(+4.20)AutoGenUntrained 52.60 38.40 39.63 29.80 20.24 16.29 37.40\nHomogeneous71.20(+18.60)45.80(+7.40)50.61(+10.98)37.40(+7.60)24.65(+4.41)22.54(+6.25)50.40(+13.00)\nHeterogeneous 63.00 (+10.40) 40.40 (+2.00) 48.17 (+8.54) 37.20 (+7.40)24.65(+4.41) 21.21 (+4.91) 46.20 (+8.80)DebateUntrained 84.60 55.00 71.34 54.80 38.68 28.35 62.80\nHomogeneous 85.60 (+1.00)56.20(+1.20) 72.56 (+1.22) 56.60 (+1.80) 38.88 (+0.20) 29.91 (+1.56) 64.00 (+1.20)\nHeterogeneous86.20(+1.60) 56.00 (+1.00)73.78(+2.44)57.80(+3.00)40.28(+1.60)31"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "(+1.20)\nHeterogeneous86.20(+1.60) 56.00 (+1.00)73.78(+2.44)57.80(+3.00)40.28(+1.60)31.03(+2.68)65.40(+2.60)\n17\nCoMAS\nD PROMPTTEMPLATES\nIn this section, we provide the prompt templates used in our CoMAS framework. Note that they\nmay vary slightly across different task domains. Here we present the science tasks as an example.\nPrompt Template for Solution\nThe problem is presented as follows:\n{problem}\nCurrent discussion on the problem is presented as follows for your reference:\n{discussion}\nProvide your step-by-step solution to the problem. The final answer should be a\ndecimal number enclosed within \\\\boxed{{}}, e.g. \\\\boxed{{1}}, \\\\boxed{{0.1}}, or\n\\\\boxed{{0.01}}. The unit part given in the problem should not be enclosed.,\u2192\n,\u2192\nPrompt Template for Evaluation\nThe problem is presented as follows:\n{problem}\nCurrent discussion on the problem is presented as follows for your reference:\n{discussion}\nYou are required to evaluate the following solution:\n{solution}\nYou should point out every possible error and defect in the solution. Provide your\nevaluation by listing all the mistakes you find in the solution, specifying what\nis wrong and why. Keep your evaluation concise and clear. Avoid using a lot of\nwords to retell the reasoning process.,\u2192\n,\u2192\n,\u2192\nPrompt Template for Scoring\nThe problem is presented as follows:\n{problem}\nYou are required to score the following solution:\n{solution}\nThe evaluation on the solution is presented as follows:\n{evaluation}\nYou should consider the rationality of the evaluation and score the solution. The\nscore should be an integer between 1 and 3 with the following standards: ,\u2192\n3: The solution is completely correct, and none of the mistakes mentioned in the\nevaluation is effective.,\u2192\n2: Some minor mistakes mentioned in the evaluation do exist, but they do not affect\nthe overall correctness.,\u2192\n1: Some of the mistakes mentioned in the evaluation are fatal, which directly lead to\nan incorrect answer.,\u2192\nYour score should be enclosed within \"<score>\" and \"</score>\" tags. You should also\nbriefly explain the reasons before providing your score. Keep your decision"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "in the evaluation are fatal, which directly lead to\nan incorrect answer.,\u2192\nYour score should be enclosed within \"<score>\" and \"</score>\" tags. You should also\nbriefly explain the reasons before providing your score. Keep your decision\nconcise and clear. Avoid using a lot of words to retell the reasoning process.,\u2192\n,\u2192\nFor example:\nThe calculation error mentioned in the evaluation cannot be ignored and leads to an\nincorrect answer.,\u2192\n<score>1</score>\nE EXAMPLETRAJECTORY\nIn this section, we present an example trajectory generated by our CoMAS framework to illustrate\nthe complete pipeline and enhance readers\u2019 understanding of the process.\n18\nCoMAS\nExample Trajectory\nQuestion:\nSuppose a copper wire (density = 8.96 g/cm\u02c63) has a diameter of 0.25 mm. If a sample of\nthis copper wire has a mass of 22 g, how long is the wire? ,\u2192\nThe unit of the answer should be \"cm\".\nSolution:\nTo find the length of the copper wire, we need to follow these steps:\n1. Calculate the volume of the copper wire:\nThe mass of the copper wire is given as 22 g, and the density of copper is given as\n8.96 g/cm\u00b3. The formula to find the volume V of an object is: V =\n\\frac{\\text{mass}}{\\text{density}},\u2192\n,\u2192\nSubstituting the given values: V = \\frac{22 \\text{ g}}{8.96 \\text{ g/cm}\u02c63}\nPerforming the division: V = 2.457 \\text{ cm}\u02c63\n2. Determine the volume of a single segment of the wire:\nThe wire is cylindrical in shape. The volume V of a cylinder is given by: V = \\pi\nr\u02c62 h,\u2192\nwhere r is the radius of the cylinder and h is the height (or length) of the\ncylinder. The diameter of the wire is 0.25 mm, so the radius r is half of the\ndiameter: r = \\frac{0.25 \\text{ mm}}{2} = 0.125 \\text{ mm} = 0.125 \\text{ cm},\u2192\n,\u2192\nSubstituting V and r into the volume formula: 2.457 \\"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "25 \\text{ mm}}{2} = 0.125 \\text{ mm} = 0.125 \\text{ cm},\u2192\n,\u2192\nSubstituting V and r into the volume formula: 2.457 \\text{ cm}\u02c63 = \\pi (0.125 \\text{\ncm})\u02c62 h,\u2192\n3. Solve for the length h:\nFirst, calculate (0.125 \\text{ cm})\u02c62: (0.125 \\text{ cm})\u02c62 = 0.015625 \\text{ cm}\u02c62\nNow, substitute this into the volume equation: 2.457 \\text{ cm}\u02c63 = \\pi \\times\n0.015625 \\text{ cm}\u02c62 \\times h,\u2192\nSolve for h by dividing both sides by \\pi \\times 0.015625 \\text{ cm}\u02c62: h =\n\\frac{2.457 \\text{ cm}\u02c63}{\\pi \\times 0.015625 \\text{ cm}\u02c62} ,\u2192\nUsing the approximation \\pi \\approx 3.14159: h = \\frac{2.457 \\text{ cm}\u02c63}{3.14159\n\\times 0.015625 \\text{ cm}\u02c62},\u2192\nPerform the multiplication in the denominator: 3.14159 \\times 0.015625 \\approx\n0.05000,\u2192\nNow, divide: h = \\frac{2.457}{0.05000} \\approx 49.14 \\text{ cm}\nTherefore, the length of the wire is \\boxed{49.14} cm.\nEvaluation:\nThe solution contains several errors:\n1. The volume calculation is incorrect because the given mass (22 g) is not the volume,\nbut the mass of the wire. The correct volume should be calculated as V = \\frac{22\n\\text{ g}}{8.96 \\text{ g/cm}\u02c63} \\approx 2.457 \\text{ cm}\u02c63.,\u2192\n,\u2192\n2. The radius calculation is incorrect. The diameter is given as 0.25 mm, which is\n0.025 cm, so the radius should be 0.0125 cm, not 0.125 cm. ,"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": ",\u2192\n2. The radius calculation is incorrect. The diameter is given as 0.25 mm, which is\n0.025 cm, so the radius should be 0.0125 cm, not 0.125 cm. ,\u2192\n3. The formula for the volume of a cylinder is V = \\pi r\u02c62 h, but the solution\nincorrectly uses V = \\pi r\u02c62 without the h term. ,\u2192\n4. The final calculation for h is incorrect. It should be h = \\frac{2.457 \\text{\ncm}\u02c63}{\\pi \\times (0.0125 \\text{ cm})\u02c62} \\approx 49.14 \\text{ cm}, not h =\n\\frac{2.457}{0.05000} \\approx 49.14 \\text{ cm}.,\u2192\n,\u2192\nThese errors lead to an incorrect final length of the wire.\nScoring:\nThe solution is fundamentally correct but contains a critical error in the final step\ndue to the incorrect value of \\pi. This error directly leads to an incorrect final\nlength of the wire. Therefore, the score is 1, indicating that some of the mistakes\nmentioned in the attack are fatal.,\u2192\n,\u2192\n,\u2192\n<score>1</score>\nSolution:\nLet's go through the problem step-by-step to find the correct length of the copper\nwire.,\u2192\nStep 1: Calculate the Volume of the Wire\nThe volume V of the wire can be calculated using the formula: V = \\frac{m}{\\rho}\nSubstituting the given values: V = \\frac{22 \\text{ g}}{8.96 \\text{ g/cm}\u02c63} \\approx\n2.457 \\text{ cm}\u02c63,\u2192\nStep 2: Relate the Volume to the Cross-Sectional Area and Length\nThe volume V of a cylinder (which the wire can be considered as) is given by: V = A\n\\times h,\u2192\nwhere A is the cross-sectional area and h is the length of the wire.\nThe cross-sectional area A of a circular wire is: A = \\pi \\left(\n\\frac{\\text{diameter}}{2} \\right)\u02c62,\u2192\nGiven the diameter of the wire is 0.25 mm, convert it"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "area A of a circular wire is: A = \\pi \\left(\n\\frac{\\text{diameter}}{2} \\right)\u02c62,\u2192\nGiven the diameter of the wire is 0.25 mm, convert it to cm: \\text{diameter} = 0.25\n\\text{ mm} = 0.025 \\text{ cm},\u2192\nSo, the radius r is: r = \\frac{0.025 \\text{ cm}}{2} = 0.0125 \\text{ cm}\nNow, calculate the cross-sectional area A: A = \\pi \\left( 0.0125 \\text{ cm} \\right)\u02c62 =\n\\pi \\times 0.00015625 \\text{ cm}\u02c62 \\approx 0.000490873852123405 \\text{ cm}\u02c62 ,\u2192\n19\nCoMAS\nStep 3: Solve for the Length of the Wire\nUsing the volume formula V = A \\times h, solve for h: h = \\frac{V}{A}\nSubstitute the values of V and A: h = \\frac{2.457 \\text{ cm}\u02c63}{0.000490873852123405\n\\text{ cm}\u02c62} \\approx 499.9999999999999 \\text{ cm} ,\u2192\nStep 4: Round to a Reasonable Number of Significant Figures\nGiven the precision of the provided values, we can round the length to a reasonable\nnumber of significant figures. The length is already very close to 5000 cm, so we\ncan simply state: h \\approx 5000 \\text{ cm},\u2192\n,\u2192\nThus, the length of the wire is: \\boxed{5000}\nEvaluation:\nHere are the mistakes in the solution:\n1. Volume Calculation: The volume calculation is correct, but the intermediate step of\nconverting the diameter to cm is unnecessary and complicates the calculation. The\ndiameter is 0.25 mm, which is 0.025 cm, and the radius is 0.0125 cm. The\ncross-sectional area should be calculated as A = \\pi (0.0125)\u02c62 \\approx 4.91 \\times\n10\u02c6{-5} \\text{ cm}\u02c62.,\u2192\n,\u2192"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "cross-sectional area should be calculated as A = \\pi (0.0125)\u02c62 \\approx 4.91 \\times\n10\u02c6{-5} \\text{ cm}\u02c62.,\u2192\n,\u2192\n,\u2192\n,\u2192\n2. Length Calculation: The length of the wire should be calculated as h = \\frac{V}{A} =\n\\frac{2.457 \\text{ cm}\u02c63}{4.91 \\times 10\u02c6{-5} \\text{ cm}\u02c62} \\approx\n49999.99999999999 \\text{ cm}. Rounding to a reasonable number of significant\nfigures, the length is approximately 50000 cm or 5000 cm.,\u2192\n,\u2192\n,\u2192\n3. Units Consistency: The final answer should be rounded to a sensible number of\nsignificant figures. Given the precision of the provided values, 5000 cm is a\nreasonable and accurate answer.,\u2192\n,\u2192\nSo, the correct length of the wire is: \\boxed{5000}\nScoring:\nThe solution contains minor mistakes that are not fatal to the overall correctness. The\nvolume calculation and cross-sectional area calculation are correct, and the length\ncalculation, while slightly rounded, is reasonable and does not affect the final\nanswer.,\u2192\n,\u2192\n,\u2192\n<score>2</score>\nSolution:\nTo solve this problem, we need to determine the length of the copper wire given its\nmass and diameter. Let's follow the steps to find the solution. ,\u2192\nStep 1: Determine the Volume of the Copper Wire\nFirst, convert the diameter from millimeters to centimeters: \\text{Diameter} = 0.25 \\,\n\\text{mm} = 0.025 \\, \\text{cm},\u2192\nThe radius (r) of the wire is half of the diameter: r = \\frac{0.025}{2} \\, \\text{cm} =\n0.0125 \\, \\text{cm},\u2192\nThe cross-sectional area (A) of the wire can be calculated using the formula for the\narea of a circle: A = \\pi r\u02c62 = \\pi (0.0125)\u02c62 \\, \\text{cm}\u02c62 \\approx 0.00050685 \\,\n\\text{cm}"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "area of a circle: A = \\pi r\u02c62 = \\pi (0.0125)\u02c62 \\, \\text{cm}\u02c62 \\approx 0.00050685 \\,\n\\text{cm}\u02c62,\u2192\n,\u2192\nStep 2: Calculate the Length of the Wire\nUsing the density formula: \\rho = \\frac{m}{V}\nRearrange the formula to solve for the volume (V): V = \\frac{m}{\\rho}\nSubstitute the given values: V = \\frac{22 \\, \\text{g}}{8.96 \\, \\text{g/cm}\u02c63} \\approx\n2.455 \\, \\text{cm}\u02c63,\u2192\nStep 3: Relate Volume to Length\nThe volume of a cylinder (wire) is given by: V = A \\times L\nSolve for the length (L): L = \\frac{V}{A} = \\frac{2.455 \\, \\text{cm}\u02c63}{0.00050685 \\,\n\\text{cm}\u02c62} \\approx 4840 \\, \\text{cm},\u2192\nFinal Answer: \\boxed{4840}\nEvaluation:\nThe solution contains a significant error in the calculation of the volume of the wire.\nSpecifically:,\u2192\nThe volume calculation is incorrect because the cross-sectional area A = \\pi r\u02c62 should\nbe squared, but the final volume calculation does not reflect this. The correct\narea is A = \\pi (0.0125)\u02c62 \\approx 0.00049087 \\, \\text{cm}\u02c62, and the correct\nvolume is V = 0.00049087 \\times 2.455 \\approx 0.00121 \\, \\text{cm}\u02c63.,\u2192\n,\u2192\n,\u2192\nThe length L should then be calculated as L = \\frac{0.00121 \\, \\text{cm}\u02c63}{0.00050685\n\\, \\text{cm}\u02c62} \\approx 2.38 \\, \\text{cm}.,\u2192\nTherefore, the correct length of the wire is approximately 2.38 \\, \\text{cm}.\nScoring:\nThe original solution is completely correct, and none of the"}
{"file": "2510.08529v1.pdf", "section": "7 REPRODUCIBILITYSTATEMENT", "chunk": "approx 2.38 \\, \\text{cm}.,\u2192\nTherefore, the correct length of the wire is approximately 2.38 \\, \\text{cm}.\nScoring:\nThe original solution is completely correct, and none of the mistakes mentioned in the\nattack are effective. The volume and length calculations are accurate, and the\nfinal length of the wire is 4840 \\, \\text{cm}.,\u2192\n,\u2192\n<score>3</score>\n20"}
{"file": "2510.08567v1.pdf", "section": "Introduction", "chunk": "Preprint\nMATRIX: Multimodal Agent Tuning for\nRobust Tool-Use Reasoning\nTajamul Ashraf\u22171\u0000, Umair Nawaz1, Abdelrahman M. Shaker1, Rao Anwer1,\nPhilip Torr2,Fahad Shahbaz Khan1,Salman Khan1\n1Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), United Arab Emirates\n2University of Oxford, United Kindgom\nhttps://mbzuai-oryx.github.io/MATRIX\nTask: I want to buy a dog toy for each dog in the photo. How\nmany dollars will I need to spend in total?\nGround Truth:\n79.96 dollars\nQwen2-VL: I want to buy a dog toy for each dog in the photo.\nHow many dollars will I need to spend in total?\\nAttachment:\nimage11.jpg, mage_12.jpg\"},\n.\nObservation: None\nLow Answer Accuracy\nX\nX\nMAT: I need to determine the number of dogs in the\nprovided images and then calculate the total cost\nfor purchasing a dog toy for each of them. I will use\nthe `visualizer` tool to locate the dogs in the\nimages and then sum the costs accordingly.\nFinal Answer: 159.92 dollars\nHighest Answer Accuracy\nTool-Augm ented CoT\nReasoning Consistency\nLower Hallucinations\nMATRIX: Now that I have determined there are 4\ndogs in the image, I'll calculate the total cost by\nmultiplying the number of dogs by the price of the\ndog toy, which is $19.99 each....\nQWEN2-VL\nMAT\nMATRIX (ours)\nMATRIX: To calculate the total cost, I will first count\nthe number of dogs in `image_12.jpg` using object\ndetection. Then, I will multiply that by the price of the\ndog toy ($19.99), which is given in `image_11.jpg`.\nPythonInterpreter: Count the number of dogs in the image.\nCode:\\n```py\\n#\nCount the number of dogs in the image.\nboxes = objectlocation(object=\\\"dog\\\",\nimage_path=\\\"image_12.jpg\\\")\\n\nprint(len(boxes))\\n```\nPythonInterpreter:\nCode:\\n```py\\n#\nCalculate total cost\\nprice = 19.99\\n\nnum_dogs = 4\\n\ntotal_cost"}
{"file": "2510.08567v1.pdf", "section": "Introduction", "chunk": "=\\\"image_12.jpg\\\")\\n\nprint(len(boxes))\\n```\nPythonInterpreter:\nCode:\\n```py\\n#\nCalculate total cost\\nprice = 19.99\\n\nnum_dogs = 4\\n\ntotal_cost = price * num_dogs\\n\nprint(round(total_cost, 2))\\n```\nMATRIX: [OUTPUT] -> 79.96\nFinal Answer: The total cost is 79.96 dollars.\nObservation: Error: Code execution failed due to\nthe following error: EXECUTION FAILED: Evaluation\nstopped at line 'from mtcnn.mtcnn import MTCNN'\nMAT: It seems there was an error while trying to\naccess the image file. I will use the `visualizer`\ntool to analyze the image directly without loading\nit. I will ask the tool to identify dogs in the first\nimage and then proceed with the same method.....\nObservation: Analyzing the first image for\ndogs\\ndog_bboxes_image_1.2.3....4]. The image\ncontain four dogs.\nMAT: Each dog toy costs $19.99. I will calculate the\ntotal cost based on this\nassumption.\\n\\nCode:\\n```py\\n# Assuming there\nare 4 dogs in each image\\nnumber_of_dogs = 4 * 2\n# 4 dogs in the first image and 4 in the\nsecond\\ncost_per_toy = 19.99\\n\\n# 159.92 total cost\n....\nLower Hallucinations\nAnswer Accuracy\nTool-Augmented CoT\nReasoning Consistency\nLower Hallucinations\nObservation: Observation:\\nExecution failed: EXECUTION\nFAILED:\\nEvaluation stopped at line 'with\nopen('.cache/image_11.jpg', 'r') as f:\\n  image_info = f.read()'\nbecause of the following error:\\nIt is not permitted to\nevaluate ot ...}\nFinal Answer: {'ok': False, 'error': \\\"EXECUTION\nFAILED:\\\\nEvaluation stopped at line ....}\nObservation: Verified 4 dogs and total cost is 79.96\nTool-Augm ented CoT\nReasoning Consistency\nQwen2-VL: To solve this task, I need to determine the\nnumber of dogs in the photo and multiply it by the price of\nthe dog toy. I will use the `inspect_file_as_text` tool"}
{"file": "2510.08567v1.pdf", "section": "Introduction", "chunk": "ing Consistency\nQwen2-VL: To solve this task, I need to determine the\nnumber of dogs in the photo and multiply it by the price of\nthe dog toy. I will use the `inspect_file_as_text` tool to\nextract the number of dogs from the image information\nand the `final_answer` tool to calculate the total\ncost.\\nCode:\\n```py\\n# Inspect the image information to\nfind the number of dogs\\nwith open('.cache/image_11.jpg',\n'r') as f:\\n  image_info = f.read()\\n\\nnumber_of_dogs =\nint(image_info.split(' ')[1])\\n\\n# Calculate the total\ncost\\nprice_per_toy = 19.99\\ntotal_cost = number_of_dogs *\nprice_per_toy\\n\\n# Use final_answer tool to provide the total\ncost\\nfinal_answer(total_cost)\\n```\"},\nX\nX\nX\nX\nX\nFigure 1: Comparison of baseline Qwen2-VL (Yang et al., 2024), MAT (Gao et al., 2025b),\nand proposed MATRIX agent on a visual reasoning task. MATRIX shows superior tool\nuse, fewer hallucinations, and more consistent reasoning, while Qwen2-VL and MAT often\nstruggle with tool coordination and fallback strategies.\nAbstract\nVision language models ( VLMs) are increasingly deployed as controllers with\naccess to external tools for complex reasoning and decision-making, yet their\neffectiveness remains limited by the scarcity of high-quality multimodal\ntrajectories and the cost of manual annotation. We address this challenge\nwith a vision-centric agent tuning framework that automatically synthesizes\nmultimodal trajectories, generates step-wise preference pairs, and trains a\nVLMcontroller for robust tool-use reasoning. Our pipeline first constructs\nM-TRACE, a large-scale dataset of 28.5K multimodal tasks with 177K verified\ntrajectories, enabling imitation-based trajectory tuning. Building on this, we\ndevelopMATRIXAgent, a controller finetuned on M-TRACE for step-wise tool\nreasoning. To achieve finer alignment, we further introduce Pref-X, a set of\n11K automatically generated preference pairs, and optimize MATRIXon it via\nstep-wise preference learning. Across three benchmarks, Agent-X, GTA, and\nGAIA,MATRIXconsistently surpasses both open- and closed-source VLMs,\nd"}
{"file": "2510.08567v1.pdf", "section": "Introduction", "chunk": "11K automatically generated preference pairs, and optimize MATRIXon it via\nstep-wise preference learning. Across three benchmarks, Agent-X, GTA, and\nGAIA,MATRIXconsistently surpasses both open- and closed-source VLMs,\ndemonstrating scalable and effective multimodal tool use. Our data and\ncode is avaliable athttps://github.com/mbzuai-oryx/MATRIX\n\u2217\u0000Corresponding author:tajamul.ashraf@mbzuai.ac.ae\n1arXiv:2510.08567v1  [cs.CV]  9 Oct 2025\nPreprint\n1 Introduction\nVision language models ( VLMs) augmented with external tools are increasingly used as\ncontrollers for complex reasoning and decision-making tasks (Gao et al., 2024; Sur\u00eds et al.,\n2023; Gupta & Kembhavi, 2023; Yuan et al., 2024). Acting as central planners, they\ninvoke diverse tools through structured prompts, enabling applications in visual editing (Wu\net al., 2023), embodied control (ichter et al., 2023), question answering (Shen et al., 2023),\nvideo reasoning (Fan et al., 2024), and desktop automation (Trivedi et al., 2024). Existing\napproaches improve tool use by fine-tuning controllers on trajectories collected via manual\nannotation or closed-source APIs. However, such data are costly to obtain and often biased\ntoward narrow environments, which limits their generalization to broader multimodal tasks.\nLimitations of existing approaches.Most existing agents are trained with supervised\nfine-tuning ( SFT) on curated tool-use demonstrations (Peng et al., 2023; Wang et al., 2024d;\nSun et al., 2024). Such datasets are expensive to collect, difficult to scale, and biased toward\nnarrow domains or tool distributions. Reliance on static demonstrations further constrains\ngeneralization: if the examples emphasize only certain tools (e.g., video_parser ,image_qa ),\nagents often fail when confronted with tasks requiring different capabilities, such as live\nweb search or object grounding. As a result, models overfit to specific usage patterns and\nstruggle in unseen multimodal contexts. Recent works (Gao et al., 2025b; Li et al., 2025)\nexp"}
{"file": "2510.08567v1.pdf", "section": "1 Introduction", "chunk": "web search or object grounding. As a result, models overfit to specific usage patterns and\nstruggle in unseen multimodal contexts. Recent works (Gao et al., 2025b; Li et al., 2025)\nexplore synthetic generation to reduce annotation cost, but key challenges remain. Generated\ntrajectories are inconsistent in quality, exploration is limited, and evaluations are restricted\nto narrow domains. Moreover, imitation learning alone cannot teach agents to refine tool\nusage or recover from partially correct rollouts, since it lacks reinforcement-based feedback.\nOurinsight: robustagentsneedbothtracesandpreferences.Thecentralchallengein\ntraining tool-using agents lies in bridging two gaps: (i) acquiring fundamental tool-usage skills\nfrom scarce, high-quality trajectories, and (ii) refining these skills to handle the ambiguities,\nerrors, and partial successes that naturally occur in open-ended reasoning. Existing methods\neither focus solely on imitation, which cannot teach recovery or refinement, or rely on\ntrajectory-level labels, which are too coarse to capture step-wise decision quality. Our key\ninsight is that these challenges require a staged approach.First,large-scale supervised traces\nare essential to ground a VLMcontroller in multimodal reasoning and tool invocation.Second,\nonce the model can follow trajectories, its limitations are best addressed through step-level\npreference optimization, where alternative actions are explored, compared, and refined. This\npairing of supervised grounding with preference-based alignment enables agents not only to\nexecute valid tool calls but also to select the most effective ones in complex reasoning chains.\nOur framework.We introduce MATRIX, a two-stage framework that first equips a controller\nwith supervised tool-use skills and then refines its decision-making through preference opti-\nmization. (1)Trajectory-driven SFT:We construct M-TRACE, a dataset of 28.5K multimodal\ntasks with 177K verified step-level tool-use trajectories, providing broad coverage of tool\nreasoning skills. (2)Preference optimization:We build Pref-X, 11K automatically generated\npreference pairs from step-level exploration and verification, and apply Direct Preference\nOptimization (DPO) (Kong et al., 2025) to align the controller with fine-grained tool-use\npreferences. This staged design grounds the agent in verified traces while enabling progressive\nimprovement through self-exploration and automatic verification. We evaluate MATRIXon\nthree challenging benchmarks"}
{"file": "2510.08567v1.pdf", "section": "1 Introduction", "chunk": "2025) to align the controller with fine-grained tool-use\npreferences. This staged design grounds the agent in verified traces while enabling progressive\nimprovement through self-exploration and automatic verification. We evaluate MATRIXon\nthree challenging benchmarks, Agent-X (Ashraf et al., 2025), GTA (Wang et al., 2024b),\nand GAIA (Mialon et al., 2023), where it improves answer accuracy by 14%, 23%, and 11%,\nrespectively. As shown in Fig. 1, MATRIXachieves consistent reasoning and more adaptive\ntool selection compared to prior agents. Our main contributions are as follows:\n1.M-TRACE : a large-scale corpus of 28.5K multimodal tasks and 177K verified tool-use\ntrajectories built via automated synthesis and verification.\n2.Pref-X : 11K preference-labeled step pairs that enable fine-grained alignment of tool-usage\ndecisions beyond imitation learning.\n3.MATRIX : a robust, vision-centric agent that leverages trajectory supervision with preference\noptimization for efficient tool-use reasoning.\n2\nPreprint\n4.We show consistent improvements over strong baselines on Agent-X, GTA, and GAIA.\nTogether, these contributions establish MATRIXas a scalable and effective agent for training\nrobust multimodal agents.\n2 Related Work\nMultimodal Agents.The rapid progress of large multimodal models ( LMMs) (Achiam\net al., 2023; Grattafiori et al., 2024; Team et al., 2023; Bi et al., 2024; Bai et al., 2023) has\nenabled agents that integrate perception, reasoning, and external tools. Moving beyond\ntext generation, modern agents act as central planners by invoking APIs (Zhang et al.,\n2025), operating systems (Mei et al., 2024), document analyzers (Musumeci et al., 2024), or\nweb environments (Song et al., 2024), supporting broad interaction with digital ecosystems.\nThis has fueled the rise of orchestration frameworks such as Avatar(Wu et al., 2024),\nLangChain (Chase, 2022), and AutoGPT (Gravitas, 2023), which couple reasoning with\ntool execution. Specialized systems extend these capabilities to web browsing (Yao et al.,\n2022; Nak"}
{"file": "2510.08567v1.pdf", "section": "2 Related Work", "chunk": "LangChain (Chase, 2022), and AutoGPT (Gravitas, 2023), which couple reasoning with\ntool execution. Specialized systems extend these capabilities to web browsing (Yao et al.,\n2022; Nakano et al., 2021; Qin et al., 2023), REST APIs (Song et al., 2023), or multi-model\ncollaboration(Shenetal.,2024;Lietal.,2023). Multimodalvariantssuchas MLLMTool (Wang\net al., 2025) combine vision and language for perception-driven reasoning. Despite these\nadvances, most frameworks lack systematic training and evaluation protocols for sequential\ntool reasoning, limiting their robustness in open-ended multimodal environments.\nTool Usage Datasets.Datasets for tool-using agents have primarily targeted text-based\nsettings (Tang et al., 2023; Qin et al., 2024; Du et al., 2024; Liu et al., 2024b). Multimodal\nbenchmarks including Agent-X (Ashraf et al., 2025), OSWorld (Xie et al., 2024), MMInA(Zhang\net al., 2024b), GAIA(Mialon et al., 2023), and GTA(Wang et al., 2024b) broaden coverage\nbut still rely heavily on curated or repurposed trajectories. Existing tuning methods fall into\ntwo paradigms: supervised fine-tuning ( SFT) on annotated tool-use traces (Shen et al., 2023;\nLiu et al., 2024b), which is costly and brittle, or reinforcement learning (RL) with synthetic\nrewards or preferences (Lee et al., 2024; Fu et al., 2024; Yu et al., 2024), which assume reliable\nreward signals. Recent work explores step-wise preferences (Lai et al., 2024; Wang et al.,\n2024a; Chen et al., 2024a; Kong et al., 2025), but applications remain narrow (e.g., code or\nmath) where ground-truth labels exist. In contrast, MATRIXintroduces scalablestep-wise\npreference optimization with AI-based verification, where the agent generates, evaluates, and\nimproves its own trajectories. This reduces reliance on manual annotation and enables robust\nmultim"}
{"file": "2510.08567v1.pdf", "section": "2 Related Work", "chunk": "labels exist. In contrast, MATRIXintroduces scalablestep-wise\npreference optimization with AI-based verification, where the agent generates, evaluates, and\nimproves its own trajectories. This reduces reliance on manual annotation and enables robust\nmultimodal tool-use reasoning in diverse environments.\n3 MATRIX Agent\nMATRIXis a vision-centric multimodal agent built to perform reliable step-wise reasoning and\ntool use. The key challenge for such agents lies in the scarcity of high-quality trajectories\nand the cost of manual annotations, which limit scalability and generalization. To overcome\nthis, we design a two-stage training framework that leverages trajectory supervision with\npreference optimization. In the first stage, supervised fine-tuning ( SFT) on automatically\nsynthesized multimodal trajectories ( M-TRACE) equips the controller with structured tool-use\nskills. In the second stage, preference optimization via Direct Preference Optimization\n(DPO) (Kong et al., 2025) on step-level exploration data ( Pref-X) refines decision-making\nbeyond imitation, encouraging the agent to favor accurate, consistent, and goal-directed\nactions. The overall framework is illustrated in Fig. 2.\n3.1M-TRACEFormulation\nPipeline Overview.Our M-TRACE synthesis pipeline (Fig. 2) consists of four stages:\n(1)query generation, (2)artifact construction, (3)trajectory collection, and (4)parallel\nverification. To ensure reliability, we include two parallel verifiers: aquery\u2013artifact verifier\nthat checks task feasibility and input alignment, and atrajectory verifierthat validates\ntool-use consistency.\n3\nPreprint\nQuery\nGenerator\nPROMPT\nContent\nGeneration\nMultimodel\nArtifact\nConstruction\nFile\nCombinations\nImage Database\nMulti-Modal files\nTrajectory\nCollection\nFiles to MTrace\nChecker\nParallel\nVerification\nM-TRACE\nTrajectory Dataset\nSTAGE 1:  CoT - SFT\nSTAGE 2: Step wise preference tuning on Pref-X\nTask: How many grams\nwill I take in if I drink\ntwo drinks of this 12 oz\ncan a day like that in a\npicture for a week?\nDPOParallel\nSampling\nMATRIX\nAction 1.1\nAction 1.2\nTask\nTask\nPreferred\nDis-Preferred\nQUERIES\nTools list\nThought 1\nCode 1\nThought 2"}
{"file": "2510.08567v1.pdf", "section": "3 MATRIX Agent", "chunk": ", we iteratively prompt GPT-4o-mini (Hurst\net al., 2024) with tool descriptions and structured templates to generate diverse and\nexecutable queries, using hyperparameters (e.g., temperature) to promote diversity.\n2.Artifact Construction.Unlike prior works that sample files first, we adopt a query-\nfirst strategy, ensuring tighter alignment between queries and resources. This is\ncrucial as real tasks often require heterogeneous inputs (e.g., DOCX, PPTX, XLSX,\nPDF) and multiple resources. For each query, GPT-4o-mini specifies the file type\nand draft content; images are retrieved via BGE (Chen et al., 2024b) embeddings\nwith similarity search, while non-image files are programmatically generated.\n3.Trajectory Collection.A zero-shot ReAct-style agent (Yao et al., 2023a) powered\nby GPT-4o-mini generates multi-step trajectories. Given a query and artifacts,\nthe agent produces step-wisethoughts, executabletool calls, and corresponding\nobservations. Only valid executions are retained, ensuring high-quality reasoning\ntraces.\n4.Parallel Verification.Two verifiers ensure robustness: (i) thequery\u2013artifact verifier\nchecks task feasibility and input relevance, and (ii) thetrajectory verifiervalidates\ntool usage, arguments, and outputs. Following prior verification protocols (Liu\net al., 2024b; Wang et al., 2023; Gao et al., 2025b), GPT-4o-mini filters noisy or\ninconsistent samples, discarding ill-posed queries and trajectories.\n4\nPreprint\n.jpg\n20.2%\n.pdf\n18.3%\n.xlsx\n14.5%.png\n12.1%Food\n7.9%\n.docx\n5%Travel\n3.7%\nEntertainment\n3.4%Social\n3.3%\nOther\n2.3%.mp4\n1.7%.pptx\n0.9%\n(a)\n(b)\nFile inspector 12.7 %\nImage Editing 3.2 %\nVisual\nSegmentation7.3 %\nImage\nGeneration3.3 %\nObject\nLocalization12.8 %\nFace Detection 4.0 %\nImage Question\nAnswering21.4 %\nWeb Search 35.3 %\n(c)\n(d)\nFigure 3: Statistics of M-TRACE."}
{"file": "2510.08567v1.pdf", "section": "3 MATRIX Agent", "chunk": "Generation3.3 %\nObject\nLocalization12.8 %\nFace Detection 4.0 %\nImage Question\nAnswering21.4 %\nWeb Search 35.3 %\n(c)\n(d)\nFigure 3: Statistics of M-TRACE. (a) File-type distribution, (b) Domain coverage, (c) Tool\nusage, (d) Step complexity.\nData Sources.To diversify visual context, we collect \u223c100K image\u2013caption pairs from\neight datasets: COCO (Lin et al., 2014), ChartQA (Masry et al., 2022), LLaVA (Liu et al.,\n2024a), SAM (Kirillov et al., 2023), TextVQA (Singh et al., 2019), WebCelebrity (Liu et al.,\n2015), Web-Landmark (Weyand et al., 2020), and WikiArt (Saleh & Elgammal, 2015). We\nfurther enrich coverage with ShareGPT4V (Zhang et al., 2024a) captions, spanning charts,\ndocuments, science QA, visual reasoning, and art.\nM-TRACEAnalysis.After verification, M-TRACE yields 28.5K multimodal tasks with 27.5K\nassociated artifacts, distilled from 43.5K initial candidates. These tasks produce 177K verified\ntrajectories, ensuring both scale and quality. The dataset is diverse across several dimensions:\n(i)File types: M-TRACE spans over 10 formats\u2014including images, documents, spreadsheets,\naudio, video, and slides\u2014capturing realistic multimodal contexts, with additional coverage of\nformats like HTML and JSON (Fig. 3a). (ii)Knowledge domains:Tasks cover 16 categories\nsuch as finance, health, culture, environment, and history, ensuring broad topical coverage\n(Fig. 3b). (iii)Tool usage:Trajectories invoke a wide range of tools, with web search\nmost common, followed by image QA, file inspection, visualization, and Python execution,\nmirroring real-world problem solving (Fig. 3c). (iv)Step complexity:Tasks vary in reasoning\ndepth, with most requiring 2\u20135 steps and some up to 9, reflecting both practical and complex\nreasoning cases (Fig. 3d).\n3.2 Stage 1: Supervised Fine-Tuning"}
{"file": "2510.08567v1.pdf", "section": "3 MATRIX Agent", "chunk": "Tasks vary in reasoning\ndepth, with most requiring 2\u20135 steps and some up to 9, reflecting both practical and complex\nreasoning cases (Fig. 3d).\n3.2 Stage 1: Supervised Fine-Tuning (SFT) with Tool-Use Reasoning\nStep-wise reasoning with ReAct.We use Qwen2-VL-7B (Yang et al., 2024) as the\ncontroller, an open-source VLMwith integrated vision-language grounding. To equip the\ncontroller with structured tool-usage skills, we adopt the ReAct paradigm (Yao et al., 2023b),\nwhere reasoning unfolds step by step. At each step i, the controller first generates athought ti\n(a natural language plan) and then produces a correspondingcode snippet cito invoke a tool.\nCompared with fixed formats (e.g., JSON), Python-style code provides greater flexibility for\ndiverse input\u2013output types and seamless integration with real tools.\n5\nPreprint\nFormally, given a query Q, optional external resources F\u2217, and historyHi=\n{t1,c1,o1,...,ti\u22121,ci\u22121,oi\u22121}, the controller chooses a tool and arguments by maximiz-\ning:\nt\u22c6\ni,c\u22c6\ni= arg max\nti,ciP\u03b8(ti,ci|Q,F\u2217,Hi),(2)\nwhereoiis the observed outcome of executingc i.\nTool integration.Unlike symbolic simulations, the agent executesreal toolsspanning\ncategories such as web search, visual perception, image generation/editing, file inspection,\nmultimodal reasoning, and a broad set of Python libraries (see Tab. 5). This setup grounds\nreasoning in executable actions, ensuring realistic trajectories and better generalization to\npractical tasks. Further implementation details are provided in Appendix \u00a7A.1.\nTraining objective.Given a trajectory \u03c4={t1,c1,o1,...,tn,cn,on}paired with query Q\nand resourcesF\u2217, the controller is optimized with a step-level cross-entropy objective:\nLSFT=E (Q,F\u2217,\u03c4,A)\u223cD M-TRACE/bracketleft\uf8ecig\n\u2212n/summationdisplay\ni=1logP\u03b8(ti,ci|Q,F\u2217,Hi)/bracketright\uf8ecig\n.(3)\nCr"}
{"file": "2510.08567v1.pdf", "section": "3.2 Stage 1: Supervised Fine-Tuning (SFT) with Tool-Use Reasoning", "chunk": "\ufffdD M-TRACE/bracketleft\uf8ecig\n\u2212n/summationdisplay\ni=1logP\u03b8(ti,ci|Q,F\u2217,Hi)/bracketright\uf8ecig\n.(3)\nCrucially, thefinalanswer Aisnot supervised, forcingthecontrollertorelyontoolinteractions\nrather than memorized knowledge. This design grounds the model in executable tool use\nand sets the stage for further refinement with preference optimization.\n3.3 STAGE 2: Preference Tuning\nWhileSFTequips the controller with high-quality demonstrations, it remains restricted to\nimitation, limiting the ability to refine tool usage, recover from partially correct rollouts,\nor adapt beyond static trajectories. To overcome this, we leverage step-wise preference\noptimization on 11K preference pairs ( Pref-X), enabling the agent to compare candidate\nactions and learn to favor accurate, consistent, and semantically useful behaviors. This\nreinforcement-style refinement improves robustness and adaptability across multimodal tasks\n(see Fig. 4).\nFormulation.As in Stage 1, we adopt the ReAct framework (Yao et al., 2023b), where\nat stepi, the agent generates an action ai= (ti,ci)consisting of a natural-language\nthoughttiand executablecode ci. Given queryQ, optional artifacts F\u2217, historyhi=\n{t1,c1,o1,...,ti\u22121,ci\u22121,oi\u22121}, and tool setT, the controller selects:\nt\u22c6\ni,c\u22c6\ni= arg max\nti,ci\u03c0\u03b8(ti,ci|Q,F\u2217,hi,T),(4)\nwhere\u03c0\u03b8is the Stage 1 SFT-initialized controller.\nPreference data synthesis.We construct Pref-X, a dataset of 11K preference pairs.\nStarting from seed queries in M-TRACE, anLLM(e.g., Qwen2.5-7B) expands queries and\nspecifies artifact types. Relevant images are retrieved via embedding search, while documents\n(DOCX, PPTX, XLSX, PDF) are synthesized programmatically. Each task thus consists of\n(Q,F\u2217), enriched with realistic multimodal context (see Appendix \u00a7A).\nStep exploration and verification.At step i, the controller proposes multiple candidates\n{a1\ni,...,an\ni}, eachexec"}
{"file": "2510.08567v1.pdf", "section": "3.3 STAGE 2: Preference Tuning", "chunk": "Each task thus consists of\n(Q,F\u2217), enriched with realistic multimodal context (see Appendix \u00a7A).\nStep exploration and verification.At step i, the controller proposes multiple candidates\n{a1\ni,...,an\ni}, eachexecutedtoyieldoutcomes {o1\ni,...,on\ni}. AnLLM-basedverifier, conditioned\non(Q,hi), compares these outcomes and selects the most reliable action apre\ni= (t\u22c6\ni,c\u22c6\ni). The\nremaining candidates form the dispreferred set Ddis\ni. A task with msteps yields m(n\u22121)\npreference pairs:\nD={(xi,apre\ni,adis\ni)|i\u2208[1,m]}.(5)\nPref-Xpipeline.Unlike traditional RLHF approaches based on PPO (Schulman et al.,\n2017), which require reward modeling and costly reinforcement learning updates, Direct\nPreference Optimization (DPO) directly optimizes over preference pairs (Rafailov et al., 2023).\nIt leverages a fixed reference policy to stabilize training, avoids the need for explicit reward\nmodels, and is significantly more computationally efficient. This makes DPO particularly\nwell-suited for step-wise preference tuning, where fine-grained comparisons are abundant but\nfull reinforcement learning would be prohibitively expensive. To create preference-labeled\n6\nPreprint\ntrajectories, we combine two complementary components:step samplingandstep verification.\nInstead of relying on static demonstrations, we employ an online exploration scheme (Fig. 2)\nwhere the agent iteratively samples actions and verifies their quality within each task.\nAt stepi, the controller proposes ncandidate actions {a1\ni,a2\ni,...,an\ni}, each decomposed into\n(tk\ni,ck\ni), which are executed to yield observations {o1\ni,...,on\ni}. We then prompt an LLM-based\nverifier with the query Q, historyhi, candidate actions, and corresponding observations, and\nselect the most reliable action(t\u22c6\ni,c\u22c6\ni,o\u22c6\ni). This process expands the trajectory step by step\nuntil the task is solved.\nThe preference data is constructed in a pairwise manner: for each input xi, the selected best\nactionapre\ni= (t\u22c6\ni,c\u22c6"}
{"file": "2510.08567v1.pdf", "section": "3.3 STAGE 2: Preference Tuning", "chunk": "expands the trajectory step by step\nuntil the task is solved.\nThe preference data is constructed in a pairwise manner: for each input xi, the selected best\nactionapre\ni= (t\u22c6\ni,c\u22c6\ni)serves as thepreferredlabel, while the remaining candidates {aj\ni}j\u0338=\u22c6\nform thedispreferredset Ddis\ni. A single task with msteps thus yields m(n\u22121)preference\npairs, summarized as\nD={(xi,apre\ni,adis\ni)|i\u2208[1,m]}.(6)\nObjective.Given the constructed dataset D, we optimize the controller using the Direct\nPreference Optimization (DPO) objective (Kong et al., 2025):\nL(\u03b8) =\u2212E(xi,apre\ni,adis\ni)\u223cD/bracketleft\uf8ecig\nlog\u03c3/parenleft\uf8ecig\n\u03b2/parenleftbig\nlog\u03c0\u03b8(apre\ni|xi)\n\u03c0ref(apre\ni|xi)\u2212log\u03c0\u03b8(adis\ni|xi)\n\u03c0ref(adis\ni|xi)/parenrightbig/parenright\uf8ecig/bracketright\uf8ecig\n,(7)\nwhere\u03c0refis the reference controller (obtained after supervised fine-tuning), \u03b2controls\ndeviation from the reference, and\u03c3(\u00b7)is the logistic function.\nTraining scheme.The final MATRIX controller is trained in two phases: (i)Trajectory-\ndriven SFTon 177K verified traces ( M-TRACE), grounding step-wise tool reasoning. (ii)\nPreference tuningon 11K preference pairs ( Pref-X), where the agent self-explores, generates\ncandidate actions, and updates via the DPO objective. This staged design allows the agent to\nbenefitfromverifiedtraceswhileprogressivelyimprovingdecision-makingthroughexploration\nand preference alignment. A summary of the training loop is given in Algorithm 1.\n4 Results\nWe evaluate MATRIXacross three challenging multimodal agent benchmarks.Agent-\nX(Ashraf et al., 2025) comprises 828 tasks spanning six environments (e.g., web browsing,\ndriving, sports), requiring fine-grained step-wise reasoning.GTA(Wang et al., 2024b)\nconsists of 229 real-world queries paired"}
{"file": "2510.08567v1.pdf", "section": "4 Results", "chunk": "2025) comprises 828 tasks spanning six environments (e.g., web browsing,\ndriving, sports), requiring fine-grained step-wise reasoning.GTA(Wang et al., 2024b)\nconsists of 229 real-world queries paired with authentic tools and multimodal inputs, em-\nphasizing long-horizon tool usage.GAIA(Mialon et al., 2023) provides 106 open-ended\nmultimodal questions covering diverse reasoning levels and task domains.\nWe benchmark against: (i)closed-source controllers(GPT-4, GPT-4o), (ii)open-source\ncontrollers(LLaVA-NeXT-8B (Liu et al., 2024a), InternVL2-8B (Chen et al., 2024c), Qwen2-\nVL-7B (Yang et al., 2024), MiniCPM-V-8.5B (Yao et al., 2024)), and (iii)agent baselines\n(Lego (Team, 2023), Sibyl (Wang et al., 2024c), Warm-up Act (Mialon et al., 2023), HF\nAgent (HuggingFace Contributors, 2024)). Beyond head-to-head comparisons, we conduct\nablations on data generation and preference tuning, and provide qualitative case studies\nillustrating howMATRIXsolves complex multimodal tasks through adaptive tool reasoning.\n4.1 Experimental Setup\nImplementation.We adopt Qwen2-VL-7B (Yang et al., 2024) as the controller and fine-\ntune the language backbone with LoRA (Hu et al., 2022). Training runs for five epochs on\nM-TRACE using LoRA rank 32 applied to query, key, and value projections in all attention\nlayers. We optimize with AdamW (lr=1 \u00d710\u22126), cosine annealing, batch size 2 per device,\nand a 10,240-token context window. Experiments use 4 \u00d7H200 GPUs, and inference is\nperformed without sampling or verification for fair comparison.\nEvaluation Metrics.Following prior works (Ashraf et al., 2025; Wang et al., 2024b;\nGao et al., 2025a), we evaluate performance of Agent-X using three modes: Step-by-Step\n(correctness of individual tool-use steps), Deep Reasoning (co"}
{"file": "2510.08567v1.pdf", "section": "4.1 Experimental Setup", "chunk": "2025; Wang et al., 2024b;\nGao et al., 2025a), we evaluate performance of Agent-X using three modes: Step-by-Step\n(correctness of individual tool-use steps), Deep Reasoning (coherence and factual accuracy\n7\nPreprint\nTable 1:Overall results on Agent-X. Best values in each column (within open/closed-\nsource) are inbold, and second-best are underlined . Metrics are detailed in Appendix\nModelStep-by-Step Deep Reasoning Outcome\nGsTpTaccFaccCsFpSacc Gacc G\u2217\naTs\nacc\nClosed-source\nGemini-2.5-Pro 0.40 0.36 0.81 0.72 0.48 0.64 0.73 0.40 0.56 0.62\nGPT-4o 0.60 0.470.72 0.81 0.57 0.790.59 0.370.70 0.68\nOpenAI-o4-mini 0.42 0.320.89 0.71 0.51 0.600.80 0.450.67 0.63\nOpen-source\nPhi-4-VL-Instruct 0.13 0.21 0.24 0.61 0.19 0.47 0.40 0.11 0.26 0.42\nInternVL2.5-8B 0.45 0.31 0.47 0.68 0.47 0.52 0.60 0.28 0.55 0.58\nGemma-3-4B 0.26 0.30 0.78 0.61 0.54 0.38 0.54 0.27 0.67 0.60\nInternVL3-8B 0.46 0.34 0.54 0.68 0.45 0.70 0.40 0.20 0.59 0.62\nVideoLLaMA3-7B 0.45 0.28 0.46 0.65 0.46 0.62 0.54 0.28 0"}
{"file": "2510.08567v1.pdf", "section": "4.1 Experimental Setup", "chunk": "0.59 0.62\nVideoLLaMA3-7B 0.45 0.28 0.46 0.65 0.46 0.62 0.54 0.28 0.54 0.54\nQwen2-VL-7B 0.510.390.540.620.410.34 0.38 0.25 0.55 0.57\nOurs\nMATRIX (Ours) 0.59 0.44 0.91 0.710.480.88 0.71 0.39 0.76 0.77\nBaseline Improvement (Qwen2-VL-7B) +8% +5% +37% +9% +7% +54% +33% +14% +21% +20%\n*Closed-source results shown for reference; best/second-best highlighting applies only to Open-source models.\nTable 2:Results on GTA and GAIA benchmarks.Bold numbers indicate the best\nperformance among open-source models, underline denotes the second best.\nMethod ControllerGTA GAIA\nAnsAcc Level 1 Level 2 Level 3 AnsAcc\nClosed-source Controllers\nLego Agent GPT-4 46.59 - - - -\nLego Agent GPT-4o 41.52 - - - -\nSibyl Agent GPT-4-turbo - 43.40 27.90 7.70 29.70\nWarm-up Agent GPT-4-turbo - 30.20 15.10 0.00 17.60\nHF Agent GPT-4o 57.05 47.17 31.40 11.54 33.40\nHF Agent GPT-4o-mini 57.69 33.96 27.91 3.84 26.06\nOpen-source Controllers\nHF Agent InternVL2-8B 32.05 7.55 4.65 0.00 4.85\nHF Agent MiniCPM-V-8.5B 33.97 13.21 5.81 0.00 7.27\nHF Agent Qwen2-VL-7B 42.31 16.98 8.14 0.00 9.70\nT3"}
{"file": "2510.08567v1.pdf", "section": "4.1 Experimental Setup", "chunk": "13.21 5.81 0.00 7.27\nHF Agent Qwen2-VL-7B 42.31 16.98 8.14 0.00 9.70\nT3-Agent MAT-MiniCPM-V-8.5B 52.56 26.42 11.63 3.84 15.15\nT3-Agent MAT-Qwen2-VL-7B 53.85 26.42 15.12 3.84 16.97\nOurs\nMATRIX Agent Tuned Qwen2-VL-7B 65.38\u00b14% 29.15\u00b14% 19.28\u00b12% 6.84\u00b13% 21.47\u00b13%\nImprovement over Qwen2-VL-7B +23.07% +12.17% +11.14% +6.84 +11.77%\n*The variance and error study are given in Appendix\u00a7 C.1\nof multi-step reasoning), and Outcome (overall task-solving success via final answers and tool\nexecution). For GTA and GAIA, we report AnsAcc, with GAIA results further broken down\nby difficulty levels (Level 1, Level 2, and Level 3).\n4.2 State-of-the-art Comparisons\nAgent-X:Tab. 1 shows that while open-source models like Qwen2-VL-7B ,InternVL3-8B ,\nandVideoLLaMA3-7B improve grounding and factual precision on Agent-X, they remain\nbehind closed-source controllers (e.g., GPT-4o, Gemini). Key metrics for Agent-X include\nTool Accuracy (correct execution), Faithfulness Accuracy (evidence alignment), and\nSemantic Accuracy (contextual fit). MATRIXachieves the highest scores, 0.91, 0.71, and\n0.71, respectively, yielding relative gains of+8%grounding,+5%precision,+37%tool\naccuracy, and+50%factual precision over Qwen2-VL-7B . These results confirm that step-\nwise preference optimization with AI feedback substantially enhances grounding with tool-use\nreasoning and offers a scalable open-source alternative.\nGTA and GAIA:Tab. 2 reports results on GTA and GAIA. On GTA, MATRIXoutperforms\nboth closed-source ("}
{"file": "2510.08567v1.pdf", "section": "4.2 State-of-the-art Comparisons", "chunk": "with AI feedback substantially enhances grounding with tool-use\nreasoning and offers a scalable open-source alternative.\nGTA and GAIA:Tab. 2 reports results on GTA and GAIA. On GTA, MATRIXoutperforms\nboth closed-source (GPT-4/4o) and open-source (InternVL2-8B, Qwen2-VL-7B) controllers,\n8\nPreprint\nTable 3:Ablation studies for MATRIXon GTALeft: Effect of steps( d).Middle: BLEU\nscores for verifier discrimination (lower is better).Right: Effect of two-verifier design.\nIteration Steps (d) Verifier BLEU (\u2193) Two-Verifier Ablation\nd200 500 1000\nAnsAcc 55.1765.3860.50Verifier B1 B2 B3 B4\nRandom 0.53 0.41 0.36 0.34\nOurs 0.21 0.22 0.19 0.17Method GTA GAIA\nw/o verifiers 50.00 13.33\nOurs 65.38 19.28\nTable 4:Ablation studies for MATRIXon Agent-X.Left: Effect of computation budget.\nMiddle: Effect of dataset size.Right: Comparison of different RL methods.\nGPU Compute Dataset Size (samples) RL Method Comparison\nData 8K 17.5K 28.5K\nMemory (GB) 221 270 318Size 8K 17.5K 28.5K\nGoal_Acc 0.29 0.350.39Method SFT ORPO DPO\nGoal_Acc 0.31 0.370.39\nwith a+23 .07%AnsAccgain over Qwen2-VL-7B. Compared to SFT-based methods like\nT3-Agent, it shows clear advantages from self-exploration and preference refinement, relying\nless on costly annotations. On GAIA,Matrix-Agentachieves best performance among\nopen-source models, surpassing Qwen2-VL-7B by+11 .77%inAnsAcc. While a small gap\nremains to closed-source models, we attribute this to scale and proprietary data. The results\nvalidate the effectiveness of our step-wise preference optimization for multimodal tool-use.\n4.3 Ablation and Analysis\nEffect of Iter"}
{"file": "2510.08567v1.pdf", "section": "4.2 State-of-the-art Comparisons", "chunk": "a small gap\nremains to closed-source models, we attribute this to scale and proprietary data. The results\nvalidate the effectiveness of our step-wise preference optimization for multimodal tool-use.\n4.3 Ablation and Analysis\nEffect of Iteration Step Size.The iteration step size dcontrols the trade-off between\nupdate frequency (how often the policy is updated) and data diversity (breadth of sampled\ntrajectories). A very small d(e.g., 200) yields frequent updates but limited diversity, while\na larged(e.g., 1000) increases diversity at the cost of slower adaptation. As shown in\nTab. 3(left),d= 500achieves the best balance, giving the highestAnsAccof 65.38%.\nVerifier Discrimination Ability.We measure how well our verifier distinguishes candidate\nsteps by comparing it with random selection using BLEU scores (lower is better, since\nlower overlap means more diverse actions). Tab. 3(middle) shows that our verifier achieves\nconsistently lower BLEU (e.g., BLEU-1 = 0.21 vs. 0.53 for random), indicating it selects\nmore distinct and informative steps, which translates to improvedAnsAcc.\nImpact of Dual-Verifier Framework.We further ablate the two-verifier design by\nremoving one verifier. As reported in Tab. 3(right), performance drops substantially (GTA:\n65.38%\u219250.00%, GAIA: 19.28%\u219213.33%), confirming that combining both verifiers is\ncritical for filtering inconsistent or low-quality samples.\nAblation on Dataset Scale, Memory, and Optimization Methods.Tab. 4 summarizes\nthe effect of training data size and tuning strategies. Increasing the dataset from 8K to\n28.5K samples raises memory usage (from 221 GB to 318 GB across 4 \u00d7H200 GPUs) but\nyields steady gains in Goal_Acc (0.29\u21920.39). On the optimization side, ORPO improves\nover pure SFT (0.37 vs. 0.31), while DPO achieves the highest score (0.39), underscoring\nthe effectiveness of preference-based tuning for step-level tool reasoning.\nAdditional ablations on variance analysis, tool preference, and modality contributions are in\nAppendix \u00a7C. Qualitative/failure case analysis is shown in Appendix \u00a7D.\n5 Conclusion"}
{"file": "2510.08567v1.pdf", "section": "4.3 Ablation and Analysis", "chunk": "the effectiveness of preference-based tuning for step-level tool reasoning.\nAdditional ablations on variance analysis, tool preference, and modality contributions are in\nAppendix \u00a7C. Qualitative/failure case analysis is shown in Appendix \u00a7D.\n5 Conclusion\nWe introduced MATRIX, a vision-centric framework for multimodal agent tuning that advances\ntool-use reasoning through staged training. MATRIX combines large-scale trajectory super-\nvision (M-TRACE) with step-wise preference optimization ( Pref-X), enabling agents to both\nacquire fundamental tool-use skills and refine their decision-making beyond imitation. This\nunified design achieves consistent gains across Agent-X, GTA, and GAIA, surpassing existing\n9\nPreprint\nbaselines. Our results highlight the scalability and effectiveness of integrating synthetic data\ngeneration with iterative self-exploration for building robust multimodal agents.\nLimitations and Future Directions.While effective, MATRIXhas some limitations.\nCurrently, it only grounds multimodal signals at the query/task level, relies on prompt-based\nverifiers that may falter under distribution shifts, and optimizes step-level preferences without\ntrajectory-level credit assignment. Future work will address these by exploring adaptive\nverifiers, continuous multimodal grounding, and hierarchical preference modeling.\nReferences\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni\nAleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4\ntechnical report.arXiv preprint arXiv:2303.08774, 2023. 3\nTajamul Ashraf, Amal Saqib, Hanan Ghani, Muhra AlMahri, Yuhao Li, Noor Ahsan, Umair\nNawaz, Jean Lahoud, Hisham Cholakkal, Mubarak Shah, Philip Torr, Fahad Shahbaz\nKhan, Rao Muhammad Anwer, and Salman Khan. Agent-x: Evaluating deep multimodal\nreasoning in vision-centric agentic tasks. 2025. URL https://arxiv.org/abs/2505.\n24876. 2, 3, 4, 7\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "2505.\n24876. 2, 3, 4, 7\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin\nGe, Yu Han, Fei Huang, et al. Qwen technical report.arXiv preprint arXiv:2309.16609,\n2023. 3\nXiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui\nDing, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language\nmodels with longtermism.arXiv preprint arXiv:2401.02954, 2024. 3\nTim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow\nimage editing instructions. InThe IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 18392\u201318402, 2023. 20\nHarrison Chase. Langchain, October 2022. URL https://github.com/langchain-ai/\nlangchain. 3\nGuoxin Chen, Minpeng Liao, Chengxi Li, and Kai Fan. Step-level value preference optimiza-\ntion for mathematical reasoning. InAnnual Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pp. 7889\u20137903, 2024a. 3\nJianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng Liu. Bge m3-\nembedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through\nself-knowledge distillation. 2024b. 4\nZhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong,\nQinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation\nmodels and aligning for generic visual-linguistic tasks. InProceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 24185\u201324198, 2024c. 7\nYu Du,"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "models and aligning for generic visual-linguistic tasks. InProceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 24185\u201324198, 2024c. 7\nYu Du, Fangyun Wei, and Hongyang Zhang. Anytool: Self-reflective, hierarchical agents\nfor large-scale api calls. InInternational Conference on Machine Learning (ICML), pp.\n11812\u201311829, 2024. 3\nYue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. Videoagent:\nA memory-augmented multimodal agent for video understanding. InEuropean Conference\non Computer Vision (ECCV), 2024. 2\nYuwei Fu, Haichao Zhang, Di Wu, Wei Xu, and Benoit Boulet. Furl: visual-language models\nas fuzzy rewards for reinforcement learning. InInternational Conference on Machine\nLearning (ICML), pp. 14256\u201314274, 2024. 3\n10\nPreprint\nZhi Gao, Yuntao Du, Xintong Zhang, Xiaojian Ma, Wenjuan Han, Song-Chun Zhu, and Qing\nLi. Clova: A closed-loop visual assistant with tool usage and update. InThe IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 13258\u201313268, 2024.\n2\nZhi Gao, Bofei Zhang, Pengxiang Li, Xiaojian Ma, Tao Yuan, Yue Fan, Yuwei Wu, Yunde\nJia, Song-Chun Zhu, and Qing Li. Multi-modal agent tuning: Building a vlm-driven agent\nfor efficient tool usage. InInternational Conference on Learning Representations (ICLR),\n2025a. 7\nZhi Gao, Bofei Zhang, Pengxiang Li, Xiaojian Ma, Tao Yuan, Yue Fan, Yuwei Wu, Yunde\nJia, Song-Chun Zhu, and Qing Li. Multi-modal agent tuning: Building a vlm-driven\nagent for efficient tool usage. InThe Thirteenth International Conference on Learning\nRepresentations, 2025b. 1, 2, 4\nAaron Grattafiori, Abhimanyu Dubey, Abhin"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "agent for efficient tool usage. InThe Thirteenth International Conference on Learning\nRepresentations, 2025b. 1, 2, 4\nAaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian,\nAhmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The\nllama 3 herd of models.arXiv preprint arXiv:2407.21783, 2024. 3\nSignificant Gravitas. Autogpt, 2023. URL https://github.com/Significant-Gravitas/\nAutoGPT. 3\nTanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual\nreasoning without training. InThe IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 14953\u201314962, 2023. 2\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,\nLu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. In\nInternational Conference on Learning Representations (ICLR), 2022. 7\nHuggingFace Contributors. Agents and tools, 2024. URL https://huggingface.co/docs/\ntransformers/agents. 7\nAaron Hurst, Adam Lerer, Adam P Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark,\nAJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card.arXiv\npreprint arXiv:2410.21276, 2024. 4\nbrian ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander\nHerzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Kalashnikov,\nSergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander T\nToshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "nikov,\nSergey Levine, Yao Lu, Carolina Parada, Kanishka Rao, Pierre Sermanet, Alexander T\nToshev, Vincent Vanhoucke, Fei Xia, Ted Xiao, Peng Xu, Mengyuan Yan, Noah Brown,\nMichael Ahn, Omar Cortes, Nicolas Sievers, Clayton Tan, Sichun Xu, Diego Reyes, Jarek\nRettinghouse, Jornell Quiambao, Peter Pastor, Linda Luu, Kuang-Huei Lee, Yuheng\nKuang, Sally Jesmonth, Nikhil J. Joshi, Kyle Jeffrey, Rosario Jauregui Ruano, Jasmine\nHsu, Keerthana Gopalakrishnan, Byron David, Andy Zeng, and Chuyuan Kelly Fu. Do as\ni can, not as i say: Grounding language in robotic affordances. InConference on Robot\nLearning (CoRL), pp. 287\u2013318, 2023. 2\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything.\nInInternational Conference on Computer Vision (ICCV), pp. 4015\u20134026, 2023. 5\nAobo Kong, Wentao Ma, Shiwan Zhao, Yongbin Li, Yuchuan Wu, Ke Wang, Xiaoqian Liu,\nQicheng Li, Yong Qin, and Fei Huang. Sdpo: Segment-level direct preference optimization\nfor social agents.arXiv preprint arXiv:2501.01821, 2025. 2, 3, 7\nXin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Step-\ndpo: Step-wise preference optimization for long-chain reasoning of llms.arXiv preprint\narXiv:2406.18629, 2024. 3\nHarrison Lee, Samrat Phatale, Hassan Mansoor, Thomas Mesnard, Johan Ferret, Kellie Ren\nLu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif vs. rlhf:\nScaling reinforcement learning from human"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": ", Thomas Mesnard, Johan Ferret, Kellie Ren\nLu, Colton Bishop, Ethan Hall, Victor Carbune, Abhinav Rastogi, et al. Rlaif vs. rlhf:\nScaling reinforcement learning from human feedback with ai feedback. InInternational\nConference on Machine Learning (ICML), pp. 26874\u201326901. PMLR, 2024. 3\n11\nPreprint\nChenliang Li, He Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng\nZhang, Wenmeng Zhou, Yingda Chen, Chen Cheng, et al. Modelscope-agent: Building\nyour customizable agent system with open-source large language models. InProceedings\nof the 2023 Conference on Empirical Methods in Natural Language Processing: System\nDemonstrations, pp. 566\u2013578, 2023. 3\nJian Li, Yabiao Wang, Changan Wang, Ying Tai, Jianjun Qian, Jian Yang, Chengjie Wang,\nJilin Li, and Feiyue Huang. Dsfd: dual shot face detector. InThe IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pp. 5060\u20135069, 2019. 20\nWeizhen Li, Jianbo Lin, Zhuosong Jiang, Jingyi Cao, Xinpeng Liu, Jiayu Zhang, Zhenqiang\nHuang, Qianben Chen, Weichen Sun, Qiexiang Wang, Hongxuan Lu, Tianrui Qin, Cheng-\nhao Zhu, Yi Yao, Shuying Fan, Xiaowan Li, Tiannan Wang, Pai Liu, King Zhu, He Zhu,\nDingfeng Shi, Piaohong Wang, Yeyi Guan, Xiangru Tang, Minghao Liu, Yuchen Eleanor\nJiang, Jian Yang, Jiaheng Liu, Ge Zhang, and Wangchunshu Zhou. Chain-of-agents:\nEnd-to-end agent foundation models via multi-agent distillation and agentic rl, 2025. URL\nhttps://arxiv.org/abs/2508.13167. 2\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "https://arxiv.org/abs/2508.13167. 2\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In\nComputer vision\u2013ECCV 2014: 13th European conference, zurich, Switzerland, September\n6-12, 2014, proceedings, part v 13, pp. 740\u2013755. Springer, 2014. 5, 26\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.\nAdvances in Neural Information Processing Systems, 36, 2024a. 5, 7\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in\nthe wild. InInternational Conference on Computer Vision (ICCV), 2015. 5\nZuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Shirley Kokane, Juntao Tan,\nWeiran Yao, Zhiwei Liu, Yihao Feng, et al. Apigen: Automated pipeline for generating\nverifiable and diverse function-calling datasets.arXiv preprint arXiv:2406.18518, 2024b.\n3, 4\nAhmed Masry, Xuan Long Do, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa:\nA benchmark for question answering about charts with visual and logical reasoning. In\nAnnual Meeting of the Association for Computational Linguistics (ACL), pp. 2263\u20132279,\n2022. 5\nKai Mei, Xi Zhu, Wujiang Xu, Wenyue Hua, Mingyu Jin, Zelong Li, Shuyuan Xu, Ruosong\nYe, Yingqiang Ge, and Yongfeng Zhang. Aios: Llm agent operating system.arXiv preprint\narXiv:2403.16971, 2024. 3\nGr\u00e9goire Mialon, Cl\u00e9mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom.\nGaia: a benchmark for general ai assistants. InThe Twelfth International Conference"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "4. 3\nGr\u00e9goire Mialon, Cl\u00e9mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom.\nGaia: a benchmark for general ai assistants. InThe Twelfth International Conference on\nLearning Representations, 2023. 2, 3, 7\nMatthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn,\nAlexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran\nShen, et al. Simple open-vocabulary object detection. InEuropean Conference on Computer\nVision (ECCV), pp. 728\u2013755. Springer, 2022. 20\nEmanuele Musumeci, Michele Brienza, Vincenzo Suriani, Daniele Nardi, and\nDomenico Daniele Bloisi. Llm based multi-agent generation of semi-structured docu-\nments from semantic templates in the public administration domain. InInternational\nConference on Human-Computer Interaction, pp. 98\u2013117. Springer, 2024. 3\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christo-\npher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-\nassisted question-answering with human feedback.arXiv preprint arXiv:2112.09332, 2021.\n3\n12\nPreprint\nZhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu\nWei. Kosmos-2: Grounding multimodal large language models to the world.arXiv preprint\narXiv:2306.14824, 2023. 2\nYujia Qin, Zihan Cai, Dian Jin, Lan Yan, Shihao Liang, Kunlun Zhu, Yankai Lin, Xu Han,\nNing Ding, Huadong Wang, et al. Webcpm: Interactive web search for chinese long-form\nquestion answering. InProceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "Huadong Wang, et al. Webcpm: Interactive web search for chinese long-form\nquestion answering. InProceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 8968\u20138988, 2023. 3\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin\nCong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie\nZhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. ToolLLM: Facilitating\nlarge language models to master 16000+ real-world APIs. InThe Twelfth International\nConference on Learning Representations, 2024. 3\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and\nChelsea Finn. Direct preference optimization: Your language model is secretly a reward\nmodel.Advances in neural information processing systems, 36:53728\u201353741, 2023. 6\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\nHigh-resolutionimagesynthesiswithlatentdiffusionmodels. InThe IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), pp. 10684\u201310695, 2022. 20\nBabak Saleh and Ahmed Elgammal. Large-scale classification of fine-art paintings: Learning\nthe right metric on the right feature.arXiv preprint arXiv:1505.00855, 2015. 5\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal\npolicy optimization algorithms.arXiv preprint arXiv:1707.06347, 2017. 6\nYongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu,\nDongsheng Li, and Yueting Zhuang. Taskbench: Benchmarking large language models for\ntask automation.arXiv preprint arXiv:231"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": ", Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu,\nDongsheng Li, and Yueting Zhuang. Taskbench: Benchmarking large language models for\ntask automation.arXiv preprint arXiv:2311.18760, 2023. 2, 3\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in hugging face.Advances in\nNeural Information Processing Systems, 36, 2024. 3\nAmanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and\nMarcus Rohrbach. Towards vqa models that can read. InThe IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR), pp. 8317\u20138326, 2019. 5\nYifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, and Sujian Li. Restgpt:\nConnecting large language models with real-world applications via restful apis.arXiv\npreprint arXiv:2306.06624, 2023. 3\nYueqi Song, Frank Xu, Shuyan Zhou, and Graham Neubig. Beyond browsing: Api-based\nweb agents.arXiv preprint arXiv:2410.16464, 2024. 3\nXiaowen Sun, Xufeng Zhao, Jae Hee Lee, Wenhao Lu, Matthias Kerzel, and Stefan Wermter.\nDetails make a difference: Object state-sensitive neurorobotic task planning.arXiv preprint\narXiv:2406.09988, 2024. 2\nD\u00eddac Sur\u00eds, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python\nexecution for reasoning. InInternational Conference on Computer Vision (ICCV), pp.\n11888\u201311898, 2023. 2\nQiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun.\nToolalpaca: Generalized tool learning for language models with"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "2\nQiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun.\nToolalpaca: Generalized tool learning for language models with 3000 simulated cases.\narXiv preprint arXiv:2306.05301, 2023. 3\nAgentLego Developer Team. Enhance llm agents with versatile tool apis. https://github.\ncom/InternLM/agentlego, 2023. 7\n13\nPreprint\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui\nYu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family\nof highly capable multimodal models.arXiv preprint arXiv:2312.11805, 2023. 3\nHarsh Trivedi, Tushar Khot, Mareike Hartmann, Ruskin Manku, Vinty Dong, Edward\nLi, Shashank Gupta, Ashish Sabharwal, and Niranjan Balasubramanian. Appworld: A\ncontrollable world of apps and people for benchmarking interactive coding agents. In\nAnnual Meeting of the Association for Computational Linguistics (ACL), pp. 16022\u201316076,\n2024. 2\nChenyu Wang, Weixin Luo, Sixun Dong, Xiaohua Xuan, Zhengxin Li, Lin Ma, and Shenghua\nGao. Mllm-tool: A multimodal large language model for tool agent learning. In2025\nIEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pp. 6678\u2013\n6687. IEEE, 2025. 3\nHuaijie Wang, Shibo Hao, Hanze Dong, Shenao Zhang, Yilin Bao, Ziran Yang, and Yi Wu. Of-\nfline reinforcement learning for llm multi-step reasoning.arXiv preprint arXiv:2412.16145,\n2024a. 3\nJize Wang, Ma Zerun, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le.\nGta: a benchmark for general"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "2.16145,\n2024a. 3\nJize Wang, Ma Zerun, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le.\nGta: a benchmark for general tool agents. InThe Thirty-eight Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track, 2024b. 2, 3, 4, 7\nJunke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan Wu, and Yu-Gang Jiang. To\nsee is to believe: Prompting gpt-4v for better visual instruction tuning.arXiv preprint\narXiv:2311.07574, 2023. 4\nYulong Wang, Tianhao Shen, Lifeng Liu, and Jian Xie. Sibyl: Simple yet effective agent\nframework for complex real-world reasoning. 2024c. URL https://arxiv.org/abs/2407.\n10718. 7\nZhenyu Wang, Aoxue Li, Zhenguo Li, and Xihui Liu. Genartist: Multimodal llm as an agent\nfor unified image generation and editing.arXiv preprint arXiv:2407.05600, 2024d. 2\nT. Weyand, A. Araujo, B. Cao, and J. Sim. Google Landmarks Dataset v2 - A Large-Scale\nBenchmark for Instance-Level Recognition and Retrieval. InThe IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), 2020. 5\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.\nVisual chatgpt: Talking, drawing and editing with visual foundation models.arXiv preprint\narXiv:2303.04671, 2023. 2\nShirley Wu, Shiyu Zhao, Qian Huang, Kexin Huang, Michihiro Yasunaga, Kaidi Cao, Vassilis\nIoannidis, Karthik Subbian, Jure Leskovec, and James Y Zou. Avatar: Optimizing llm\nagents for tool usage via contrastive reasoning.Advances in Neural Information Processing\nSystems, 37:259"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "idis, Karthik Subbian, Jure Leskovec, and James Y Zou. Avatar: Optimizing llm\nagents for tool usage via contrastive reasoning.Advances in Neural Information Processing\nSystems, 37:25981\u201326010, 2024. 3\nTianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao,\nToh J Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. Osworld: Benchmarking\nmultimodal agents for open-ended tasks in real computer environments.Advances in\nNeural Information Processing Systems, 37:52040\u201352094, 2024. 3\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,\nChengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong\nTang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren\nZhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin\nYang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize\nGao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu\nLiu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin\nWei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong\nLiu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. Qwen2 technical report.arXiv preprint\narXiv:2407.10671, 2024. 1, 5, 7\n14\nPreprint\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards\nscalable real-world web interaction with"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "2024. 1, 5, 7\n14\nPreprint\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards\nscalable real-world web interaction with grounded language agents.Advances in Neural\nInformation Processing Systems, 35:20744\u201320757, 2022. 3\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and\nYuan Cao. ReAct: Synergizing reasoning and acting in language models. InInternational\nConference on Learning Representations, 2023a. 4\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and\nYuan Cao. React: Synergizing reasoning and acting in language models. InInternational\nConference on Learning Representations (ICLR), 2023b. 5, 6\nYuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai,\nHaoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: A gpt-4v level mllm on your phone.\narXiv preprint arXiv:2408.01800, 2024. 7\nTianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui,\nTaiwen He, Zhiyuan Liu, Tat-Seng Chua, et al. Rlaif-v: Aligning mllms through open-\nsource ai feedback for super gpt-4v trustworthiness.arXiv preprint arXiv:2405.17220,\n2024. 3\nLifan Yuan, Yangyi Chen, Xingyao Wang, Yi R Fung, Hao Peng, and Heng Ji. Craft:\nCustomizing llms by creating and retrieving from specialized toolsets. InInternational\nConference on Learning Representations (ICLR), 2024. 2\nChaoyun Zhang, Shilin He, Liqun Li, Si Qin, Yu Kang, Qingwei Lin, and Dongmei Zhang.\nApi agents"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "Conference on Learning Representations (ICLR), 2024. 2\nChaoyun Zhang, Shilin He, Liqun Li, Si Qin, Yu Kang, Qingwei Lin, and Dongmei Zhang.\nApi agents vs. gui agents: Divergence and convergence.arXiv preprint arXiv:2503.11069,\n2025. 3\nRuohong Zhang, Bowen Zhang, Yanghao Li, Haotian Zhang, Zhiqing Sun, Zhe Gan, Yinfei\nYang, Ruoming Pang, and Yiming Yang. Improve vision language model chain-of-thought\nreasoning.arXiv preprint arXiv:2410.16198, 2024a. 5\nZiniu Zhang, Shulin Tian, Liangyu Chen, and Ziwei Liu. Mmina: Benchmarking multihop\nmultimodal internet agents.arXiv preprint arXiv:2404.09992, 2024b. 3\n15\nPreprint\nAppendix for Matrix\nAppendix Contents\nA Additional Details onPref-XPipeline 18\nA.1 Tools Used . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nB MATRIX Training Algorithm 20\nC Additional Experiments 21\nC.1 Error Bars and Variance Analysis. . . . . . . . . . . . . . . . . . . . . . . . . 21\nC.2 Ablation on Modalities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\nC.3 Tool Preference. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nD Qualitative and Failure Analysis 22\nD.1 Example 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nD.2 Example 2 . . . . . . . . . . . . . . . . ."}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": ". . . . . . . . . . . . . . . . . . . . . . . . 22\nD.2 Example 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\nE Human and AI Verification Study 24\nE.1 Human Verification ofM-TRACE. . . . . . . . . . . . . . . . . . . . . . . . . . 24\nE.2 Automatic Verification for Preference Data . . . . . . . . . . . . . . . . . . . 25\nE.3 Broader Impacts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\nE.4 User Study on Agent Outputs and Preferences . . . . . . . . . . . . . . . . . 26\nF Additional Details on Data Generation 26\nF.1 Task Generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nF.2 Query\u2013File Verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\nF.3 Model Comparison for Task Generation . . . . . . . . . . . . . . . . . . . . . 27\nG Case Studies 28\nG.1 GTA Qualiative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\nG.2 GAIA Qualiative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nG.3 Agent-X Qualiative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\nH Stage-1 Prompts 37\nH.1 Query Generation Prompts . . . . . . . . . ."}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": ". . . . . . . . . . . . . . . . . . . . . 34\nH Stage-1 Prompts 37\nH.1 Query Generation Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\nH.2 File Generation Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\nH.3 File Verification Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\nH.4 Trajectory Verification Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . 38\nH.5 MATRIX Prompt - System . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\nI Stage-2 Prompts 38\n16\nPreprint\nI.1 Step Verifier Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\nI.2 Preference Data Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n17\nPreprint\nTask Seed\nLLM Model\nTask\nController\nTool Calling\nVerifier\nTask Generation\nPreference Tuning\nQuery\nFile\nImage\nThought 1\nCode 1\nThought 2\nCode 2\nThought 1\nCode 1\nResult 1\nController\nStep Sampling\nResult 1\nResult 2\nResult 3\nStep Verfication\nThought 1\nCode 1\nThought 2\nCode 2\nThought 3\nCode 3\nThought 1\nCode 1\nThought 3\nCode 3\nFigure 4: Overview of the Pref-Xconstruction pipeline. Tasks are sampled from M-TRACE,\nthen expanded through step sampling, step verification, and preference collection. Verified\npreference pairs are used in Direct Preference Optimization (DPO) to update the controller.\nA Additional Details onPref-"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "construction pipeline. Tasks are sampled from M-TRACE,\nthen expanded through step sampling, step verification, and preference collection. Verified\npreference pairs are used in Direct Preference Optimization (DPO) to update the controller.\nA Additional Details onPref-XPipeline\nIn the main paper (\u00a73.2), we described how Pref-Xis constructed to enable step-wise prefer-\nence optimization. Here, we provide further details of the data generation and verification\npipeline, illustrated in Fig. 4.\nTask Generation.We begin by sampling a set of seed tasks Sfrom the Stage 1 corpus\n(M-TRACE, see \u00a73.2). Each task is defined as a query\u2013file pair( Q,F), whereQdenotes the\nuser query and Fcontains the associated multimodal evidence (e.g., text files, images).\nCandidate step-wise trajectories are produced by the current controller \u03c0\u03b8through iterative\ninteraction with the toolset.\nStep Sampling.For each task( Q,F), the controller generates multiple candidate steps at\neach reasoning turn. These steps include tool calls, arguments, and intermediate reasoning\ntraces. From this pool, diverse samples are retained to avoid mode collapse and to ensure\nbroad coverage of possible reasoning paths. To contextualize our contributions, we position\nMATRIX against representative RL based sampling frameworks. As illustrated in Fig. 5,\nthe comparison spans three axes: task domain, collection granularity, and annotation format.\nUnlike prior methods that primarily operate in narrow domains with trajectory-level rewards,\nMATRIX emphasizes diverse multimodal tasks, collects preferences at the step level, and\nleverages executable tool feedback for scalable and precise supervision.\nStep Verification.Each sampled step is then automatically verified. Verification checks\nwhether (i) the tool call matches the schema, (ii) the arguments are valid and executable,\nand (iii) the intermediate output remains consistent with the task context. Invalid or\nincomplete steps are filtered out. This process corresponds to the loop over history states hi\nin Algorithm 1.\nPreference Collection.For the verified steps, pairwise preferences are collected using\na mixture of automated heuristics and model-based evaluators. Preference signals capture\n18\nPreprint\nTrajectory-Level\nCollection\nPAE\nWebRL\nDMPO\nETO\nGUI Control\nGUI Control\n& Embodied AI\nBrowser/GUI\nGUI + Embodied\nMatrix Agent\n(ours)\nDigiRL\nIPR\nStepAgent-\nInverse\nStep-"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "WebRL\nDMPO\nETO\nGUI Control\nGUI Control\n& Embodied AI\nBrowser/GUI\nGUI + Embodied\nMatrix Agent\n(ours)\nDigiRL\nIPR\nStepAgent-\nInverse\nStep-  Level Collection\nTrajectory-Level\nCollection\nTrajectory-Level\nCollection\nTrajectory-Level\nCollection\nStep-  Level Collection\nStep-  Level Collection\nStep-  Level Collection\nMultimodal\nReasoning\nVision-Text\n+Tools\nAPI Calling\nAPI /tool calls\nTP-LLAMA\nStep-  Level CollectionTask Domain\nInterface\nTask Domain\nInterfaceTask Domain\nInterface\nTask Domain\nInterface\nFigure 5:Comparison of preference data construction frameworks.We contrast\nMATRIX with reinforcement-learning\u2013based sampling pipelines across three key dimensions:\n(1)Task domain, i.e., the scope and modality of tasks considered; (2)Collection granularity,\ni.e., whether data is gathered at the full-trajectory or step level; and (3)Annotation format,\ni.e., the type of supervision used for preference signals. MATRIX uniquely focuses on\nmultimodal tasks, constructs preferences at the step level, and employs executable tool\nfeedback for scalable, fine-grained supervision.\nrelative correctness, faithfulness to the query, and progression towards the goal. The resulting\npreference pairs form the core supervision signal for preference optimization.\nFour Stages of DPO Process.The overall Pref-Xconstruction pipeline aligns with the\nfour-step DPO process outlined in Fig. 4:\n1.Trajectory sampling:Generate candidate step-wise rollouts from \u03c0\u03b8for each seed\ntask.\n2.Step verification:Discard malformed or invalid steps using automatic schema and\nexecution checks.\n3.Preference generation:Construct preference pairs by comparing valid candidate\nsteps.\n4.Policy update:Apply Direct Preference Optimization (DPO) to update \u03c0\u03b8against\nthe reference policy\u03c0 refusing the verified preference datasetD.\nThis iterative pipeline produces the Pref-Xcorpus, which directly supervises step-wise\nimprovements of the agent while preventing reliance on memorized final answers.\nA.1 Tools Used\nTo enable flexible and realistic multimodal task solving, our framework integrates a diverse\nset of tools spanning vision, web, file understanding, and generative capabilities. Below, we\nprovide details of each tool and justify its inclusion. Together, these tools allow agents to\nground reasoning in real data, perform cross-modal"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "a diverse\nset of tools spanning vision, web, file understanding, and generative capabilities. Below, we\nprovide details of each tool and justify its inclusion. Together, these tools allow agents to\nground reasoning in real data, perform cross-modal analysis, and produce actionable outputs.\nWeb Search.This tool is implemented as a meta-agent consisting of three sub-modules:\n(i)SearchInformation, which retrieves candidate webpages given a query, (ii)Visit, which\nextracts textual content from webpages, and (iii)WebQA, which performs targeted question\nanswering over retrieved text. This design ensures the agent can dynamically access and\nreason over up-to-date web knowledge rather than relying solely on static training data.\nImage Question Answering.We integrate GPT-4o-mini as an image QA module, which\naccepts an image and a natural language question to output a textual answer. This capability\nallows the agent to perform grounded reasoning over visual inputs such as charts, natural\nimages, or scanned documents.\n19\nPreprint\nTable 5: Overview of tools used inMatrix-Agent. Each tool specifies the model or library\nbackbone and its primary functionality.\nTool Model / Package Functionality\nWeb Search Google search + GPTSub-tools:SearchInformation(title/abstract/URL),Visit\n(HTML\u2192text),WebQA(Q&A on text).\nImage QA GPT Answers questions given an image input.\nFile InspectorMarkdownConverter +\nGPT-4o-miniConverts multi-modal files into markdown and performs Q&A.\nObject LocalizationOWL-ViT (Minderer et al.,\n2022)Detects objects in images and outputs bounding boxes.\nImage GenerationStable Diffusion (Rombach\net al., 2022)Generates an image from a text query.\nImage EditingInstructPix2Pix (Brooks\net al., 2023)Edits an image according to an instruction.\nFace Detection DSFD (Li et al., 2019) Detects and outputs bounding boxes of faces in an image.\nPython Package Standard libraries + packagesEnables code execution with: requests ,pandas,numpy,scipy,\ntorch,cv2, etc.\nFile Inspector.For structured documents (PDFs, Word, Excel, PowerPoint), we use the\nPython package MarkdownConverter to parse raw files into text. The resulting content is\ncombined with a query and passed to GPT-4o-mini for reasoning. This tool extends the"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "PDFs, Word, Excel, PowerPoint), we use the\nPython package MarkdownConverter to parse raw files into text. The resulting content is\ncombined with a query and passed to GPT-4o-mini for reasoning. This tool extends the\nagent\u2019s ability to understand heterogeneous non-image resources, which are common in\nreal-world multimodal tasks.\nObject Localization.We employ OWL-ViT (Minderer et al., 2022) for object grounding.\nGiven an image and a query (e.g., \u201clocalize all cups\u201d), the tool outputs bounding boxes for\nrelevant objects. This allows the agent to handle spatial reasoning and locate specific entities\nin visual contexts.\nImage Generation.Stable Diffusion (Rombach et al., 2022) is used for text-to-image\ngeneration, enabling agents to produce synthetic images from natural language prompts.\nThis supports tasks such as visualization, illustration, or generating multimodal artifacts\nrequired by a query.\nImage Editing.We incorporate InstructPix2Pix (Brooks et al., 2023), which takes an\ninstruction and an input image to output a modified version. This capability is essential\nfor tasks requiring visual manipulation, such as highlighting regions, altering attributes, or\niterative refinement of generated content.\nFace Detection.We use DSFD (Li et al., 2019) as a robust face detection backbone. It\nidentifies bounding boxes of all visible faces in an image. Face-level grounding is a critical\ncapability for tasks involving identity verification, demographic analysis, or interaction\nreasoning.\nPython Package Execution.Finally, we allow the agent to call a curated set of Python\npackages ( pandas,numpy,matplotlib ,torch, etc.). This provides a flexible computational\nbackend for data analysis, symbolic reasoning, and numerical tasks. By combining tool\nexecution with code-level reasoning, the agent can go beyond natural language planning and\nsolve complex multimodal problems.\nIn summary, these tools collectively enable MATRIXto handle tasks requiring perception,\nreasoning, retrieval, and generation across diverse modalities. The broad coverage of tool\ncategories (search, vision, file understanding, generation, and computation) ensures the agent\nis capable of solving realistic and complex multimodal tasks.\nB MATRIX Training Algorithm\nTo complement the description in the main paper, we provide a detailed summary of the\ntraining pipeline and its algorithmic formulation (Algorithm 1).\nOverview.After Stage 1 supervised fine-tuning (SFT), the agent is"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "B MATRIX Training Algorithm\nTo complement the description in the main paper, we provide a detailed summary of the\ntraining pipeline and its algorithmic formulation (Algorithm 1).\nOverview.After Stage 1 supervised fine-tuning (SFT), the agent is refined withstep-wise\npreference optimization. Unlike static imitation learning, this stage enables the controller\n20\nPreprint\nto activelyexploremultiple candidate actions per reasoning step and receive structured\nfeedback from AI-based verifiers. This design addresses three limitations of pure imitation:\n(i) it improves adaptability by allowing recovery from suboptimal or partially correct rollouts,\n(ii) it leverages exploration rather than discarding incomplete or noisy demonstrations, and\n(iii) it scales preference data construction without requiring expensive manual annotations.\nAlgorithm 1 MATRIX: Iterative Step-Wise\nPreference Optimization\n1:Input:Seed tasks S, controller \u03c0\u03b8, refer-\nence\u03c0 ref=\u03c0\u03b8\n2:Output:Updated controller \u03c0\u22c6\n\u03b8from\nStage 1.\n3:whilenot convergeddo\n4:D\u2190\u2205\n5:fortask(Q,F)\u2208Sdo\n6:h 1\u2190\u2205\n7:fori= 1...mdo\n8:Sample candidates from\u03c0 \u03b8\n9: Execute, verify, and add prefer-\nences\n10:Update historyh i+1\n11:end for\n12:end for\n13:Update\u03c0 \u03b8\u2190DPO(\u03c0 \u03b8,\u03c0ref,D)\n14:end whileProcess.The algorithm starts with a seed\ntask poolSand a controller \u03c0\u03b8initialized\nfrom Stage 1. For each task, the agent inter-\nacts step-by-step: it generates candidate ac-\ntions, executes them through real tool calls,\nand submits outcomes to a verifier. The\nverifier compares the candidates and ranks\nthem, producing preference pairs that dis-\ntinguish consistent, accurate behaviors from\nweaker alternatives. These pairs are accu-\nmulated into a dataset D, which is then\nused to update the controller via the Direct\nPreference Optimization (DPO) objective,\nwith\u03c0ref(the Stage 1 model) serving as the\nreference.\nIteration.This loop is repeated itera-\ntively until convergence. Over time, the\nagent becomes aligned with behaviors that\nare not onlycorrectbut alsorobust,consis-\ntent, andsemantically usefulacross diverse\nmultimodal tasks"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "loop is repeated itera-\ntively until convergence. Over time, the\nagent becomes aligned with behaviors that\nare not onlycorrectbut alsorobust,consis-\ntent, andsemantically usefulacross diverse\nmultimodal tasks. The procedure is formal-\nized in Algorithm 1, which illustrates the\nalternating phases of step-level exploration, preference pair construction, and parameter\nupdates.\nC Additional Experiments\nC.1 Error Bars and Variance Analysis.\nWe observe small but non-negligible fluctuations across repeated runs (Tab. 6), even though\nthe tuning pipeline itself is deterministic. The primary sources of variance stem from external\nAPI dependencies: (i) the Google Search API occasionally fails or returns unstable rankings\nof web results, leading to variation in retrieved evidence; (ii) the OpenAI API (used for\nGPT-4o-mini based verification and artifact generation) can occasionally time out or produce\nslightly different responses under identical prompts. These inconsistencies propagate into\ntool execution and trajectory verification, ultimately affecting downstream accuracy metrics\nby a few percentage points. Importantly, despite this natural variance, our improvements\nover the baseline remain statistically significant, confirming the robustness of our framework.\nTable 6: Performance with variance on the GTA benchmark. Results are reported as mean\n\u00b1standard deviation over 5 runs.\nMethod AnsAcc\nBaseline (Qwen2-VL-7B) 43.21\nMATRIX Agent with Qwen2-VL-7B) 63.26\u00b14.78\nC.2 Ablation on Modalities\nTo analyze the contribution of different modalities, we perform ablation experiments on the\nGTA benchmark. As shown in Tab. 7, removing the image modality drastically reduces\n21\nPreprint\nperformance, with AnsAccdropping by nearly 40%. This highlights the critical role of visual\ninputs for accurate tool-use reasoning.\nTable 7: Ablation on GTA benchmark. OnlyAnsAccis reported.\nMethod AnsAcc\nMATRIX Agent w/o image 8.67\nMATRIX Agent w/ image 63.56\nC.3 Tool Preference.\nFigure 6: Tool distribution for the chosen\nand rejected steps.Wefurtheranalyzethedistributionoftoolsacross\nselected and rejected steps (Fig. 6). In MATRIX,\nfrequently adopted tools such as visualizer\n(2101 uses) and objectloc"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "for the chosen\nand rejected steps.Wefurtheranalyzethedistributionoftoolsacross\nselected and rejected steps (Fig. 6). In MATRIX,\nfrequently adopted tools such as visualizer\n(2101 uses) and objectloc (1051 uses) domi-\nnate the chosen steps, while the rejected steps\nshow heavier reliance on objectloc (1442 uses),\nvisualizer (1524 uses), and less effective utili-\nties such as ocrandseg. This mismatch results\nin a 45.62% divergence between the two distribu-\ntions, suggesting that MATRIX\u2019s verifier favors\ntool combinations that are more semantically\naligned and practically useful, while systemati-\ncally filtering out noisy or redundant tool usage.\nD Qualitative and Failure\nAnalysis\nGoal.We analyze how agentic vision\u2013language pipelines fail on image-grounded arithmetic\nand counting tasks, and why ourMATRIXagent is more robust than the baseline (MAT).\nWe focus on two representative cases: (i) computing the number ofboxesof eggs required\nfor 12 servings (discrete reasoning), and (ii) summing calories from a table (continuous\narithmetic).\nD.1 Example 1\nObserved behavior.On visually grounded arithmetic (e.g., the \u201ceggs/servings\u201d task) as\nshown in Fig. 7, the baseline MATfrequently entered atool-use loopwhere it produced the\nsame action multiple times, received similar observations, and then repeated the action again\nwithout incorporating the feedback. This repetition culminated in a confident but incorrect\nanswer. In contrast, MATRIXexhibited an initially brittle code synthesis (a parsing error and\nmissingprintstatement) but subsequently self-corrected and produced the correct discrete\ncount.\nFailure modes in MAT.We categorize the baseline errors into three coupled modes:\n1.Mis-interpretation of the task.The agent failed to decompose the instruction\ninto sub-goals (extract numbers \u2192compute total\u2192round up to boxes), so its\nactions did not target missing information.\n2.Planning deficit.Absent an explicit \u201cplan\u2013act\u2013observe\u2013revise\u201d scaffold, the agent\ntreated unchanged observations as if they were new evidence, never triggering a\nbranch to an alternative tool or a reformulated query.\n3.Looping/hallucination.Repeating identical (or template-like) tool calls despite\nidentical observations indicates policy collapse toward a habitual trajectory"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "if they were new evidence, never triggering a\nbranch to an alternative tool or a reformulated query.\n3.Looping/hallucination.Repeating identical (or template-like) tool calls despite\nidentical observations indicates policy collapse toward a habitual trajectory, rather\nthan belief update from evidence.\n22\nPreprint\nWhy MATRIX eventually succeeds (but starts loose). MATRIX\u2019s first attempt\nproduced (i) an invalid parse ( NoneType.group ) and (ii) no printed value, hence no us-\nable observation. However, its reflective step modified the code to (a) explicitly compute\nceil(total_eggs/eggs_per_box) and (b)printthe result, restoring the tool\u2013feedback\nloop and yielding the correct answer. This success originates from a minimal but effective\nrevise-and-retry behavior whereas the brittleness stems from unconstrained code generation\nand weak I/O contracts.\nFigure 7:Baseline vs. MATRIX.For the given task, the baseline repeats and answers1\n(wrong), whileMATRIXself-corrects after early code/IO hiccups and outputs the correct\ndiscrete count2.\nD.2 Example 2\nObserved Behavior.In Fig. 8, the baseline (MAT/Qwen-2VL) tries multiple extraction\nroutes:(i)it first fails on a missing image file, then(ii)fails on a visualizer call, and(iii)\nfinally falls back to parsing the provided image as a table text. It correctly recovers the\nper-item values like Egg= 157kcal/100 g, Tomato= 19kcal/100 g, and even logs them, but\nit returns them directly as the final answer rather than performing the requested aggregation,\nwhich was the main logic for the provided task. By contrast, MATRIXreads the same two\nnumbers and, given the task\u2019s simplicity, directly performs the scalar sum to be176because\nof the simple nature of the task, but it finalizes without emitting any intermediate logs (no\n\u2018print\u2019), i.e., a \u201cno-tool path\u201d with an empty observation buffer.\nFailure Modes in MAT.The core error is task misinterpretation / incomplete aggregation\nas MAT stops at entity-level extraction and treats the tuple {157,19}as the answer, ignoring\nthe query\u2019s composition verb (\u201cin total\u201d). This reflects a weak finalization contract (it\npermits non-scalar answers) and missing schema constraints ("}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "at entity-level extraction and treats the tuple {157,19}as the answer, ignoring\nthe query\u2019s composition verb (\u201cin total\u201d). This reflects a weak finalization contract (it\npermits non-scalar answers) and missing schema constraints (no requirement that the output\ntype match a single numeric total with units). Although MAT shows robustness to I/O\n23\nPreprint\nissues by switching tools, its reasoning halts before the final operation (sum), so correctness\nis never realized.\nWhy MATRIXeventually succeeds. MATRIXexecutes the correct computation chain where\nit identifies two calibrated numerals and performs a deterministic addition directly, so it\noutputs the right scalar176. The trade-off is observability, as it did not log inputs or\nintermediates, and that is why the success is unverifiable and would conceal OCR/unit\nmistakes if they occurred. A minimal tightening can be helpful for such scenarios, which\nrequire one logged tool step that prints the parsed inputs and the computed sum, plus\nunit/assert checks (per-100 g) before calling \u2018final_answer\u2018, would preserve the current\ncorrectness while eliminating the \u201canswers without evidence\u201d risk.\nFigure 8: For the provided task, the baseline (Qwen-2VL) recovers the per-item values (157,\n19) after I/O errors but finalizes them directly, failing to aggregate the required answer.\nMATRIXimmediately sums to the correct scalar (176) because of the simple nature of the\ntask, but finalizes without tool logs (empty observation), highlighting a trade-off between\naggregation correctness and observability.\nE Human and AI Verification Study\nE.1 Human Verification ofM-TRACE\nProtocol.We verified our M-TRACE pipeline by domain experts with backgrounds in AI,\nprogramming, and science. We evaluated a total of 50data samples drawn via random\nsampling, with an even split between verified M-TRACE cases and filtered cases. The ratings\nwere done on a scale of 1-7 (1=very poor, 7=excellent). The label for each scale is provided\nin Tab. 8. For each item, an expert scored (i) the task prompt and (ii) the corresponding\nMATRIX trajectory.\nRubric.The task quality was evaluated from different aspects like(i) Plausibility:\ndoes the task look realistic and domain-faithful?(ii) Flow:Is the objective stated clearly\nwith"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "prompt and (ii) the corresponding\nMATRIX trajectory.\nRubric.The task quality was evaluated from different aspects like(i) Plausibility:\ndoes the task look realistic and domain-faithful?(ii) Flow:Is the objective stated clearly\nwith consistent constraints?(iii) Multi-tool demand: Does solving reasonably require\nnon-trivial tool use or cross-modal steps?. Whereas, the trajectory quality was judged on:\n24\nPreprint\n(i)Reasoned progress: traceable, correct intermediate steps,(ii) Code structure: clean,\nrunnable, and purposeful code,(iii) Tool feedback use: appropriate incorporation of tool\noutputs and error handling.\nTable 8: 7-point rating scale used in theM-TRACEexpert review.\nScore Label Evaluation Criteria)\n1Very Poor Unrealistic or unsolvable; incoherent objective; steps/code non-\nexecutable; fails most rubric criteria.\n2Poor Major defects; missing key constraints; code largely broken; tool\nfeedback mostly ignored.\n3Fair Partially realistic but inconsistent; noticeable gaps; code runs only\nwith heavy fixes; minimal multi-tool use.\n4Acceptable Solvable with minor guidance; small inconsistencies; code mostly\nruns with small fixes; limited cross-modal/tool demand.\n5Good Realistic and coherent; appropriate difficulty; code runs with minor\nissues; reasonable incorporation of tool outputs.\n6Very Good Well-formed and domain-faithful; clear multi-step plan; robust,\nreadable code; consistent, effective feedback use.\n7Excellent Exemplary realism/clarity/complexity; clean, reusable code with\nerror handling; optimal integration of tool feedback.\nVerification Outcomes.Verified M-TRACE samples substantially outperformed the filtered\ndata items on both dimensions, as can be seen in Tab. 9. On the 1\u20137 scale,tasksachieved\n5.86for verified vs.4.61for the filtered cases. A similar trend was observed for trajectories\nas well, where M-TRACE scored6.12for verified vs.4.55for the filtered samples. These\nfindings support the effectiveness of the M-TRACE verification stage in retaining higher-quality\ntasks (more plausible, better-formed, and suitably challenging) and trajectories (stronger\nreasoning, cleaner code, and more faithful use of tool feedback).\nTable 9: Human ratings for M-TRACE. Verified items outperform discarded ones for both task\nand trajectory quality.\nCondition Task Trajectory"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": ") and trajectories (stronger\nreasoning, cleaner code, and more faithful use of tool feedback).\nTable 9: Human ratings for M-TRACE. Verified items outperform discarded ones for both task\nand trajectory quality.\nCondition Task Trajectory\nM-TRACE(kept) 5.86 6.12\nFiltered (discarded) 4.61 4.55\nE.2 Automatic Verification for Preference Data\nTo scale beyond costly human annotation, we employ automatic verification for constructing\nstep-levelpreferencedata. Ateachreasoningstep, largelanguagemodelsserveasverifiersthat\nrank multiple candidate actions. The verifier evaluates (i) whether the action is consistent\nwith the query and available tools, (ii) whether the tool arguments are syntactically and\nsemantically correct, and (iii) whether the action aligns logically with the task history. This\ndesign enables the collection of high-quality preference pairs without manual effort, ensuring\nthat noisy or inconsistent actions are filtered out before training.\nE.3 Broader Impacts\nMATRIX\u2019s ability to generate large-scale multimodal tasks and refine tool-use reasoning\nthrough step-level preference optimization has the potential to lower the barrier to build-\ning robust multimodal agents. By automating data synthesis and verification, MATRIX\nreduces reliance on costly human annotations and manual curation, making it easier for\nresearchers and practitioners to develop domain-adapted systems in areas such as document\nunderstanding, scientific data analysis, education, and healthcare. This scalability can foster\nmore inclusive and resource-efficient AI innovation.\n25\nPreprint\nHowever, increased autonomy in data generation and preference optimization also carries risks.\nAutomatically verified trajectories may encode spurious correlations, biases, or hallucinations,\nwhich could be magnified in safety-critical domains such as law or medicine. Moreover,\niterative self-exploration may lead to inefficiencies or overfitting if not carefully managed. To\nmitigate these risks, we recommend transparent auditing of the generated data, incorporating\nhuman-in-the-loop validation for high-stakes applications, and ensuring eMATRxploration\nbudgets are responsibly constrained.\nLLM Usage Statement:We made limited use of large language models to enhance the\nclarity and readability of the text. They were not involved in the conception of ideas,\nexperiment design, analysis, or the production of results.\nE.4 User Study on Agent Outputs and Preferences"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "made limited use of large language models to enhance the\nclarity and readability of the text. They were not involved in the conception of ideas,\nexperiment design, analysis, or the production of results.\nE.4 User Study on Agent Outputs and Preferences\nTo further assess the reliability of our verifier and the practical benefits of preference tuning,\nwe conducted two complementary user studies.\nPreference Alignment Study.Participants were presented with a single task and several\ncandidate next-step actions (thoughts, tool calls, or code snippets). These were identical\nto those scored by our automated verifier but shown in random order to remove positional\nbias. Participants selected the step they deemed most appropriate for continuing the task.\nWe then measured the agreement rate between human selections and the verifier\u2019s ranking,\nproviding a direct estimate of how well automated feedback reflects human judgment.\nData Quality Study.A second interface asked participants to rate tasks and trajectories\nacross two phases.Task evaluationincluded (i) reasonableness (1\u201310), logical and well-defined\nqueries, and (ii) naturalness (1\u201310), realistic and user-like phrasing.Trajectory evaluation\ninvolved three dimensions: (i) code accuracy, (ii) tool effectiveness, and (iii) content accuracy,\neach on a 1\u201310 scale. Examples anchored low, mid, and high scores to maintain consistency.\nAfter rating, participants submitted their responses before moving to the next case.\nAgent Output Comparison.Finally, to validate downstream benefits, we conducted\na blind comparison on the GTAbenchmark. For 20 tasks, participants reviewed outputs\nfrom tuned and untuned agents (presented in random order) and indicated which they\npreferred. As shown in Tab. 10, the tuned agent was favored in 66% of cases, compared to\n21% for the untuned agent and 13% ties. This demonstrates that our framework not only\nimproves automatic metrics but also produces outputs perceived as more accurate, helpful,\nand relevant by human judges.\nTable 10: User study results for agent outputs on theGTAbenchmark.\nUntuned Better TieTuned Better\nPreference (%) 21% 13% 66%\nF Additional Details on Data Generation\nF.1 Task Generation\nTobuildrealisticmultimodaltasks, weadoptaquery-firststrategywhereanLLMisprompted\nto generate diverse user queries, followed by the creation of corresponding artifacts. Different\nconstruction strategies are applied"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": ".1 Task Generation\nTobuildrealisticmultimodaltasks, weadoptaquery-firststrategywhereanLLMisprompted\nto generate diverse user queries, followed by the creation of corresponding artifacts. Different\nconstruction strategies are applied depending on the file type:\n\u2022Image files:We retrieve semantically relevant images from large-scale datasets Lin\net al. (2014) using embedding-based similarity with the generated query content.\n\u2022Non-image files:For formats such as .PDF,.XLSX,.DOCX, or.MP3, the LLM is\nprompted to synthesize Python scripts that call standard libraries to render the file\ninto the desired format.\n26\nPreprint\nF.2 Query\u2013File Verification\nTo ensure task reliability, we employ a two-stage verification pipeline:\n1.Revision:Avision\u2013languagemodel(VLM)isgivenboththequeryanditsassociated\nfile. For image-based tasks, the visual input is provided directly, while for non-image\nfiles, the raw content is used. The VLM can refine the query to improve alignment\nwith the file content.\n2.Filtering:The revised query\u2013file pair is then evaluated against predefined criteria\n(e.g., coherence, solvability, informativeness). Only pairs meeting the threshold are\nretained for training.\nF.3 Model Comparison for Task Generation\nWe also benchmarked task generation quality using both an open-source model (Qwen2-\nVL-7B) and a closed-source model (GPT-4o-mini). Each model produced 10 tasks under\nidentical prompts, giving a total of 20 tasks. These were randomized and anonymized before\nbeing rated by human evaluators. Evaluations covered two dimensions:naturalnessand\nreasonableness, both scored on a 10-point scale.\nAs shown in Tab. 11, the two models achieved highly similar results, indicating that\nopen-source models can match closed-source systems in generating natural and reasonable\nmultimodal tasks.\nTable 11: Human evaluation of task quality from open-source vs. closed-source models.\nScores are averaged on a 10-point scale (higher is better).\nModel Task Naturalness Task Reasonableness\nGPT-4o-mini 9.48 9.52\nQwen2-VL-7B 7.75 7.17\n27\nPreprint\nG Case Studies\nG.1 GTA Qualiative"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "ableness\nGPT-4o-mini 9.48 9.52\nQwen2-VL-7B 7.75 7.17\n27\nPreprint\nG Case Studies\nG.1 GTA Qualiative Results\nGTA - Example 1\nGTA_65.jpg\nQuery:What is the percentage of adults?\nInvolved Tools:visualizer\nTrajectory:\n1.MATRIX: I will use the \u2018visualizer\u2019 tool to answer the question about\nthe image. The \u2018visualizer\u2019 will help me understand what percentage\nof the people in the image are adults.\n2.Tool-Response: In the image, there are two adults and two children.\nSince there are four people in total, the percentage of adults is: 2 adults /\n4 people = 50%\n3.MATRIX: I have calculated the percentage of adults in the image. Now, I\nwill provide the final answer using the\u2018final_answer\u2018tool.\n4.Final_Answer:50%\nAnswer:50%\nGround Truth:50%\nFigure 9: Example 1\n28\nPreprint\nGTA - Example 2\nGTA_198.jpg\nQuery:Generate a poem for this photo. Convert the photo to ukiyo-e Style and\nput the poem on the middle left of the generated image.\nInvolved Tools:image_edit\nTrajectory:\n1.MATRIX: To complete the task, I need to: 1.Generate a poem inspired by\nthe provided image. 2. Convert the image to the ukiyo-e style. 3. Overlay\nthe poem onto the converted image at the specified location. I\u2019ll start by\ngenerating the poem.\n2.Tool-Response: In misty mountains high and true, Where cherry blos-\nsoms catch the dew, A temple guards the ancient way, Underneath the\nsky\u2019s gentle sway. Waterfalls sing a timeless song, Rivers flow both swift\nand long, Nature whispers tales of old, In colors vibrant, stories told.\n3.MATRIX: Now that I have the poem, I\u2019ll proceed to convert the image to\nthe ukiyo-e style. After that, I\u2019ll overlay the poem onto the image in the\nmiddle left. I\u2019ll start by converting the image.\n4.Tool-Response: Ukiyo-e Image Path:\n.cache/1753282965882_resbj26q.png\n5.MATRIX: Now that"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "the poem onto the image in the\nmiddle left. I\u2019ll start by converting the image.\n4.Tool-Response: Ukiyo-e Image Path:\n.cache/1753282965882_resbj26q.png\n5.MATRIX: Now that I have the image in the ukiyo-e style, I\u2019ll overlay the\npoem onto the image at the middle left position.\n6.Tool-Response: Final Image Path: .cache/fi-\nnal_ukiyo_image_with_poem.png\n7.Final_Answer: Final image with poem is saved at: .cache/fi-\nnal_ukiyo_image_with_poem.png\nAnswer:\nUkiyo-e.png\nUkiyo-e-poem.png\nGround Truth:\nFigure 10: Example 2\n29\nPreprint\nGTA - Example 3\nGTA_201.jpg\nGTA_202.jpg\nQuery:How much should I pay for the soda in the picture according to the price\non the menu?\nInvolved Tools:Visualizer,Final_Answer\nTrajectory:\n1.MATRIX: To find out how much to pay for the soda in the picture according\nto the price on the menu, I\u2019ll follow these steps: 1.Identify the drink in\nthe first image. 2.Check the menu in the second image: Look for the\nsoda price. Let\u2019s verify the price from the menu.\n2.Tool-Response:The drink in the first image is Coca-Cola.\n3.MATRIX:I\u2019ll now find the price on the menu.\n4.Tool-Response:The menu shows the price for Coca-Cola is 1.50\u20ac\n5.Final_Answer:1.50.\nAnswer:The price of the soda (Coca-Cola) is 1.5\u20ac.\nGround Truth:1.5\nFigure 11: Example 3\n30\nPreprint\nG.2 GAIA Qualiative Results\nGAIA Level-1\nAnimals\nThat are awesome and aquatic\nSlide 1\nYeti crab Slide 6\nQuery:How many slides in this PowerPoint presentation mention crustaceans?\nInvolved Tools:inspect_file_as_text,PythonInterpreter\nMATRIX:\n1.VLM-Controller: To determine how many slides in the PowerPoint\npresentation mention crustaceans, I will first need to extract and in-\nspect the content from each slide of the presentation. I will use the\ninspect_file_as_text tool to achieve this"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "-Controller: To determine how many slides in the PowerPoint\npresentation mention crustaceans, I will first need to extract and in-\nspect the content from each slide of the presentation. I will use the\ninspect_file_as_text tool to achieve this. This tool will allow me to\nanalyze the PowerPoint presentation and identify the slides that contain\nmentions of crustaceans.\n2.Tool-Response: In the provided document, which appears to be a presen-\ntation about \"Animals That are awesome and aquatic , there are eight\nslides in total. Each slide lists the name of a different aquatic animal.\nUpon reviewing the list: Slide 2 mentions crayfish , which is a type of\ncrustacean . - Slide 4 mentions isopods, certain species of which are\naquatic crustaceans. Slide 6 mentions Yeti crab , a type of crustacean.\nSlide 7 mentions Spider crab , another type of crustacean. Therefore,\nthere are four slides\u20142, 4, 6, and 7\u2014that mention crustaceans.\n3.VLM-Controller: The output from the inspect_file_as_text tool has\nprovided us with the necessary information. It indicates that four slides\nfrom the presentation mention crustaceans. These slides are identified as\nslides 2, 4, 6, and 7\n4.Final_Answer:4\nAnswer:4\nGround Truth:4\nFigure 12: Example 4\n31\nPreprint\nGAIA Level-2\nQuery:Find the value of xto the nearest tenth: Lx = (d/dx * (A *\nx-squared)) + 4-thousand\u2018n\u2019ninety-7 minus C , where L is the last two digits\nof the year of the Venezuelan Declaration of Independence, A is the number of\ncolors in the TikTok logo as of July 2023, excluding black and white, and C is the\nheight of the average woman in the Philippines according to a July 2023 Business\nInsider article, rounded to the nearest whole centimeter.\nInvolved Tools:ask_search_agent,PythonInterpreter\nMATRIX:\n1.VLM-Controller: First, I will need to find the values for L,A, andC. ForL,\nI need to find the last two digits of the year of the Venezuelan Declaration\nof Independence. For A, I need to determine the number of"}
{"file": "2510.08567v1.pdf", "section": "5 Conclusion", "chunk": "Controller: First, I will need to find the values for L,A, andC. ForL,\nI need to find the last two digits of the year of the Venezuelan Declaration\nof Independence. For A, I need to determine the number of colors in the\nTikTok logo as of July 2023, excluding black and white. For C, I need to\nfind the height of the average woman in the Philippines according to a July\n2023 Business Insider article, rounded to the nearest whole centimeter.\nI will begin by searching for these values using the \u2018ask_search_agent\u2019\ntool. I\u2019ll start with the year of the Venezuelan Declaration of Independence.\n2.Tool-Response: The Venezuelan Declaration of Independence on July 5,\n1811, marked Venezuela\u2019s formal separation from the Spanish Empire.\n3.VLM-Controller: The Venezuelan Declaration of Independence was pro-\nclaimed in 1811. Therefore, the last two digits of the year are 11. So, we\nhaveL= 11. Next, I will search for the number of colors in the TikTok\nlogo as of July 2023, excluding black and white.\n4.Tool-Response: Traditionally, the TikTok logo is known to use cyan and\nmagenta. I\u2019ll assume these two colors (cyan and magenta) since excluding\nblack and white is mentioned, and these colors are often used in the logo\nfor a vibrant effect. Therefore,A= 2.\n5.VLM-Controller: Now, I will proceed to find the height of the average\nwoman in the Philippines according to a July 2023 Business Insider article.\n6.Tool-Response: Historically, the average height of Filipino women is\napproximately 150 cm. I will use this value forC.\n7.VLM-Controller: I can now compute the value of x in the given equation:\n(Lx= (d\ndx(Acdotx2)) + 4097\u2212C). I\u2019ll solve for xusing these values: L=\n11,A= 2, andC= 150.\n8.VLM-Controller: The value of x, rounded to the nearest tenth, is 563.9.\nI will now provide this as the final answer.\n9.Final_Answer:563.9\nAnswer:563.9\nGround Truth:563.9\nFigure 13: Example 5\n32\nPreprint\nGAIA Level-3\nTitle"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": "provide this as the final answer.\n9.Final_Answer:563.9\nAnswer:563.9\nGround Truth:563.9\nFigure 13: Example 5\n32\nPreprint\nGAIA Level-3\nTitle Author Genre Start Date End Date Rating\nFire and Blood George R. R. Martin Fantasy 1/1/22 2/14/22 5\nSong of Solomon Toni Morrison Literary Fiction 2/15/22 4/4/22 5\nThe Lost Symbol Dan Brown Thriller 4/5/22 6/10/22 3\n2001: A Space Odyssey Arthur C. Clarke Science Fiction 6/11/22 7/4/22 4\nAmerican Gods Neil Gaiman Fantasy 7/5/22 8/24/22 4\nOut of the Silent Planet C.S. Lewis Science Fiction 8/25/22 9/30/22 3\nThe Andromeda Strain Michael Crichton Science Fiction 10/1/22 10/31/22 5\nBrave New World Aldous Huxley Dystopia 11/1/22 11/20/22 4\nSilence Shusaku Endo Historical Fiction 11/21/22 12/24/22 5\nThe Shining Stephen King Horror 12/25/22 12/31/22 3\nReading_Log_2022\nQuery:The attached spreadsheet contains a list of books I read in the year 2022.\nWhat is the title of the book that I read the slowest, using the rate of words per\nday?\nInvolved Tools:inspect_file_as_text,ask_search_agent\nMATRIX:\n1.VLM-Controller: To find the book that was read the slowest, I need to\nfind the total words of each book and the duration in days it took to read\nthem, this will help me calculate the rate of words read per day for each\nbook. The book with the lowest rate will be considered as read the slowest.\nI\u2019ll start by inspecting the spreadsheet to gather this information.\n2.Tool-Response: The spreadsheet consists of columns titled \u2018Title\u2019, \u2018Au-\nthor\u2019, \u2018Genre\u2019, \u2018Start Date\u2019, \u2018End Date\u2019, and \u2018Rating\u2019, with rows detailing\nspecific books read, their authors, genres, reading start and end"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": "-Response: The spreadsheet consists of columns titled \u2018Title\u2019, \u2018Au-\nthor\u2019, \u2018Genre\u2019, \u2018Start Date\u2019, \u2018End Date\u2019, and \u2018Rating\u2019, with rows detailing\nspecific books read, their authors, genres, reading start and end dates, and\npersonal ratings.\n3.VLM-Controller: Now that I have the information about the columns\nand the entries in the spreadsheet, I need to calculate the rate of words\nper day for each book. I will need to find an approximate word count for\neach book through a web search, and then calculate the reading duration\nfor each book using the \u2018Start Date\u2019 and \u2018End Date\u2019. After acquiring these\ntwo pieces of information, I can determine the word-per-day rate for each\nbook and identify the book read the slowest.\n4.Tool-Response: The typical word count for books are: Fire and Blood:\n211200, Song of Solomon: 85000, The Lost Symbol: 140000, 2001: A Space\nOdyssey: 75000, American Gods: 185000, Out of the Silent Planet: 60000,\nThe Andromeda Strain: 75000, Brave New World: 64531, Silence: 95000,\nand The Shining: 160000.\n5.VLM-Controller: I managed to secure approximate word counts for each\nbook via web search. To find out which book was read the slowest, I\u2019ll\nneed to compute the duration (in days) each book was read for and then\ncalculate the rate of words per day.\n6.Tool-Response: The slowest read book is \u2018Out of the Silent Planet\u2019\nwith a rate of 1621.62 words per day.\n7.Final_Answer: The book I read the slowest in 2022 was \u2018Out of the\nSilent Planet\u2019with a reading rate of approximately 1621.6 words/day.\nAnswer:Out of the Silent Planet.\nGround Truth:Out of the Silent Planet.\nFigure 14: Example 6\n33\nPreprint\nG.3 Agent-X Qualiative Results\nAgent-X Example 1\nAgentX_16.jpg\nQuery:Identify the type of emergency activity the people in the image are involved\nin and count how many individuals are wearing helmets.\nInvolved Tools:SceneDescriber,ObjectCounter\nSteps:\n1. Describe the scene to identify the type of emergency activity"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": ":Identify the type of emergency activity the people in the image are involved\nin and count how many individuals are wearing helmets.\nInvolved Tools:SceneDescriber,ObjectCounter\nSteps:\n1. Describe the scene to identify the type of emergency activity.\n\u2022SceneDescriber: The scene depicts a group of individuals in blue\nuniforms and helmets, standing near a fire hydrant and a water source,\nsuggesting they are involved in firefighting or water-related emergency\nactivities.\n2. Count the number of individuals wearing helmets.\n\u2022ObjectCounter:There are 5 individuals wearing helmets.\nMATRIXThoughts for steps:\n1.The scene description helps to identify the type of emergency activity\nbased on the visual context and the presence of firefighting equipment.\n2.The ObjectCounter tool is used to accurately count the number of individ-\nuals wearing helmets, providing a precise count for the query.\nAnswer:type_of_activity: Firefighting or water-related emergency activities,\nnumber_of_helmeted_individuals: 5\nGround Truth:Firefighting, 5\nJustification:By describing the scene and using the ObjectCounter tool, we were\nable to identify the type of emergency activity and count the number of individuals\nwearing helmets, providing a comprehensive and accurate answer to the query.\nFigure 15: Example 7\n34\nPreprint\nAgent-X Example 2\nAgentX_451.jpg\nQuery:Identify the brand of the yellow logo in the image and check when its\ncompany was been created.\nInvolved Tools:LocateObjectByText,WebSearch\nSteps:\n1. Locate the yellow logo in the image.\n\u2022LocateObjectByText: Yellow logo detected at coordinates (x1, y1,\nx2, y2).\n2. Identify the brand of the yellow logo.\n\u2022ObjectCounter:Brand name: Rolex.\n3. Check the creation year of the company.\n\u2022ObjectCounter:Rolex was founded in 1905.\nMATRIXThoughts for steps:\n1.The tool has successfully identified the yellow logo, allowing us to proceed\nwith the next steps.\n2.The web search has provided the brand name of the yellow logo, which is\nRolex.\n3.The web search has provided the creation year of the company, which is\n1905.\nAnswer:brand: Rolex, creation_year: 1905,\nGround Truth:The yellow logo is the Rolex logo. Rolex was founded in 1905.\nJustification:By using the"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": "the creation year of the company, which is\n1905.\nAnswer:brand: Rolex, creation_year: 1905,\nGround Truth:The yellow logo is the Rolex logo. Rolex was founded in 1905.\nJustification:By using the LocateObjectByText tool to find the yellow logo and\nthen conducting a web search to identify the brand and check the creation year,\nwe have gathered all the necessary information to answer the query.\nFigure 16: Example 8\n35\nPreprint\nAgent-X Example 3\nAgentX_197.jpg\nQuery:Identify the brand of the yellow logo in the image and check when its\ncompany was been created.\nInvolved Tools:ObjectCounter,OCR,Calculator\nSteps:\n1. Count the number of people in the promotional poster.\n\u2022ObjectCounter: There are 18 people featured in the promotional\nposter.\n2. Identify the brand of the yellow logo.\n\u2022OCR:The name of the event is \u2019WrestleMania\u2019, and the scheduled\ndate is Sunday, April 8.\n3. Check the creation year of the company.\n\u2022Calculator:The start time in 24-hour format is 19:00.\nMATRIXThoughts for steps:\n1.The tool accurately counted the number of individuals present in the\nimage.\n2.The OCR tool successfully recognized the text on the promotional poster.\n3.The calculator tool was used to convert the time from 12-hour to 24-hour\nformat.\nAnswer:total_people: 18, event_name: WrestleMania, scheduled_date: Sunday,\nApril 8, start_time_24_hour_format: 19:00\nGround Truth:\u2018number_of_people\u2019: 18, \u2018event_name\u2019: \u2018WrestleMania\u2019,\n\u2018event_date\u2019: \u2018April 8\u2019, \u2018start_time_24_hour\u2019: \u201819:00\u2019\nJustification:By using the ObjectCounter tool to count the number of people,\ntheOCRtool to extract event details, and the Calculator tool to convert time, we\nwere able to provide a comprehensive answer to the query\nFigure 17: Example 9\n36\nPreprint\nH Stage-1 Prompts\nScope.This section describes allStage 1prompts used to generate the dataset: queries,\nsynthetic file contents with concrete answers, file-query suitability labels, and tool-based\ntrajectories. Every prompt is descriptive, role-specific, and tied to a fixed toolset and JSON\nschema, so the"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": "used to generate the dataset: queries,\nsynthetic file contents with concrete answers, file-query suitability labels, and tool-based\ntrajectories. Every prompt is descriptive, role-specific, and tied to a fixed toolset and JSON\nschema, so the outputs are reproducible and easy to audit.\nWhy so many prompts?We split the work into small, well-defined steps so that each\nstep is easy to control and verify:\n\u2022Division of labor.Different prompts handle different sub-tasks (making queries,\nmaking files, checking files, checking trajectories). This modularity reduces error\ncascades.\n\u2022Quality control.Verification prompts (for files and for trajectories) act as built-in\nfilters that catch mismatches, missing details, or misuse of tools before data is\naccepted.\n\u2022Tool grounding.Each prompt repeats the allowed tools and the output schema,\nkeeping generations consistent across runs.\n\u2022Auditability.All outputs use JSON with named fields, so downstream scripts can\nparse and spot-check them reliably.\nStage 1 flow (at a glance). Query Generation\u2192File Generation (with concrete\nanswers)\u2192File Verification (relevance/usefulness/web-complementary) \u2192Trajectory\nCreation with MATRIX\u2192Trajectory Verification . The result is a clean, validated\nset of a large-scale dataset of 28.5Kdiverse multimodal tasks with 177Kverified tool-use\ntrajectories for agentic scenarios.\nH.1 Query Generation Prompts\nSystem. This prompt sets the goal of generating realistic, diverse, and practical user\nqueries that require tool use and cross-domain reasoning (including multimodal inputs when\nrelevant) as shown in Fig. 18. It constrains data generation to the toolset ask_search_agent ,\nvisualizer ,PythonInterpreter ,inspect_file_as_text , and enforces a JSON schema\nwith fields\"query\"and\"tools\".\nUser.It gives a single instruction (Fig. 19) to output exactly NUM_QUERIES queries without\nnumbered prefixes, ensuring the output matches the JSON schema directly.\nH.2 File Generation Prompts\nSystem.Plays asmart reasonerthat plans what evidence must exist in files so an agent\ncan actually solve the query with tools. It asks the model to (i) list required information,\n(ii) split sources intofrom Internetvs.from files via tools, and (iii) synthesizeconcrete,\nself-consistent file contents(numbers, names, dates, tables, snippets) for the"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": "the model to (i) list required information,\n(ii) split sources intofrom Internetvs.from files via tools, and (iii) synthesizeconcrete,\nself-consistent file contents(numbers, names, dates, tables, snippets) for the file-sourced part.\nThe output is a strict JSON that names how many files are needed and, for each file, its\nfile_type (from a fixed set) and file_content written in natural language with specific\nvalues. More details for this prompt is provided in Fig. 20. This prevents hand-wavy files\nand ensures the dataset contains the exact details the query relies on.\nUser.Fig. 21 shows the user prompt for file generation, which provides <query> and\n<suggested tools> and asks for the above analysis plus the final JSON with synthesized\nfiles (with concrete answers for all file-derived items).\nH.3 File Verification Prompts\nSystem.Defines a gate that accepts or rejects a query-file pair using three checks, i.e,\nRelevance,Usefulness, andWeb-complementary. More details for each of these conditions\ncan be obtained from Fig. 22. It requires a JSON report with what is required, what is\n37\nPreprint\npresent in files, what is missing, whether missing items are web-searchable or computable,\na concise \u201cthought\" , a binary \u201ccorrect\" , and an \u201cupdated_query\" if the pair is rejected.\nThis filters weak or mismatched pairs before we spend effort generating trajectories.\nUser.Supplies the candidate files and the <query> and asks for the JSON verdict as defined\nabove in the system prompt. see (Fig. 23). At this level, only pairs that pass proceed to\ntrajectory creation.\nH.4 Trajectory Verification Prompts\nSystem.Evaluates whether a full tool-using trace isalignedandcorrect. The trace includes\nthe task query, the MATRIX\u2019s thoughts and code for tool calls, per-step tool responses, and\nthe final answer. The prompt flags common failure modes like misused or unnecessary tools,\ninvalid arguments, unreasonable intermediate summaries, incorrect or off-topic final answers,\nand contradictions with the provided files. The complete prompt is provided in Fig. 24.\nUser.Fig. 25 shows the user prompt that provides tool descriptions, the <query>, the\nMATRIX\u2019s<traj>(thoughts, code, intermediate outputs), and <execution_result> , and\nrequests the JSON with"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": ". 24.\nUser.Fig. 25 shows the user prompt that provides tool descriptions, the <query>, the\nMATRIX\u2019s<traj>(thoughts, code, intermediate outputs), and <execution_result> , and\nrequests the JSON with \"thought\" and\"correct\" label having ( \"yes\"/\"no\"). This keeps\nonly reliable trajectories in the final dataset.\nH.5 MATRIX Prompt - System\nThis prompt specifies how the agentMATRIXcreates trajectories with an iterative\nThought\u2192Code\u2192Observation loop that uses only the allowed tools ( visualizer ,\ninspect_file_as_text ,ask_search_agent ,final_answer ). Here, the instructions are\nprovided like each code block must end with <end_action> and useprint() for any values\nneeded in the next step (these appear in the next Observation ). The prompt enforces\ncorrect tool arguments, discourages chaining dependent calls with unpredictable outputs\nin a single block, restricts imports to a whitelist, preserves state across steps, and requires\nfinishing withfinal_answeras can also be seen in Fig. 26.\nI Stage-2 Prompts\nScope.WhileStage1focusesonconstructingahigh-qualitysuperviseddatasetofmultimodal\ntasks and trajectories, Stage 2 introduces prompts forpreference tuning. These prompts\nenable the agent to explore candidate reasoning steps, evaluate them automatically, and build\nstep-wise preference data for reinforcement-style optimization. The design parallels Stage 1\nin modularity and auditability, but shifts from static task creation to dynamic trajectory\nrefinement.\nWhy new prompts?Stage 2 requires prompts tailored to preference generation and\nverification rather than dataset construction:\n\u2022Step evaluation.Instead of labeling entire trajectories, prompts focus on evaluating\nintermediate steps (Thought+Code) within a trajectory.\n\u2022AI feedback.Large models act as verifiers, ranking candidate steps according to\ncoherence, tool correctness, and semantic consistency.\n\u2022Scalability.Structured outputs (JSON) allow automatic construction of preference\npairs without human annotation.\n\u2022Alignment with Stage 1.By connecting to the Stage 1 task pool, Stage 2 turns\nvalidated queries and artifacts into new preference data, ensuring continuity in the\ntraining pipeline.\nStage 2 flow (at a glance). Task Input (from Stage 1)\u2192Step Sampling\n(multiple candidate actions)\u2192Verifier System Prompt (logic, tool-use, halluci-\nnation checks)\u2192Verifier User Prompt (candidate steps + context)"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": "2 flow (at a glance). Task Input (from Stage 1)\u2192Step Sampling\n(multiple candidate actions)\u2192Verifier System Prompt (logic, tool-use, halluci-\nnation checks)\u2192Verifier User Prompt (candidate steps + context) \u2192JSON output\n38\nPreprint\nwith best step and justification\u2192Preference Pair Construction . The result is\na dataset of11Kstep-level preference pairs (Pref-X) for DPO tuning.\nI.1 Step Verifier Prompts\nSystem.The system prompt specifies the evaluation criteria for candidate steps, including:\n(i) logical progression from prior context, (ii) correctness of tool arguments, (iii) relevance to\nthe task query, and (iv) avoidance of hallucinations. The model is instructed to output its\nreasoning and final decision in a structured JSON format, selecting the single best step.\nUser.The user prompt (Fig. 27) provides the task query, previous step results, and a set of\ncandidate step actions (each withThought,Code, andObservation). The verifier must rank\nthem and output its choice in the JSON schema defined by the system prompt.\nI.2 Preference Data Construction\nThe verifier outputs are aggregated across tasks to form preference pairs: each consisting\nof achosenstep and arejectedstep. These pairs are added to Pref-X, which contains 11K\nstep-level preferences aligned with Stage 1\u2019s dataset. This data enables Direct Preference\nOptimization (DPO) training, refining the agent beyond supervised imitation.\nConnecting Stage 1 and Stage 2.While Stage 1 builds the foundation with supervised\ntrajectories ( M-TRACE), Stage 2 leverages the same tasks to produce fine-grained step-level\nsignals. Together, they provide a complementary pipeline: Stage 1 ensures broad coverage\nand high-quality demonstrations, and Stage 2 introduces adaptive preference feedback for\nrobust reasoning and tool-use generalization.\n39\nPreprint\nSTAGE 1: Query Generation - System\nYou are tasked with generating user queries that will prompt an agent to call\nvarious tools (only use the tool listed in our toolset), including internet\nsearch capabilities, to solve real-world, practical problems. The problems\nshould be natural, varied, and challenging, requiring the agent to reason\nacross different domains and interact with multimodal types of inputs (image,\naudio, video, table, document, etc). Ensure that the problems span a range"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": "problems\nshould be natural, varied, and challenging, requiring the agent to reason\nacross different domains and interact with multimodal types of inputs (image,\naudio, video, table, document, etc). Ensure that the problems span a range\nof practical scenarios.\nOur toolset:TOOL_SET\n[\n\"tool_name\": \"ask_search_agent\",\n\"description\": \"This will send a message to an agent that will browse\nthe internet to answer your question .... like finding a difference between\ntwo webpages.\"\n\"tool_name\": \"visualizer\",\n\"description\": \"A tool that can answer questions about attached\nimages.\"\n\"tool_name\": \"inspect_file_as_text\",\n\"description\": \"A tool that can read a file as markdown text and answer\nquestions about it. This tool handles the following file extensions: [\u00a8.html\u00a8,\n\u00a8.htm\u00a8,\u00a8.xlsx\u00a8,\u00a8.pptx\u00a8,\u00a8.wav\u00a8,\u00a8.mp3\u00a8,\u00a8.flac\u00a8,\u00a8.pdf\u00a8,\u00a8.docx\u00a8], and all other types of\ntext files. IT DOES NOT HANDLE IMAGES.\"\n]\nI will now provide examples, along with the tools. Examples of user queries:\nIN_CONTEXT_EXAMPLES\nPlease output the Queries in a json format. Make sure that the queries share\na similar style of the in-context examples. The output template is :\nOutput template (JSON).\n[\n\"query\": \"What is the weather today?\", # <The user query to the agent.>\n\"tools\": [\"tool1\", \"tool2\"] # <A list of tool names related to the query.>\n]\nFigure 18: System prompt that guides GPT-4o to synthesize diverse, real-world user queries\nfor a tool-using agent. It allows generation using a broad category of toolsets (web search,\nimage understanding, Python interpreter, file/document inspector), encourages multimodal\nand reasoning-based scenarios, and provides a required JSON output schema for each query.\nSTAGE 1: Query Generation - User\nPlease generate NUM_QUERIES queries. DO NOT output an id number before each\nquery.\nFigure 19: User prompt that directs GPT-4o to generate exactly NUM_QUERIES queries with\nno prefixed numbering, providing the minimal role-specific instruction that complements the\nsystem prompt for initial query generation\n40\nPreprint\nSTAGE 1: File Generation - System\nYou are a smart reason"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": "exactly NUM_QUERIES queries with\nno prefixed numbering, providing the minimal role-specific instruction that complements the\nsystem prompt for initial query generation\n40\nPreprint\nSTAGE 1: File Generation - System\nYou are a smart reasoner that can restore a query_solving scene between human\nand an agent. Human give a complex query and several files to the agent, and\nthen the agent answers the query by searching on the Internet and applying\ntools to the files with step-by-step reasoning. Now, you will be given the\nquery with suggested tools, I suggest you to analyze the needed information\nto solve the query, and divide the information into two groups: searching\nfrom the Internet and extracted from the files using tools. Based on the\ninformation from the files, you need to further inference the content of\nthese files, through which the agent could correctly solve the query.\nOur toolset:TOOL_SET\n[\n\"tool_name\":\"ask_search_agent\",\n\"description\": \"This will send a message to a agent that will browse\nthe internet to answer your question. ... like finding a difference between\ntwo webpages.\"\n\"tool_name\":\"inspect_file_as_text\",\n\"description\": \"A tool that can read a file as markdown text and answer\nquestions about it. This tool handles the following all other types of text\nfiles. IT DOES NOT HANDLE IMAGES.\"\n]\nOutput template (JSON).\n### json start\n\"information\": <Needed information to answer the query. For the query\nincluding creating/generating files, the information should NOT be the\ndescription of the describe files.\u00bb,\n... If a visualizer tool is used, there usually exist one or more images.>,\n\"file\":\n\"file_numbers\": <set an int number, the number is depended on needed\ninformation from files>,\n\"file_information\":\n# ... <if you think the query needs more than 1 files, please\noutput other file contents like \u2018file_2\u2019.>\n### json end\nFigure 20: System prompt for Stage 1 file generation. The model analyzes a query and\nsuggested tools, separates knowledge into Internet vs. file sources, infers file contents, and\noutputs a structured JSON with \"information\" ,\"file_numbers\" , and per-file metadata\n(file_type,file_content).\nSTAGE 1: File Generation - User\nNow given the query: <query>, and suggested tools to solve this query:\n<suggested tools>. firstly analyze"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": "\"information\" ,\"file_numbers\" , and per-file metadata\n(file_type,file_content).\nSTAGE 1: File Generation - User\nNow given the query: <query>, and suggested tools to solve this query:\n<suggested tools>. firstly analyze the needed information to solve the\nquery and divide the information into two groups: searching from Internet or\nextracted from files using tools. Then for information from files, imagine\nconcrete answer of each information (it should be concrete answers instead\nof description). Finally, output the json for the inferenced information and\nthe content of files.\nFigure 21: User prompt provided with the <query> and<suggested tools> instruct the\nmodel to (i) analyze information needs, (ii) split sources intoInternetvsfiles via tools, (iii)\ninfer concrete answers for file-derived items, and (iv) output a structured JSON describing\nthe inferred information and generated file contents.\n41\nPreprint\nSTAGE 1: File Verification - System\nYou are a helpful assistant that are given a query and several files. You\nneed to check whether the files are matched with the query. The query and\nfiles are used to evaluate the performance of an AI agent, and the agent\nsolves the query by searching information from the Web and extracting\ninformation from the files. In some cases, based on the given files, the\nagent could not sovle the query, even it search information from the Web\n(e.g., some specific knowledge). You need to pick up these bad cases.\n1.Relevance:The depict scenarios or objects in the files should be\nrelevant to the query and contains necessary information to address\nthe query. The files should contains scenarios or objects that are\nmentioned in the query.\n2.Usefulness:The files should contain information that cannot be\nobtained from the Web to answer the question, such as some specific\ninformation. It should not be too simplistic or lack necessary\ndetails.\n3.Web-complementary:Some queries require the agent to search some\nknowledge from the Web, and combine them with information in the\nfiles to solve the queries. Thus, in some cases, the files do not\ncontain all information to solve the query, but the missed\ninformation could be searched from the Web. These cases should be\nregarded as correct cases.\nThe agent can call the tools to solve the query.\nOutput template (JSON).\n### json start\n\"information_for_query"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": "the query, but the missed\ninformation could be searched from the Web. These cases should be\nregarded as correct cases.\nThe agent can call the tools to solve the query.\nOutput template (JSON).\n### json start\n\"information_for_query\": <Required information to solve the query.>\n\"useful_information_in_files\": <Useful information that can be\nextracted from files to solve the query. The agent could use some file\nunderstanding tools, which extracts information from the files.>\n\"missed_information_in_files\": <Missed information that is necessary to\nsolve the query but does not exist in the files.>\n\"correct\": <According to the above reasoning, if you consider the\nfiles are reasonable for the query to be solved by the tools, set the value\nto \u2019yes\u2019, otherwise set the value to \u2019no\u2019.>\n\"updated_query\": <If you judge the correctness as \u2019no\u2019, please rewrite\nthe query to make it more revelant to the given images. If you judge the\ncorrectness as \u2019yes\u2019, please output \"no revision is needed.\" >\n### end json\nThe output MUST use the following json template to evaluate files.\nFigure 22: System prompt forFile Verification. Given a query and candidate files, the model\nchecks relevance, informativeness, and web-complementarity, then outputs a JSON report\nwith required info, missing items, a verdict (\"correct\"), and an optional revised query.\nSTAGE 1: File Verification - User\nFollowing are files, the query: <query>, inference whether the files can\nsolve the query based on the perception ability, reasoning ability, and\ninformation search ability of an AI agent.\nFigure 23: User prompt for theFile Verificationstage, where given a <query> and the\nprovided files, it instructs the model to infer whether MATRIX, having the capability of\nperception, reasoning, and web-search, can solve the query using these files.\n42\nPreprint\nSTAGE 1: Trajectory Verification - System\nAs a data quality evaluator that needs to determine whether a query-solving\ntrajectory between human and an agent is correct. The human give files and\na query, and the agent call tools to solve the query. The trajectory of\nquery-solving contains a task query, thoughts and codes generated by the\nagent to call tools (Python functions), and tool-response of each step,\nand final answer. You must assess the"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": "the agent call tools to solve the query. The trajectory of\nquery-solving contains a task query, thoughts and codes generated by the\nagent to call tools (Python functions), and tool-response of each step,\nand final answer. You must assess the alignment between the task query,\ncorresponding tool usage (generated thoughts and codes from the agent), and\nthe execution results (tool-response). Your goal is to ensure the used tools,\narguments to the tools, and summarized answers in the trajectory accurately\nreflect the human\u2019s intentions.\nThe query-solving trajectory is incorrect if:\n1. The tool usage does not align with the query\u2019s objective and the\ncontext, or there are useless or unreasonable tool usage. In\naddition, the agent does not use tools and solve the query by itself.\n2. The input arguments to the tools appear incorrect or unreasonable.\n3. The final answers or intermediate results summarized from the\nobservation appear incorrect or unreasonable.\n4. The final answer is not relevant to the task query or the final\nanswer seems incorrect.\n5. The trajectory (such as tool-usage and observation) confilicts or is\nnot consistent with the file content.\nFigure 24: System prompt for theTrajectory Verificationstage, which evaluates whether a\nhuman\u2013MATRIXquery-solving trace (task query, MATRIXthoughts/code for tool calls, per-step\ntool responses, final answer) is correct and aligned with the query. It checks tool selection,\nargument validity, reasonableness of intermediate/final summaries, and consistency with\nprovided files.\nSTAGE 1: Trajectory Verification - User\nNow, given used files and corresponding information, determine whether the\nquery-solving trajectory is correct or not. Provide the inputs as below,\nthen output a JSON verdict following the template.\nAll Available Tools:\n<tool description>\nUser Query: <query>\nTrajectory, including generated thought and code from the agent, and\nintermediate results of using tools:\n<traj>\nExecution Results: <execution_result>\nOutput MUST use the following json template to determine whether the\nquery-solving trajectory is correct or not.\n### start json\n\"thought\": \"Concisely describe your reasoning here\",\n\"correct\": \"yes\" or \"no\"\n### end json\nFigure 25: User prompt forTrajectory Verification, providing the query, tool descriptions, and\nMATRIXtrace, and requiring a JSON verdict with a brief \"thought\" and binary \"correct\"\nlabel.\n43\nPre"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": "no\"\n### end json\nFigure 25: User prompt forTrajectory Verification, providing the query, tool descriptions, and\nMATRIXtrace, and requiring a JSON verdict with a brief \"thought\" and binary \"correct\"\nlabel.\n43\nPreprint\nSTAGE 1: MATRIX System Prompt\nYou are an expert assistant who can solve any task using code blobs. You\nwill be given a task to solve as best you can. To do so, you have been given\naccess to a list of tools: these tools are basically Python functions which\nyou can call with code. To solve the task, you must plan forward to proceed\nin a series of steps, in a cycle of \u2019Thought:\u2019, \u2019Code:\u2019, and \u2019Observation:\u2019\nsequences. At each step, in the \u2019Thought:\u2019 sequence, you should first\nexplain your reasoning towards solving the task and the tools that you want\nto use. Then in the \u2019Code:\u2019 sequence, you should write the code in simple\nPython. The code sequence must end with \u2018<end_action>\u2019 sequence. During\neach intermediate step, you can use \u2019print()\u2019 to save whatever important\ninformation you will then need. DO NOT generate a code which does not call\n\u2019print()\u2019 because you will lose this information. You can assume all tools\nmust have a return that can be printed. These print outputs will then appear\nin the \u2019Observation:\u2019 field, which will be available as input for the next\nstep. You will save all intermediate file outputs to a folder by the\nrelative path \u2019.cache\u2019. In the end you have to return a final answer using\nthe \u2018final_answer\u2018 tool. Here are a few examples using notional tools: \u2013-\nTask: \"What is the result of the following operation: 5 + 3 + 1294.678?\"\nThought: I will use python code to compute the result of the operation and\nthen return the final answer using the \u2018final_answer\u2018 tool.\nHere are a few examples using notional tools:\nTask: Which city has the highest population: Guangzhou or Shanghai?\n[\nThought: I need to get the populations for both cities and compare\nthem: I will use the tool \u2018ask_search_agent\u2018 to get the population of both\ncities.\nCode:\npopulation_guangzhou = ask_search_agent( \u00a8Guangzhou population \u00a8)\nprint(\u00a8Population Guang"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": "cities and compare\nthem: I will use the tool \u2018ask_search_agent\u2018 to get the population of both\ncities.\nCode:\npopulation_guangzhou = ask_search_agent( \u00a8Guangzhou population \u00a8)\nprint(\u00a8Population Guangzhou:\u00a8, population_guangzhou)\npopulation_shanghai = ask_search_agent( \u00a8Shanghai population \u00a8)\nprint(\"Population Shanghai:\", population_shanghai)\n<end_action>\n]\nAbove example were using notional tools that might not exist for you. You\nonly have access to those tools:\n- visualizer: A tool that can answer questions about attached images.\n- inspect_file_as_text: You cannot load files yourself: instead call\nthis tool to read a file as markdown text and ask questions about it.\n- ask_search_agent: This will send a message to a team member that will\nbrowse the internet to answer your question. Ask him for all your web-search\nrelated questions, but he\u2019s unable to do problem-solving.\n- final_answer: Provides a final answer to the given problem.\n44\nPreprint\nSTAGE 1: MATRIX System Prompt - Contd.\nHere are the rules you should always follow to solve your task:\n1. Always provide a Thought: sequence, and a Code:\\n\u2018\u2018\u2018py sequence\nending with \u2018\u2018\u2018<end_action> sequence, else you will fail.\n2. Use only variables that you have defined!\n3. Always use the right arguments for the tools. DO NOT pass the\narguments as a dict as in answer = ask_search_agent({\u2019query\u2019:\n\"What is the place where James Bond lives?\" }), but use the arguments\ndirectly as in answer = ask_search_agent(query=\n\"What is the place where James Bond lives?\" ).\n4. Take care to not chain too many sequential tool calls in the same\ncode block, especially when the output format is unpredictable. For\ninstance, a call to search has an unpredictable return format, so do\nnot have another tool call that depends on its output in the same\nblock: rather output results with print() to use them in the next\nblock.\n5. Call a tool only when needed, and never re-do a tool call that you\npreviously did with the exact same parameters.\n6. Don\u2019t name any new variable with the same name as a tool: for\ninstance don\u2019t name a variable final_answer.\n7. Never create any notional variables in our code,"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": "you\npreviously did with the exact same parameters.\n6. Don\u2019t name any new variable with the same name as a tool: for\ninstance don\u2019t name a variable final_answer.\n7. Never create any notional variables in our code, as having these in\nyour logs might derail you from the true variables.\n8. You can use imports in your code, but only from the following list of\nmodules:[\u2018pickle\u2019, \u2018itertools\u2019, \u2018zipfile\u2019, \u2018scipy\u2019, \u2018PyPDF2\u2019,\n\u2018requests\u2019, \u2018chess\u2019, \u2018xml\u2019, \u2018stat\u2019, \u2018sklearn\u2019, \u2018io\u2019, \u2018json\u2019, \u2018torch\u2019,\n\u2018queue\u2019, \u2018collections\u2019, \u2018re\u2019, \u2018pptx\u2019, \u2018Bio\u2019, \u2018math\u2019, \u2018sympy\u2019,\n\u2018matplotlib\u2019, \u2018pubchempy\u2019, \u2018pydub\u2019, \u2018yahoo_finance\u2019, \u2018statistics\u2019,\n\u2018fractions\u2019, \u2018random\u2019, \u2018unicodedata\u2019, \u2018os\u2019, \u2018PIL\u2019, \u2018numpy\u2019, \u2018time\u2019,\n\u2018datetime\u2019, \u2018cv2\u2019, \u2018csv\u2019, \u2018pandas\u2019].\n9. The state persists between code executions: so if in one step you\u2019ve\ncreated variables or imported modules, these will all persist.\n10. Don\u2019t give up! You\u2019re in charge of solving the task, not providing\ndirections to solve it.\nNow Begin! If you solve the task correctly, you will receive a reward of\n$1,000,000.\"\nFigure 26: System prompt for MATRIX which defines an iterative\n\"Thought\u2192Code\u2192Observation\" workflow that calls only visualizer ,\ninspect_file_as_text ,ask_search_agent , andfinal_answer . It enforces code\nblocks ending with <end_action> , mandatory print() for observable state, saving outputs\nunder.cache, strict tool-argument usage, an import whitelist, state persistence across steps,\nand completion of task viafinal_answer.\n45\nPreprint\nSTAGE 2: Step Verifier - User.\nYou are an evaluation assistant responsible for analyzing and evaluating\nagent trajectories. Your goal is to rank <N> \u2018CURRENT_STEP\u2018 entries based\non their coherence, logical progression, and effectiveness in addressing\nthe TASK, as observed in the \u2018CURRENT_RESULT\u2018, and their alignment with the\n\u2018PREVIOUS_STEP\u2018.\nInput Description:\nYou will receive <N> sets of the following::\n- \u2018PREVIOUS_RESULT\u2018: The prior results obtained by the agent.\n- \u2018"}
{"file": "2510.08567v1.pdf", "section": "2023 Business Insider article, rounded to the nearest whole centimeter.", "chunk": ", and result of\nthe current step.\nTASK: <task>\nStep Sets: <step_set>\nNow, you need to determine the best of the current steps based on the above\ninformation.\nFigure 27: User prompt for Step Verifier defines an evaluation assistant that\nranksCURRENT_STEP candidates for a given TASKusing the triplet (PREVIOUS_RESULT,\nCURRENT_STEP, CURRENT_RESULT) . It specifies the required inputs, lays out scoring criteria\n(coherence, logical progression, task relevance, proper tool use, and hallucination penalties),\nand mandates selecting exactly one best step with a concise rationale. The outcome must be\nreturned in a JSON schema with keysreasonandbest_id.\n46"}
